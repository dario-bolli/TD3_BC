{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "'''\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import scipy.stats as ss\n",
    "import pingouin as pg\n",
    "\n",
    "import scipy.signal as sci\n",
    "import math\n",
    "import scipy as sp\n",
    "\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "\n",
    "import re\n",
    "\n",
    "import copy\n",
    "import argparse\n",
    "import d4rl\n",
    "from collections import deque'''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actions(cue_data, cuevel): #cuevel, cueDirection):\n",
    "    cueAngle = min_max_standardisation(compute_angle(cue_data[\"cueDirection\"])) #cueDirection\n",
    "\n",
    "    cueMagnitude,_ = compute_impulseForce(cuevel, cue_data[\"cueDirection\"])\n",
    "    cueMagnitude = min_max_standardisation(cueMagnitude)\n",
    "    actions = np.hstack((cueAngle.reshape(-1,1), cueMagnitude.to_numpy().reshape(-1,1)))\n",
    "    return actions  #cueAngle, cueMagnitude\n",
    "\n",
    "def min_max_standardisation(array):\n",
    "    #array must be 1-dimensional\n",
    "    array = (array - array.min()) / (array.max() - array.min())\n",
    "    return array\n",
    "\n",
    "def compute_angle(cueDirection):\n",
    "    cueAngle = np.rad2deg(np.arctan2(cueDirection['z'].values, cueDirection['x'].values))\n",
    "    return cueAngle\n",
    "\n",
    "def compute_impulseForce(cuevel, cuedirection):\n",
    "    impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "    shotMagnitude = np.zeros(1)\n",
    "    shotDir = np.zeros(cuedirection.shape)\n",
    "    #Reward: magnitude range\n",
    "    lbMagnitude = 0.1   #0.516149\n",
    "    ubMagnitude = 3 #0.882607\n",
    "\n",
    "    shotMagnitude = np.sqrt(np.square(cuevel).sum(axis=1))\n",
    "    #np.linalg.norm(cuevel, axis=1)\n",
    "    for i in range(cuevel.shape[0]):\n",
    "        if shotMagnitude[i] > ubMagnitude:\n",
    "            shotMagnitude[i] = ubMagnitude\n",
    "        elif shotMagnitude[i] < lbMagnitude:\n",
    "            shotMagnitude[i] = 0\n",
    "\n",
    "        shotDir[i][0] = cuedirection[\"x\"].iloc[i]\n",
    "        shotDir[i][1] = cuedirection[\"z\"].iloc[i]\n",
    "        if shotMagnitude[i] == 0:\n",
    "            impulseForce[i][:] = 0\n",
    "        else:\n",
    "            impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "    return shotMagnitude, impulseForce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_reward(Angle_data): #Success, SuccessFunnel_table, SuccessMedian_table):\n",
    "    print(\"Reward function\")\n",
    "    reward = np.zeros(len(Angle_data.Trial))\n",
    "    \n",
    "    for i in range(len(Angle_data.Trial)):\n",
    "        if Angle_data['Success'][i] == 1:\n",
    "            reward[i] = 100\n",
    "        elif Angle_data['SuccessFunnel'][i] == 1:\n",
    "            reward[i] = 100\n",
    "        #elif SuccessMedian_table.iloc[i] == 1:     #Agent will not understand reward for trial better than past 10 trials\n",
    "            #reward[i] = 20\n",
    "        else:\n",
    "            reward[i] = -10\n",
    "\n",
    "    return reward\n",
    "\n",
    "def static_for_n_timesteps(cueballvel, redballvel, timestep, n):\n",
    "    err = 0.001\n",
    "    count=0\n",
    "    for i in range(timestep, timestep+n, 1):\n",
    "        if np.linalg.norm(cueballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) < err and np.linalg.norm(redballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) < err:\n",
    "            count += 1\n",
    "    if count == n:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def start_hit_timesteps(subjData): #, TrialNumber):\n",
    "    \n",
    "    print(\"start hit timesteps function\")\n",
    "    cueballPos = subjData[\"cbpos\"]\n",
    "    cueballvel = subjData[\"cbvel\"]\n",
    "    redballvel = subjData[\"rbvel\"]\n",
    "    cuevel = subjData[\"cuevel\"]\n",
    "    #Find when cue hits cueball\n",
    "    hit_ind=[]\n",
    "    start_ind=[]\n",
    "    n = 10\n",
    "    threshold = 0.1\n",
    "\n",
    "    miss_hit = False\n",
    "\n",
    "    prev_trial = 0\n",
    "    block_until_next_trial = True\n",
    "    for i in range(len(cueballPos)):\n",
    "        #The first 200 timesteps approximately are calibration and parasite movements\n",
    "        if i > 200:   #!= 0:\n",
    "            #New trial started\n",
    "            if cueballPos[\"trial\"].iloc[i] > prev_trial:\n",
    "                if miss_hit == True and len(hit_ind) < len(start_ind):   \n",
    "                    hit_ind = np.append(hit_ind, start_ind[-1]+350)#on average hitting cueball after 350 timesteps\n",
    "                    miss_hit = False \n",
    "                if static_for_n_timesteps(cueballvel, redballvel, i, n):\n",
    "                    #Wait for cueball vel y-axis and redball vel y-axis to be zero after new trial started\n",
    "                    start_ind = np.append(start_ind, i)\n",
    "                    prev_trial = cueballPos[\"trial\"].iloc[i]\n",
    "                    block_until_next_trial = False\n",
    "                    miss_hit = True\n",
    "            elif cueballPos[\"trial\"].iloc[i] == prev_trial:\n",
    "                if np.linalg.norm(cueballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) > threshold and block_until_next_trial == False:\n",
    "                    #Add 6 timesteps for margin\n",
    "                    hit_ind = np.append(hit_ind, i+5)\n",
    "                    block_until_next_trial = True\n",
    "                    miss_hit = False\n",
    "\n",
    "        #if last Trial is missed\n",
    "        if i == len(cueballPos.index) and miss_hit:\n",
    "            hit_ind = np.append(hit_ind, start_ind[-1]+350)#on average hitting cueball after 350 timesteps\n",
    "\n",
    "    if len(start_ind) != 250 or len(hit_ind) != 250:\n",
    "        print(\"WARNING: either missed a start-hit index, or a trial was not properly recorded\")\n",
    "        #raise ValueError( \"Missed an index. start ind size\", len(start_ind), \"and hit ind size: \", len(hit_ind))\n",
    "    for i in range(len(start_ind)):\n",
    "        if start_ind[i] >= hit_ind[i]:\n",
    "            raise ValueError(\"start ind > hit_ind\", i , start_ind, hit_ind)\n",
    "    return start_ind.astype(int), hit_ind.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_hit_reduced_1_timesteps(subjData):\n",
    "    \n",
    "    print(\"start hit timesteps function\")\n",
    "    cueballPos = subjData[\"cbpos\"]\n",
    "    cueballvel = subjData[\"cbvel\"]\n",
    "    redballvel = subjData[\"rbvel\"]\n",
    "    cuevel = subjData[\"cuevel\"]\n",
    "    #Find when cue hits cueball\n",
    "    hit_ind=[]\n",
    "    start_ind=[]\n",
    "    n = 10\n",
    "    threshold = 0.06\n",
    "\n",
    "    start_movement = False\n",
    "\n",
    "    prev_trial = 0\n",
    "    start_movement_ind = 0\n",
    "    window_before_hit = 1   #30  #200\n",
    "    window_after_hit = 1    #20   #50\n",
    "\n",
    "    for i in range(len(cueballPos[\"trial\"])):\n",
    "        #The first 200 timesteps approximately are calibration and parasite movements\n",
    "        if i > 200:   #!= 0:\n",
    "            #New trial started\n",
    "            if cueballPos[\"trial\"].iloc[i] > prev_trial:\n",
    "                if static_for_n_timesteps(cueballvel, redballvel, i, n):\n",
    "                    #Wait for cueball vel y-axis and redball vel y-axis to be zero after new trial started\n",
    "                    start_movement_ind = i\n",
    "                    start_movement = True\n",
    "                    prev_trial = cueballPos[\"trial\"].iloc[i]\n",
    "\n",
    "            elif cueballPos[\"trial\"].iloc[i] == prev_trial and start_movement == True:        \n",
    "                if np.linalg.norm(cueballvel[[\"x\", \"z\"]].iloc[i].to_numpy()) > threshold:\n",
    "                    if len(start_ind) > 0:\n",
    "                        if cueballPos[\"trial\"][i-window_before_hit] != cueballPos[\"trial\"][start_ind[-1]]:\n",
    "                            start_ind = np.append(start_ind, i-window_before_hit)\n",
    "                        else:\n",
    "                            start_ind = np.append(start_ind, start_movement_ind)\n",
    "                    else:\n",
    "                        start_ind = np.append(start_ind, i-window_before_hit)\n",
    "                    #if cueballPos[\"trial\"][start_ind[i]] == cueballPos[\"trial\"][start_ind[i+1]]:\n",
    "                        #print(\"same trial 2 index\", start_movement, cueballPos[\"trial\"].iloc[i], prev_trial)\n",
    "                    hit_ind = np.append(hit_ind, i+window_after_hit)   #+1 when selecting from trajectory\n",
    "                    start_movement = False\n",
    "                        \n",
    "    if len(start_ind) != 250 or len(hit_ind) != 250:\n",
    "        print(\"WARNING: either missed a start-hit index, or a trial was not properly recorded\")\n",
    "    for i in range(len(start_ind)-1):\n",
    "        if cueballPos[\"trial\"][start_ind[i]] == cueballPos[\"trial\"][start_ind[i+1]]:\n",
    "            print(\"start index \", i, \" and start index \", i+1, \"have same trial number\")\n",
    "        if start_ind[i] >= hit_ind[i]:\n",
    "            raise ValueError(\"start ind > hit_ind\", i , start_ind[i], hit_ind[i])\n",
    "    return start_ind, hit_ind#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_hit_reduced_timesteps(subjData):\n",
    "    \n",
    "    print(\"start hit timesteps function\")\n",
    "    cueballPos = subjData[\"cbpos\"]\n",
    "    cueballvel = subjData[\"cbvel\"]\n",
    "    redballvel = subjData[\"rbvel\"]\n",
    "    cuevel = subjData[\"cuevel\"]\n",
    "    #Find when cue hits cueball\n",
    "    hit_ind=[]\n",
    "    start_ind=[]\n",
    "    n = 10\n",
    "    threshold = 0.06\n",
    "\n",
    "    start_movement = False\n",
    "\n",
    "    prev_trial = 0\n",
    "    start_movement_ind = 0\n",
    "    window_before_hit = 100  #200\n",
    "    window_after_hit = 40   #50\n",
    "\n",
    "    for i in range(len(cueballPos[\"trial\"])):\n",
    "        #The first 200 timesteps approximately are calibration and parasite movements\n",
    "        if i > 200:   #!= 0:\n",
    "            #New trial started\n",
    "            if cueballPos[\"trial\"].iloc[i] > prev_trial:\n",
    "                if static_for_n_timesteps(cueballvel, redballvel, i, n):\n",
    "                    #Wait for cueball vel y-axis and redball vel y-axis to be zero after new trial started\n",
    "                    start_movement_ind = i\n",
    "                    start_movement = True\n",
    "                    prev_trial = cueballPos[\"trial\"].iloc[i]\n",
    "\n",
    "            elif cueballPos[\"trial\"].iloc[i] == prev_trial and start_movement == True:        \n",
    "                if np.linalg.norm(cueballvel[[\"x\", \"z\"]].iloc[i].to_numpy()) > threshold:\n",
    "                    if len(start_ind) > 0:\n",
    "                        if cueballPos[\"trial\"][i-window_before_hit] != cueballPos[\"trial\"][start_ind[-1]]:\n",
    "                            start_ind = np.append(start_ind, i-window_before_hit)\n",
    "                        else:\n",
    "                            start_ind = np.append(start_ind, start_movement_ind)\n",
    "                    else:\n",
    "                        start_ind = np.append(start_ind, i-window_before_hit)\n",
    "                    #if cueballPos[\"trial\"][start_ind[i]] == cueballPos[\"trial\"][start_ind[i+1]]:\n",
    "                        #print(\"same trial 2 index\", start_movement, cueballPos[\"trial\"].iloc[i], prev_trial)\n",
    "                    hit_ind = np.append(hit_ind, i+window_after_hit)   #+1 when selecting from trajectory\n",
    "                    start_movement = False\n",
    "                        \n",
    "    if len(start_ind) != 250 or len(hit_ind) != 250:\n",
    "        print(\"WARNING: either missed a start-hit index, or a trial was not properly recorded\")\n",
    "    for i in range(len(start_ind)-1):\n",
    "        if cueballPos[\"trial\"][start_ind[i]] == cueballPos[\"trial\"][start_ind[i+1]]:\n",
    "            print(\"start index \", i, \" and start index \", i+1, \"have same trial number\")\n",
    "        if start_ind[i] >= hit_ind[i]:\n",
    "            raise ValueError(\"start ind > hit_ind\", i , start_ind[i], hit_ind[i])\n",
    "    return start_ind, hit_ind#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Warning normalization mean and std should be the same for all positions (otherwise substract mean and we have multiple zero locations)\n",
    "def normalize(self, eps = 1e-3):\n",
    "\t\tmean_x = self[\"x\"].mean(0,keepdims=True)\n",
    "\t\tstd_x = self[\"x\"].std(0,keepdims=True) + eps\n",
    "\t\tmean_y = self[\"y\"].mean(0,keepdims=True)\n",
    "\t\tstd_y = self[\"y\"].std(0,keepdims=True) + eps\n",
    "\t\tmean_z = self[\"z\"].mean(0,keepdims=True)\n",
    "\t\tstd_z = self[\"z\"].std(0,keepdims=True) + eps\n",
    "\t\tself[\"x\"] = (self[\"x\"] - mean_x)/std_x\n",
    "\t\tself[\"y\"] = (self[\"y\"] - mean_x)/std_x\n",
    "\t\tself[\"z\"] = (self[\"z\"] - mean_x)/std_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error task\n",
    "\n",
    "def importDataError (initials, path, pocket, lbThreshold = -9.364594, ubThreshold = 4.675092):\n",
    "\n",
    "    '''Imports Behavioural Data for a subject performing Error task\n",
    "\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right corresponding to the pocket\n",
    "    - lbThreshold (float), ubThreshold (float): \n",
    "        lower and upper bounds of the interval of possible angles (out of the interval it will be considered outlier)\n",
    "    \n",
    "    Output:\n",
    "    - errorSubjectData (dict): combinations initials (key) - dataframe (value) for each subject with chosen pocket for error task.\n",
    "        Every entry of the dictionary stores three data frames:\n",
    "            1. VRData: behavioural data from the unity AAB\n",
    "            2. angleData: angle of the shot and corresponding trial numbers (250x2)\n",
    "            3. successData: binary variable for success and corresponding trial numbers (250x2)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Declare possible pocket and raise error otherwise\n",
    "    pockets = ['left', 'right']\n",
    "    if pocket.lower() in pockets:\n",
    "        pocket = pocket.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid AAB type. Expected one of: %s\" % pockets)\n",
    "\n",
    "\n",
    "    ### Definition of ideal angle and funnel of pocketing ###\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "\n",
    "    ### Import angle data ###\n",
    "    angleData = pd.read_csv(path + initials + \"/AAB/\" + initials + \"_Angle.txt\", header=None, names = ['Angle'])\n",
    "    angleData['Block'] = np.repeat(range(1,11),25)\n",
    "    # Remove outliers setting value to nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) > ubThreshold, 'Angle'] = np.nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) < lbThreshold, 'Angle'] = np.nan   \n",
    "\n",
    "\n",
    "    ### Import success data ###\n",
    "    successData = pd.read_csv(path + initials + \"/AAB/\" + initials + \"_Success.txt\",  sep = '\\t', header = None, \\\n",
    "        names = ['Block','Trial','Angle','Magnitude','RBPosition'], usecols = [0,1,2,3,5])\n",
    "\n",
    "\n",
    "\n",
    "    ### Import Behavioural Data from Blocks ###\n",
    "    VRData = pd.DataFrame()\n",
    "\n",
    "    # Merge all blocks together\n",
    "    for bl in range(1,11):\n",
    "        block = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Error\" +  pocket + \"_Block\" + str(bl) + \".txt\", sep = '\\t')\n",
    "        # remove shots after 25 trials (done by mistake)\n",
    "        while block['TrialNumber'].iloc[-1]!=25:\n",
    "            block.drop(block.tail(1).index,inplace=True)\n",
    "\n",
    "        block['TrialNumber'] = block['TrialNumber'] + (bl-1)*25  \n",
    "        VRData = pd.concat([VRData, block])\n",
    "\n",
    "\n",
    "    ### Store data in a dictionary ###\n",
    "    errorSubjectData = {}\n",
    "    names = ['VRData','Angle','Success']\n",
    "    dfs = [VRData, angleData, successData]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        errorSubjectData[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###\n",
    "    return(errorSubjectData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reward Task\n",
    "def importDataReward (initials, path, pocket, lbThreshold = -9.364594, ubThreshold = 4.675092):\n",
    "\n",
    "    '''Imports Behavioural Data for a subject performing Reward task\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right corresponding to the pocket\n",
    "    - lbThreshold (float), ubThreshold (float): \n",
    "        lower and upper bounds of the interval of possible angles (out of the interval it will be considered outlier)\n",
    "    \n",
    "    Output:\n",
    "    - rewardSubjectData (dict): combinations initials (key) - dataframe (value) for each subject with chosen pocket for reward task\n",
    "        Every entry of the dictionary stores four data frames:\n",
    "            1. VRData: behavioural data from the unity game\n",
    "            2. angleData: angle of the shot and corresponding trial numbers (250x2)\n",
    "            3. successData: binary variable for success and corresponding trial numbers (250x2)\n",
    "            4. rewardMotivation: reward motivation (Funnel or Median) and corresponding trial numbers (250x2)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Declare possible pocket and raise error otherwise\n",
    "    pockets = ['left', 'right']\n",
    "    if pocket.lower() in pockets:\n",
    "        pocket = pocket.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % pockets)\n",
    "\n",
    "    ### Definition of ideal angle and funnel ###\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "    ### Import angle data ###\n",
    "    angleData = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Angle.txt\", header=None, names = ['Angle'])\n",
    "    angleData['Block'] = np.repeat(range(1,11),25)\n",
    "    # Remove outliers setting values to nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) > ubThreshold, 'Angle'] = np.nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) < lbThreshold, 'Angle'] = np.nan   \n",
    "\n",
    "    ### Import success data ###\n",
    "    successData = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Success.txt\",  sep = '\\t', header = None, names = ['Block','Trial','Angle','Magnitude','RBPosition'], usecols = [0,1,2,3,5])\n",
    "\n",
    "    ### Import Reward Motivation (funnel or improvement) ###\n",
    "    rewardMotivation = pd.read_csv(path + initials + \"/Game/\" + initials + \"_RewardMotivation.txt\",  sep = '\\t', header = None, names = ['Block', 'Trial','Motivation'])\n",
    "    rewardMotivation['Trial'] = (rewardMotivation['Block']-1)*25 + rewardMotivation['Trial']\n",
    "\n",
    "\n",
    "    ### Import Behavioural Data from Blocks ###\n",
    "    VRData = pd.DataFrame()\n",
    "\n",
    "    for bl in range(1,11):\n",
    "        block = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Reward\" + pocket + \"_Block\" + str(bl) + \".txt\", sep = '\\t')\n",
    "        while block['TrialNumber'].iloc[-1]!=25:\n",
    "            block.drop(block.tail(1).index,inplace=True)\n",
    "        if block['TrialNumber'].iloc[-1]==26:\n",
    "            print(\"block: \", bl,block.tail(1).index)\n",
    "        block['TrialNumber'] = block['TrialNumber'] + (bl-1)*25  \n",
    "        VRData = pd.concat([VRData, block])\n",
    "        #print(\"Block\" + str(bl))\n",
    "\n",
    "    \n",
    "    ### Store data in a dictionary ###\n",
    "    rewardSubjectData = {}\n",
    "    names = ['VRData','Angle','Success','Motivation']\n",
    "    dfs = [VRData, angleData, successData, rewardMotivation]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        rewardSubjectData[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###    \n",
    "    return(rewardSubjectData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "    '''Preprocess data from Pool VR returning position of cue ball, red ball, stick and gaze\n",
    "    \n",
    "    Args:\n",
    "    - dataset (pd.Dataframe): VRData (1st dictionary entry of the output of importDataError or importDataReward)\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): four dataframes for one single subject\n",
    "        1. cbpos: position of the cue ball (X, Y, Z), trial number, time (in the game) and real time (N x 6)\n",
    "        2. rbpos: position of the red ball, trial number, time (in the game) and real time (N x 6)\n",
    "        3. stick: position of the stick, trial number, time (in the game) and real time (N x 6)\n",
    "        4. gaze: position of the gaze, trial number, time (in the game) and real time (N x 6)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    ### Create string variables from 3D vectors ###\n",
    "    \n",
    "    dataset['cueballpos_str'] = dataset['cueballpos'].str.split(',')\n",
    "    dataset['cueballvel_str'] = dataset['cueballvel'].str.split(',')\n",
    "    dataset['redballpos_str'] = dataset['redballpos'].str.split(',')\n",
    "    dataset['redballvel_str'] = dataset['redballvel'].str.split(',')\n",
    "    dataset['cueposfront_str'] = dataset['cueposfront'].str.split(',')\n",
    "    dataset['cueposback_str'] = dataset['cueposback'].str.split(',')\n",
    "    dataset['cuevel_str'] = dataset['cuevel'].str.split(',')\n",
    "    dataset['cuedirection_str'] = dataset['cuedirection'].str.split(',')\n",
    "    dataset['corner5pos_str'] = dataset['corner5pos'].str.split(',')\n",
    "    dataset['corner6pos_str'] = dataset['corner6pos'].str.split(',')\n",
    "    \n",
    "    dataset['stick'] = dataset['optifront'].str.split(',')\n",
    "    dataset['gaze_str'] = dataset['gaze'].str.split(',')\n",
    "\n",
    "    ### Create datasets from string variables ###\n",
    "    cbpos = pd.DataFrame.from_records(np.array(dataset['cueballpos_str']), columns=['x','y','z']).astype(float)\n",
    "    cbvel = pd.DataFrame.from_records(np.array(dataset['cueballvel_str']), columns=['x','y','z']).astype(float)\n",
    "    rbpos = pd.DataFrame.from_records(np.array(dataset['redballpos_str']), columns=['x','y','z']).astype(float)\n",
    "    rbvel = pd.DataFrame.from_records(np.array(dataset['redballvel_str']), columns=['x','y','z']).astype(float)\n",
    "    cueposfront = pd.DataFrame.from_records(np.array(dataset['cueposfront_str']), columns=['x','y','z']).astype(float)\n",
    "    cueposback = pd.DataFrame.from_records(np.array(dataset['cueposback_str']), columns=['x','y','z']).astype(float)\n",
    "    cuedirection = pd.DataFrame.from_records(np.array(dataset['cuedirection_str']), columns=['x','y','z']).astype(float)\n",
    "    cuevel = pd.DataFrame.from_records(np.array(dataset['cuevel_str']), columns=['x','y','z']).astype(float)\n",
    "    corner5pos = pd.DataFrame.from_records(np.array(dataset['corner5pos_str']), columns=['x','y','z']).astype(float)\n",
    "    corner6pos = pd.DataFrame.from_records(np.array(dataset['corner6pos_str']), columns=['x','y','z']).astype(float)\n",
    "\n",
    "    stick = pd.DataFrame.from_records(np.array(dataset['stick']), columns=['x','y','z']).astype(float)\n",
    "    gaze = pd.DataFrame.from_records(np.array(dataset['gaze_str']), columns=['x','y','z']).astype(float)\n",
    "\n",
    "\n",
    "    ### Standardise w.r.t cue ball initial position and add time and trial number ###\n",
    "    x_std, y_std, z_std = cbpos.iloc[0]\n",
    "    for df in (cbpos, rbpos, cueposfront, cueposback, corner5pos, corner6pos, stick, gaze):    # cbvel, rbvel, cuedirection, cuevel, \n",
    "        #df -= (x_std, y_std, z_std)\n",
    "\n",
    "        df['trial'] = np.array(dataset['TrialNumber'])\n",
    "        #df['time'] = np.array(dataset['TrialTime']) \n",
    "        #df['timeReal'] = np.array(dataset['GlobalTime'])  \n",
    "    \n",
    "    #We don't want to standardize the velocities with respect to cue ball initial position\n",
    "    for df in (cbvel, rbvel, cuedirection, cuevel):\n",
    "        df['trial'] = np.array(dataset['TrialNumber'])\n",
    "\n",
    "    ### Create a dictionary for saving dataframes and save them ###\n",
    "    subject = {}\n",
    "    names = ['cbpos', 'cbvel', 'rbpos', 'rbvel', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel', 'corner5pos', 'corner6pos', 'stick','gaze']\n",
    "    dfs = [cbpos, cbvel, rbpos, rbvel, cueposfront, cueposback, cuedirection, cuevel, corner5pos, corner6pos, stick, gaze]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        subject[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###\n",
    "    return(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successTrialsError(subject):\n",
    "\n",
    "    '''Preprocesses and derive successful trials from VR Data correcting the output for error subjects\n",
    "    \n",
    "    Args:\n",
    "    - subject (dict): output of importDataError or importDataReward\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): copy of the input with correct success definition\n",
    "    \n",
    "    '''\n",
    "\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "    \n",
    "    ### Derive Successful trials from Angle dataset\n",
    "    subject['Angle']['Success'] = subject['Angle'].Angle.isin(subject['Success']['Angle']).astype(int)\n",
    "    subject['Angle']['Trial'] = range(1,251)\n",
    "    subject['Angle']['AngleStd'] =  subject['Angle']['Angle'] - idealAngle\n",
    "\n",
    "    gameData = preprocess(subject['VRData'])\n",
    "\n",
    "    succTrials = subject['Angle'].Success.index[subject['Angle'].Success == 1] + 1\n",
    "\n",
    "    for tr in succTrials:\n",
    "        cbTrial = gameData['cbpos'][gameData['cbpos'].trial == tr]\n",
    "\n",
    "        if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n",
    "            subject['Angle'].Success.iloc[tr-1] = 0\n",
    "    print(str(subject['Success'].shape[0]-sum(subject['Angle'].Success)) + \" fake successes removed\")\n",
    "\n",
    "\n",
    "    # Correction if missing successes in perturbation phase for error\n",
    "    df = subject['Angle'][subject['Angle']['Block'] > 3]\n",
    "    df = df[df['Block'] < 10]\n",
    "    if df['Success'].sum()==0:\n",
    "      for tr in range(76, 226):\n",
    "          rbTrial = gameData['rbpos'][gameData['rbpos'].trial == tr]\n",
    "          if (np.abs(np.median(rbTrial['y'].tail(10)) - np.median(rbTrial['y'].head(rbTrial.shape[0]-10))) > 0.02) and (np.abs(np.median(rbTrial['y'].tail(10)) - np.median(rbTrial['y'].head(rbTrial.shape[0]-10))) < 0.5):\n",
    "              subject['Angle'].Success.iloc[tr-1] = 1\n",
    "    \n",
    "    ### Returns preprocessed data with correct success\n",
    "    return(subject)\n",
    "\n",
    "\n",
    "def successTrialsReward(subjData, preprocessData):\n",
    "\n",
    "  '''Preprocesses and derive successful trials from VR Data correcting the output for reward subjects, \\\n",
    "    creating three new variables in the Angle dataset: SuccessMedian (binary variable for success attributable to the median), \\\n",
    "    SuccessFunnel (binary variable for success attributable to the funnel) and Target (reference angle to be rewarded)\n",
    "  \n",
    "    Args:\n",
    "    - subject (dict): output of importDataError or importDataReward\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): copy of the input with correct success definition and three new variables in the Angle dataset\n",
    "   \n",
    "  '''\n",
    "  \n",
    "  print(\"Success Derivation function\")\n",
    "  subject = subjData\n",
    "  ### Set threshold for funnel ###\n",
    "  idealAngle = 105.0736\n",
    "  lb_idealAngle = 104.5401 - idealAngle\n",
    "  ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "  ### Derive Successful trials from Angle dataset ###\n",
    "  ind = subject['Angle']['Block'].isin([1,2,3,10])\n",
    "\n",
    "  ### Create new variables in Angle dataset: success overall, success due to funnel and success due to improvement ###\n",
    "  subject['Angle']['Success'] = 0\n",
    "  subject['Angle']['SuccessFunnel'] = 0\n",
    "  subject['Angle']['SuccessMedian'] = 0\n",
    "\n",
    "  ### Add success to angle dataset ###\n",
    "  subject['Angle']['Success'][ind] = subject['Angle'].Angle[ind].isin(subject['Success']['Angle']).astype(int)\n",
    "\n",
    "  ### Add trial and standardised angle ###\n",
    "  subject['Angle']['Trial'] = range(1,251)\n",
    "  subject['Angle']['AngleStd'] =  subject['Angle']['Angle'] - idealAngle\n",
    "\n",
    "\n",
    "\n",
    "  ### Preprocess game data ###\n",
    "  #gameData = preprocess(subject['VRData'])\n",
    "\n",
    "  ### True Success for baseline blocks ###\n",
    "  succTrials = subject['Angle'].Success.index[subject['Angle'].Success == 1] + 1\n",
    "  \n",
    "  for tr in succTrials:\n",
    "      cbTrial = preprocessData['cbpos'][preprocessData['cbpos'].trial == tr]\n",
    "\n",
    "      if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n",
    "          subject['Angle'].Success.iloc[tr-1] = 0\n",
    "\n",
    "  print(str(succTrials.shape[0]-sum(subject['Angle'].Success)) + \" fake successes removed\")\n",
    "\n",
    "  ### Success for perturbation blocks ###\n",
    "  for tr in subject['Motivation'].Trial:\n",
    "    #subject['Angle']['Success'].iloc[tr] = 1\n",
    "\n",
    "    if (subject['Motivation'].Motivation[subject['Motivation'].Trial == tr] == 'Median').all():\n",
    "      subject['Angle']['SuccessMedian'][tr] = 1\n",
    "    else:\n",
    "      subject['Angle']['SuccessFunnel'][tr] = 1 \n",
    "\n",
    "  ### Derive target for reward (median of the past 10 successful trials) ###\n",
    "\n",
    "  target = []\n",
    "  vec_median = list(subject['Angle']['AngleStd'].iloc[range(66,76)])\n",
    "\n",
    "  for tr in range(76, 226):\n",
    "    if subject['Angle']['Success'].iloc[tr]==1:\n",
    "        target.append(min(ub_idealAngle, max(np.median(vec_median), ub_idealAngle-5)))\n",
    "        vec_median.remove(max(vec_median))\n",
    "        vec_median.append(subject['Angle']['AngleStd'].iloc[tr])\n",
    "    else:\n",
    "        target.append(min(ub_idealAngle, max(np.median(vec_median), ub_idealAngle-5)))\n",
    "\n",
    "  target_overall = pd.Series(np.concatenate([np.repeat(np.nan, 75), target, np.repeat(np.nan, 25)]))\n",
    "\n",
    "  subject['Angle']['Target'] = target_overall\n",
    "  ### Returns preprocessed data with correct success ###\n",
    "  return(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Single Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsSingleSubject(initials, gameType, path, pocket):\n",
    "    '''\n",
    "    Produces results for single subject starting from raw data\n",
    "\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - gameType (str): error/reward - game mode\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right - corresponding to the pocket\n",
    "    \n",
    "    Output:\n",
    "    - outputDict (dict): dictionary with two entries:\n",
    "        1. Game (dict): game data (same output as importDataError/importDataReward) with success corrected\n",
    "            - VRData: data from the Unity Game\n",
    "            - Angle: shot directional angles for all trials\n",
    "            - Success: features of all successful trials\n",
    "            - Motivation (only for reward task): reason of successful trial (if funnel or improvement of the median)\n",
    "\n",
    "        2. PreProcessed (dict): pre-processed game data\n",
    "            - cbpos: position of the cue ball for all timeframes and all trials\n",
    "            - rbpos: position of the red ball for all timeframes and all trials\n",
    "            - stick: position of the stick for all timeframes and all trials\n",
    "            - gaze: position of the gaze\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print(\"Result Single Subject function\")\n",
    "    # Declare possible gameType and raise error otherwise\n",
    "    game_types = ['error', 'reward']\n",
    "    if gameType.lower() in game_types:\n",
    "        mode = gameType.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % game_types)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Error Mode #####\n",
    "    if mode == 'error':\n",
    "\n",
    "        # Import Data\n",
    "        gameData = importDataError(initials, path, pocket) \n",
    "        ''' \n",
    "        gameData is a dictionary with keys:\n",
    "        - VRData: data from the Unity Game\n",
    "        - Angle: shot directional angles for all trials\n",
    "        - Success: features of all successful trials\n",
    "        '''\n",
    "\n",
    "        # Preprocess Data\n",
    "        preprocData = preprocess(gameData['VRData'])\n",
    "        '''\n",
    "        preprocData is a dictionary with keys:\n",
    "        - cbpos: position of the cue ball for all timeframes and all trials\n",
    "        - rbpos: position of the red ball for all timeframes and all trials\n",
    "        - stick: position of the stick for all timeframes and all trials\n",
    "        - gaze: position of the gaze\n",
    "        '''\n",
    "\n",
    "        # Update of Success in Import Data\n",
    "        gameData = successTrialsError(gameData)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Reward Mode #####\n",
    "\n",
    "    elif mode == 'reward':\n",
    "\n",
    "        # Import Data\n",
    "        gameData = importDataReward(initials, path, pocket) \n",
    "        ''' \n",
    "        gameData is a dictionary with keys:\n",
    "        - VRData: data from the Unity Game\n",
    "        - Angle: shot directional angles for all trials\n",
    "        - Success: features of all successful trials\n",
    "        - Motivation: reason of successful trial (if funnel or improvement of the median)\n",
    "        '''\n",
    "\n",
    "        # Preprocess Data\n",
    "        preprocData = preprocess(gameData['VRData'])\n",
    "        '''\n",
    "        preprocData is a dictionary with keys:\n",
    "        - cbpos: position of the cue ball for all timeframes and all trials\n",
    "        - rbpos: position of the red ball for all timeframes and all trials\n",
    "        - stick: position of the stick for all timeframes and all trials\n",
    "        - gaze: position of the gaze\n",
    "        '''\n",
    "\n",
    "        # Update of Success in Import Data\n",
    "        angleData = successTrialsReward(gameData, preprocData)\n",
    "\n",
    "\n",
    "    ### List of outputs ###\n",
    "    outputDict = {}\n",
    "    outputDict['Game'] = gameData \n",
    "    outputDict['PreProcessed'] = preprocData\n",
    "    outputDict['Angle'] = angleData\n",
    "    return(outputDict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Multiple Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsMultipleSubjects(path, sub, gameType, pocketSide):\n",
    "    '''\n",
    "    Args:\n",
    "    - pathList (list): list of all paths of the folder with raw data\n",
    "    - gameType (str): error/reward - game type \n",
    "    - pocketSide (str): left/right - game pocket\n",
    " \n",
    "    '''\n",
    "\t\n",
    "    print(\"result Multiple Subject function\")\n",
    "    # Declare possible gameType and raise error otherwise\n",
    "    game_types = ['error', 'reward']\n",
    "    if gameType.lower() in game_types:\n",
    "        mode = gameType.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % game_types)\n",
    "\n",
    "    # Call dataframe for both rounds together\n",
    "    dataset = {}\n",
    "    \n",
    "    # Declare possible pocketSide and raise error otherwise\n",
    "    pocketChoice = ['left', 'right', 'all']\n",
    "    if pocketSide.lower() in pocketChoice:\n",
    "        pocket = pocketSide.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid pocket. Expected one of: %s\" % pocketChoice)\n",
    "\n",
    "\n",
    "    print(path, sub)\n",
    "    if mode == 'reward':\n",
    "\n",
    "        # Derive path for a specific subject and game type\n",
    "        pathSubj = path + str(sub)\n",
    "        for fil in range(len(sorted(os.listdir(pathSubj + '/Game/')))):\n",
    "            if sorted(os.listdir(pathSubj + '/Game/'))[fil].find(\"Block2\") > -1:\n",
    "                blockFile = sorted(os.listdir(pathSubj + '/Game/'))[fil]\n",
    "\n",
    "\n",
    "        if (blockFile.find('Left') > -1 and pocket != 'right'):\n",
    "            pocketSub = 'Left'\n",
    "        elif (blockFile.find('Right') > -1 and pocket != 'left'):\n",
    "            pocketSub = 'Right'\n",
    "        else:\n",
    "            pocketSub = 'None'\n",
    "\n",
    "        if blockFile.find('Reward') > -1:\n",
    "            # Derive initials of the subject\n",
    "            initials = os.path.basename(pathSubj)\n",
    "\n",
    "            # Derive All Results for One Subject\n",
    "            subjData = resultsSingleSubject(initials, 'reward', path, pocketSub)\n",
    "\n",
    "            # Add data to dataframes previously defined\n",
    "            dataset[str(initials)]={}\n",
    "\n",
    "            dataset[str(initials)][\"rewards\"] = def_reward(subjData[\"Angle\"][\"Angle\"])  #[\"Success\"], subjData[\"Angle\"][\"Angle\"][\"SuccessMedian\"], subjData[\"Angle\"][\"Angle\"][\"SuccessFunnel\"])\n",
    "\n",
    "\n",
    "            dataset[str(initials)][\"cueballpos\"] = subjData[\"PreProcessed\"][\"cbpos\"]\n",
    "            dataset[str(initials)][\"cueballvel\"] = subjData[\"PreProcessed\"][\"cbvel\"]\n",
    "            \n",
    "            dataset[str(initials)][\"start_ind\"], dataset[str(initials)][\"hit_ind\"] = start_hit_reduced_timesteps(subjData[\"PreProcessed\"])  # start_hit_timesteps(subjData[\"PreProcessed\"])     \n",
    "            dataset[str(initials)][\"start_ind\"] = dataset[str(initials)][\"start_ind\"].astype(int)\n",
    "            dataset[str(initials)][\"hit_ind\"] = dataset[str(initials)][\"hit_ind\"].astype(int)\n",
    "            dataset[str(initials)][\"redballpos\"] = subjData[\"PreProcessed\"][\"rbpos\"]\n",
    "            \n",
    "            if pocketSub == 'Left':\n",
    "                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner5pos\"]\n",
    "            else:\n",
    "                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner6pos\"]\n",
    "\n",
    "            dataset[str(initials)][\"cueposfront\"] = subjData[\"PreProcessed\"][\"cueposfront\"]\n",
    "\n",
    "            dataset[str(initials)][\"cueposback\"] = subjData[\"PreProcessed\"][\"cueposback\"]\n",
    "\n",
    "            dataset[str(initials)][\"cuedirection\"] = subjData[\"PreProcessed\"][\"cuedirection\"]\n",
    "\n",
    "            dataset[str(initials)][\"cuevel\"] = subjData[\"PreProcessed\"][\"cuevel\"]\n",
    "\n",
    "            #dataset[str(initials)][\"rewards\"] = def_reward(subjData['Game'][\"Angle\"][\"Success\"], subjData[\"Game\"][\"Angle\"][\"SuccessFunnel\"], subjData[\"Game\"][\"Angle\"][\"SuccessMedian\"])\n",
    "\n",
    "            print(\"Imported \" + initials + \" as reward subject \" + \"for \" + pocketSub + \" pocket\" + \" in round: \" + path)\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Reduced timesteps Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_Reduced(data, cue_data, cuevel, rewards, state_dim=14, action_dim=2):   #state_dim should be a multiple of 6\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = len(data[\"cueballpos\"][\"trial\"]) -1   #len(data[initial][\"start_ind\"])\n",
    "    print(N)\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    new_state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    #action = np.zeros(action_dim)\n",
    "    #reward = np.zeros(1)\n",
    "    #ball_states=6\n",
    "    #cue_states=6\n",
    "    #cue_states_iterations = int((state_dim-ball_states)/(cue_states))\n",
    "\n",
    "    #Action Velocity, Force?\n",
    "    #See car 2D -> action space acceleration and cueDirection\n",
    "    #actions = compute_actions(cuevel[[\"x\", \"z\"]], cue_data[\"cuedirection\"][[\"x\", \"z\"]])\n",
    "    #actions =  np.stack((cue_data[\"cueposfront\"][\"x\"], cue_data[\"cueposfront\"][\"z\"]))#, cue_data[\"cueposback\"][\"x\"], cue_data[\"cueposback\"][\"z\"])) \n",
    "    actions = compute_actions(cue_data, cuevel)\n",
    "    for i in range(N-1):\n",
    "        count=0\n",
    "        for x in data:\n",
    "            state[count] = data[str(x)][\"x\"].iloc[i]\n",
    "            state[count+1] = data[str(x)][\"z\"].iloc[i]\n",
    "            new_state[count] = data[str(x)][\"x\"].iloc[i+1]\n",
    "            new_state[count+1] = data[str(x)][\"z\"].iloc[i+1]\n",
    "            count+=2\n",
    "\n",
    "        #Add last j timesteps of the cuepos to states\n",
    "        #3 observations on x and z axis makes 6 observations\n",
    "        #for j in range(cue_states_iterations): #Warning if state_dim not multiple of 6, division fail for loop\n",
    "        for x in cue_data:\n",
    "            state[count] = cue_data[str(x)][\"x\"].iloc[i]#-j]\n",
    "            state[count+1] = cue_data[str(x)][\"z\"].iloc[i]#-j]\n",
    "            new_state[count] = cue_data[str(x)][\"x\"].iloc[i+1]#-j]\n",
    "            new_state[count+1] = cue_data[str(x)][\"z\"].iloc[i+1]#-j]\n",
    "            count+=2\n",
    "   \n",
    "        #Action Velocity, Force?\n",
    "        action = actions[:,i+1] #action is cuepos at t+1\n",
    "        reward = rewards.iloc[i]\n",
    "        \n",
    "        if data['cueballpos']['trial'].iloc[i+1] != data['cueballpos']['trial'].iloc[i] or i==N-1:\n",
    "            done_bool = True\n",
    "            #reward = rewards.iloc[i]\n",
    "        else:\n",
    "            done_bool = False\n",
    "            #reward=0\n",
    "\n",
    "        trial_.append(data[\"cueballpos\"][\"trial\"].iloc[i])\n",
    "        state_.append(state.copy())\n",
    "        new_state_.append(new_state.copy())\n",
    "        action_.append(action.copy())\n",
    "        reward_.append(reward)\n",
    "        done_.append(done_bool)\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline dict One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_One(data, cue_data, cuevel, rewards, state_dim=24, action_dim=2):   #state_dim should be a multiple of 6\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = len(data[\"cueballpos\"][\"trial\"]) -1   #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    new_state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    action = np.zeros(action_dim)\n",
    "    #reward = np.zeros(1)\n",
    "    #gamma = 0.6\n",
    "    ball_states=6\n",
    "    cue_states=6\n",
    "    cue_states_iterations = int((state_dim-ball_states)/(cue_states))\n",
    "    #Action Velocity, Force?\n",
    "    #See car 2D -> action space acceleration and cueDirection\n",
    "    actions = compute_impulseForce(cuevel[[\"x\", \"z\"]], cue_data[\"cuedirection\"][[\"x\", \"z\"]])\n",
    "        \n",
    "    for i in range(N):\n",
    "        count=0\n",
    "        for x in data:\n",
    "            state[count] = data[str(x)][\"x\"].iloc[i]\n",
    "            state[count+1] = data[str(x)][\"z\"].iloc[i]\n",
    "            new_state[count] = data[str(x)][\"x\"].iloc[i+1]\n",
    "            new_state[count+1] = data[str(x)][\"z\"].iloc[i+1]\n",
    "            count+=2\n",
    "\n",
    "        #Add last j timesteps of the cuepos to states\n",
    "        #3 observations on x and z axis makes 6 observations\n",
    "        for j in range(cue_states_iterations): #Warning if state_dim not multiple of 6, division fail for loop\n",
    "            for x in cue_data:\n",
    "                state[count] = cue_data[str(x)][\"x\"].iloc[i-j]\n",
    "                state[count+1] = cue_data[str(x)][\"z\"].iloc[i-j]\n",
    "                new_state[count] = cue_data[str(x)][\"x\"].iloc[i+1-j]\n",
    "                new_state[count+1] = cue_data[str(x)][\"z\"].iloc[i+1-j]\n",
    "                count+=2\n",
    "   \n",
    "        #Action Velocity, Force?\n",
    "        action = actions[i][:]\n",
    "        #reward = rewards[i]\n",
    "        \n",
    "        if data['cueballpos']['trial'].iloc[i+1] != data['cueballpos']['trial'].iloc[i]:\n",
    "            done_bool = True\n",
    "            reward = rewards.iloc[i]\n",
    "        else:\n",
    "            done_bool = False\n",
    "            reward=0\n",
    "\n",
    "        trial_.append(data[\"cueballpos\"][\"trial\"].iloc[i])\n",
    "        state_.append(state.copy())\n",
    "        new_state_.append(new_state.copy())\n",
    "        action_.append(action.copy())\n",
    "        reward_.append(reward)\n",
    "        done_.append(done_bool)\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }\n",
    "\n",
    "def compute_impulseForce_load(self, cuevel, cuedirection):\n",
    "    impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "    shotMagnitude = np.zeros(1)\n",
    "    shotDir = np.zeros(cuedirection.shape)\n",
    "    #Reward: magnitude range\n",
    "    lbMagnitude = 0.4   #0.516149\n",
    "    ubMagnitude = 0.882607\n",
    "\n",
    "    shotMagnitude = np.linalg.norm(cuevel, axis=1)\n",
    "\n",
    "    for i in range(cuevel.shape[0]):\n",
    "        if shotMagnitude[i] > ubMagnitude:\n",
    "            shotMagnitude[i] = ubMagnitude\n",
    "            #print(\"upper bounded\")\n",
    "        #elif shotMagnitude[i] > lbMagnitude:\n",
    "            #print(i, shotMagnitude[i])\n",
    "        elif shotMagnitude[i] < lbMagnitude:\n",
    "            shotMagnitude[i] = 0\n",
    "\n",
    "        shotDir[i][:] = cuedirection[i][:]\n",
    "        if shotMagnitude[i] == 0:\n",
    "            impulseForce[i][:] = 0\n",
    "        else:\n",
    "            impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "    return impulseForce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL One dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_One_big_dict(data,initial, terminate_on_end=False):\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = data[initial][\"cueballpos\"][\"trial\"].iloc[-1]    #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(14)  #np.zeros(21)\n",
    "    new_state = np.zeros(14)  #np.zeros(21)\n",
    "    action = np.zeros(2)\n",
    "    reward = np.zeros(1)\n",
    "    #gamma = 0.6\n",
    "    episode_step=0\n",
    "    for i in range(N):\n",
    "        for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]+1, 1):\n",
    "            #state timestep t\n",
    "            state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j] \n",
    "            #state[0] = data[initial][\"cueballpos\"][y].iloc[j] \n",
    "            state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j] \n",
    "            state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j] \n",
    "            #state[2] = data[initial][\"redballpos\"][y].iloc[j]\n",
    "            state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j]\n",
    "            #state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j]\n",
    "            state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j]\n",
    "            state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j]\n",
    "            #state[6] = data[initial][\"cueposfront\"][y].iloc[j]\n",
    "            state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j]\n",
    "            state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j]\n",
    "            #state[8] = data[initial][\"cueposback\"][y].iloc[j]\n",
    "            state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j]\n",
    "            state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j]\n",
    "            #state[10] = data[initial][\"cueDirection\"][y].iloc[j]\n",
    "            state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j]\n",
    "            state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j]\n",
    "            state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j]\n",
    "\n",
    "            #state Timestep t+1\n",
    "            new_state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[0] = data[initial][\"cueballpos\"][y].iloc[j+1] \n",
    "            new_state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j+1] \n",
    "            new_state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[2] = data[initial][\"redballpos\"][y].iloc[j+1]\n",
    "            new_state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j+1]\n",
    "            #new_state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j+1]\n",
    "            new_state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j+1]\n",
    "            #new_state[6] = data[initial][\"cueposfront\"][y].iloc[j+1]\n",
    "            new_state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j+1]\n",
    "            new_state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j+1]\n",
    "            #new_state[8] = data[initial][\"cueposback\"][y].iloc[j+1]\n",
    "            new_state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j+1]\n",
    "            new_state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j+1]\n",
    "            #new_state[10] = data[initial][\"cueDirection\"][y].iloc[j+1]\n",
    "            new_state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j+1]\n",
    "            new_state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j+1]\n",
    "            new_state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j+1]\n",
    "\n",
    "            #Action Velocity, Force?\n",
    "            action = compute_impulseForce(data[initial][\"cuevel\"][[\"x\", \"z\"]].iloc[j], data[initial][\"cuedirection\"][[\"x\", \"z\"]].iloc[j])\n",
    "            #action[0] = data[initial][\"cuevel\"][\"y\"].iloc[j]\n",
    "            \n",
    "\n",
    "            if j == data[initial][\"hit_ind\"][i]:\n",
    "                done_bool = True\n",
    "                reward = data[initial][\"rewards\"][i]\n",
    "            else:\n",
    "                done_bool = False\n",
    "                ## Discounted reward ##\n",
    "                #reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "                reward = 0\n",
    "\n",
    "            final_timestep = (episode_step == data[initial][\"hit_ind\"][i]-1)\n",
    "            if (not terminate_on_end) and final_timestep:\n",
    "                # Skip this transition and don't apply terminals on the last step of an episode\n",
    "                episode_step = 0\n",
    "                continue\n",
    "            if done_bool or final_timestep:\n",
    "                episode_step = 0\n",
    "\n",
    "            trial_.append(data[initial][\"cueballpos\"][\"trial\"].iloc[j] )\n",
    "            state_.append(state)\n",
    "            new_state_.append(new_state)\n",
    "            action_.append(action)\n",
    "            reward_.append(reward)\n",
    "            done_.append(done_bool)\n",
    "            episode_step += 1\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline load class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Offline_RL_dataset(object): #rewards,cueballpos,redballpos, targetcornerpos, cueposfront, cueposback, cuedirection, cuevel,\n",
    "    def __init__(self, state_dim=14, action_dim=2, nb_trial=250):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #number of trial\n",
    "        self.N = nb_trial    #int(cueballpos[0].iloc[-1])\n",
    "        self.state_dim = state_dim  \n",
    "        self.action_dim = action_dim\n",
    "        self.trajectories = []\n",
    "        \n",
    "        self.mean = np.zeros(self.state_dim)\n",
    "        self.std = np.zeros(self.state_dim)\n",
    "\n",
    "    def get_trajectories(self, list_data, rewards):\n",
    "        \n",
    "        state_ = []\n",
    "        new_state_ = []\n",
    "        action_ = []\n",
    "        reward_ = []\n",
    "        done_ = []\n",
    "\n",
    "        state = np.zeros(self.state_dim)\n",
    "        new_state = np.zeros(self.state_dim) \n",
    "        action = np.zeros(self.action_dim)\n",
    "        reward = np.zeros(1)\n",
    "\n",
    "        actions = self.compute_impulseForce_load(np.transpose(np.array([list_data[\"cuevel\"][1], list_data[\"cuevel\"][3]])), np.transpose(np.array([list_data[\"cuedirection\"][1], list_data[\"cuedirection\"][3]])))\n",
    "\n",
    "        for i in range(len(list_data[\"cueballpos\"])-1):\n",
    "            count=0\n",
    "            for x in list_data:\n",
    "                state[count] = list_data[str(x)][1].iloc[i]\n",
    "                state[count+1] = list_data[str(x)][3].iloc[i]\n",
    "                new_state[count] = list_data[str(x)][1].iloc[i+1]\n",
    "                new_state[count+1] = list_data[str(x)][3].iloc[i+1]\n",
    "                count+=2\n",
    "            #Action Velocity, Force?\n",
    "            action = actions[i][:]\n",
    "            reward = rewards.iloc[i]\n",
    "            \n",
    "            if cueballpos[0].iloc[i+1] != cueballpos[0].iloc[i]:\n",
    "                done_bool = True\n",
    "            else:\n",
    "                done_bool = False\n",
    "\n",
    "            state_.append(state.copy())\n",
    "            new_state_.append(new_state.copy())\n",
    "            action_.append(action.copy())\n",
    "            reward_.append(reward.copy())\n",
    "            done_.append(done_bool)\n",
    "\n",
    "            if list_data[\"cueballpos\"][0].iloc[i+1] != list_data[\"cueballpos\"][0].iloc[i]:\n",
    "                self.trajectories.append({\n",
    "                'size': np.array(state_).shape[0],\n",
    "                'states': np.array(state_),\n",
    "                'actions': np.array(action_),\n",
    "                'new_states': np.array(new_state_),\n",
    "                'rewards': np.array(reward_),\n",
    "                'terminals': np.array(done_),\n",
    "                })\n",
    "\n",
    "    def sample(self):   #, batch_size = 4):   #bacth size is the number of trajectory processed before gradient update\n",
    "        max_size = self.N\n",
    "        ind = 1 #np.random.randint(0, max_size)    #, size=batch_size)\n",
    "        return (torch.FloatTensor(self.trajectories[ind]['size']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['states']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['actions']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['new_states']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['rewards']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['terminals']).to(self.device))\n",
    "\n",
    "    def compute_impulseForce_load(self, cuevel, cuedirection):\n",
    "        impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "        shotMagnitude = np.zeros(1)\n",
    "        shotDir = np.zeros(cuedirection.shape)\n",
    "        #Reward: magnitude range\n",
    "        lbMagnitude = 0.4   #0.516149\n",
    "        ubMagnitude = 0.882607\n",
    "\n",
    "        shotMagnitude = np.linalg.norm(cuevel, axis=1)\n",
    "\n",
    "        for i in range(cuevel.shape[0]):\n",
    "            if shotMagnitude[i] > ubMagnitude:\n",
    "                shotMagnitude[i] = ubMagnitude\n",
    "                #print(\"upper bounded\")\n",
    "            #elif shotMagnitude[i] > lbMagnitude:\n",
    "                #print(i, shotMagnitude[i])\n",
    "            elif shotMagnitude[i] < lbMagnitude:\n",
    "                shotMagnitude[i] = 0\n",
    "\n",
    "            shotDir[i][:] = cuedirection[i][:]\n",
    "            if shotMagnitude[i] == 0:\n",
    "                impulseForce[i][:] = 0\n",
    "            else:\n",
    "                impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "        return impulseForce\n",
    "\n",
    "    def compute_mean_std(self, list_data, eps = 1e-3):\n",
    "        count = 0\n",
    "        for i,x in enumerate(list_data):\n",
    "            self.mean[count] = list_data[str(x)][1].mean(0)  #, keepdim=True)\n",
    "            self.mean[count+1] = list_data[str(x)][3].mean(0)\n",
    "\n",
    "            self.std[count] = list_data[str(x)][1].std(0)  #, keepdim=True)\n",
    "            self.std[count+1] = list_data[str(x)][3].std(0)\n",
    "            count += 2\n",
    "\n",
    "    def normalize_states(self, eps = 1e-3):\n",
    "        for i in range(2):  #self.N\n",
    "            for j in range(self.state_dim):\n",
    "                self.trajectories[i]['states'][:][j] = (self.trajectories[i]['states'][:][j] - self.mean[j])/self.std[j]\n",
    "                self.trajectories[i]['new_states'][:][j] = (self.trajectories[i]['new_states'][:][j] - self.mean[j])/self.std[j]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_RL_dataset(data,initial, terminate_on_end=False):\n",
    "    \"\"\"\n",
    "        Returns datasets formatted for use by standard Q-learning algorithms,\n",
    "        with states, actions, next_states, rewards, and a terminal\n",
    "        flag.\n",
    "\n",
    "        Args:\n",
    "            cueballPos, redballPos, targetcornerPos, cuePosfront, cuePosback: Recorded stateervation States (for each subject)\n",
    "            cueVel: Recorded Action new_state (for each Subject)\n",
    "            reward: np.array with one reward per trial\n",
    "            start_ind: starting index of each trial\n",
    "            hit_ind: hitting the cue ball index for each trial\n",
    "            target_corner: string indicating the corner in which to pocket\n",
    "            terminate_on_end (bool): Set done=True on the last timestep\n",
    "            in a trajectory. Default is False, and will discard the\n",
    "            last timestep in each trajectory.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing keys:\n",
    "                states: An N x dim_state array of states.\n",
    "                actions: An N x dim_action array of actions.\n",
    "                next_states: An N x dim_state array of next states.\n",
    "                rewards: An N-dim float array of rewards.\n",
    "                terminals: An N-dim boolean array of \"done\" or episode termination flags.\n",
    "        \"\"\"\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = data[\"AAB\"][\"cueballpos\"][\"trial\"].iloc[-1]    #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "\n",
    "    state = np.zeros(14)  #np.zeros(21)\n",
    "    new_state = np.zeros(14)  #np.zeros(21)\n",
    "    action = np.zeros(2)\n",
    "    reward = np.zeros(1)\n",
    "    gamma = 0.6 #discounted reward\n",
    "    # The newer version of the dataset adds an explicit\n",
    "    # timeouts field. Keep old method for backwards compatability.\n",
    "    #use_timeouts = False\n",
    "    #if 'timeouts' in dataset:\n",
    "        #use_timeouts = True\n",
    "    \n",
    "    '''terminate_on_end (bool): Set done=True on the last timestep\n",
    "        in a trajectory. Default is False, and will discard the\n",
    "        last timestep in each trajectory.'''\n",
    "\n",
    "    episode_step = 0\n",
    "    trajectories = []\n",
    "    for i in range(N):\n",
    "        \"\"\"\n",
    "        episode = {'true_state': [],\n",
    "                   'true_next_state': [],\n",
    "                   'x': [],\n",
    "                   'a': [],\n",
    "                   'r': [],\n",
    "                   'x_prime': [],\n",
    "                   'done': [],\n",
    "                   'base_propensity': [],\n",
    "                   'target_propensity': [],\n",
    "                   'frames': [],\n",
    "                   'extra_propensity': []}\n",
    "        \"\"\"\n",
    "        for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]+1, 1):\n",
    "            #state timestep t\n",
    "            state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j] \n",
    "            #state[0] = data[initial][\"cueballpos\"][y].iloc[j] \n",
    "            state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j] \n",
    "            state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j] \n",
    "            #state[2] = data[initial][\"redballpos\"][y].iloc[j]\n",
    "            state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j]\n",
    "            state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j]\n",
    "            state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j]\n",
    "            #state[6] = data[initial][\"cueposfront\"][y].iloc[j]\n",
    "            state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j]\n",
    "            state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j]\n",
    "            #state[8] = data[initial][\"cueposback\"][y].iloc[j]\n",
    "            state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j]\n",
    "            state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j]\n",
    "            #state[10] = data[initial][\"cueDirection\"][y].iloc[j]\n",
    "            state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j]\n",
    "            state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j]\n",
    "            state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j]\n",
    "\n",
    "            #state Timestep t+1\n",
    "            new_state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[0] = data[initial][\"cueballpos\"][y].iloc[j+1] \n",
    "            new_state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j+1] \n",
    "            new_state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[2] = data[initial][\"redballpos\"][y].iloc[j+1]\n",
    "            new_state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j+1]\n",
    "            new_state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j+1]\n",
    "            #new_state[6] = data[initial][\"cueposfront\"][y].iloc[j+1]\n",
    "            new_state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j+1]\n",
    "            new_state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j+1]\n",
    "            #new_state[8] = data[initial][\"cueposback\"][y].iloc[j+1]\n",
    "            new_state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j+1]\n",
    "            new_state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j+1]\n",
    "            #new_state[10] = data[initial][\"cueDirection\"][y].iloc[j+1]\n",
    "            new_state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j+1]\n",
    "            new_state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j+1]\n",
    "            new_state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j+1]\n",
    "\n",
    "            #Action Velocity, Force?\n",
    "            action = compute_impulseForce(data[initial][\"cuevel\"][[\"x\", \"z\"]].iloc[j], data[initial][\"cuedirection\"][[\"x\", \"z\"]].iloc[j])\n",
    "            #action[0] = data[initial][\"cuevel\"][\"y\"].iloc[j]\n",
    "            \n",
    "\n",
    "            if j == data[initial][\"hit_ind\"][i]:\n",
    "                done_bool = True\n",
    "                reward = data[initial][\"rewards\"][i]\n",
    "            else:\n",
    "                done_bool = False\n",
    "                ## Discounted reward ##\n",
    "                gamma = 0.9\n",
    "                reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "                #reward = 0\n",
    "\n",
    "            final_timestep = (episode_step == data[initial][\"hit_ind\"][i]-1)\n",
    "            if (not terminate_on_end) and final_timestep:\n",
    "                # Skip this transition and don't apply terminals on the last step of an episode\n",
    "                episode_step = 0\n",
    "                continue\n",
    "            if done_bool or final_timestep:\n",
    "                episode_step = 0\n",
    "            print(\"appending episode: \", j)\n",
    "            state_.append(state)\n",
    "            new_state_.append(new_state)\n",
    "            action_.append(action)\n",
    "            reward_.append(reward)\n",
    "            done_.append(done_bool)\n",
    "            episode_step += 1\n",
    "        \n",
    "        print(\"trajectory: \", i)\n",
    "        trajectories.append({\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_state': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        })\n",
    "    \n",
    "    return trajectories\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'states': state_,\n",
    "        'actions': action_,\n",
    "        'next_states': next_state_,\n",
    "        'rewards': reward_,\n",
    "        'terminals': done_,\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'next_states': np.array(next_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef convert_D4RL(self, dataset):\n",
    "\t\tself.state = np.array(dataset['states'])\n",
    "\t\tself.action = np.array(dataset['actions'])\n",
    "\t\tself.next_state = np.array(dataset['next_states'])\n",
    "\t\tself.reward = np.array(dataset['rewards']).reshape(-1,1)\n",
    "\t\tself.not_done = 1. - np.array(dataset['terminals']).reshape(-1,1)\n",
    "\t\tself.size = self.state.shape[0]\n",
    "\n",
    "\n",
    "\tdef normalize_states(self, eps = 1e-3):\n",
    "\t\tmean = self.state.mean(0,keepdims=True)\n",
    "\t\tstd = self.state.std(0,keepdims=True) + eps\n",
    "\t\tself.state = (self.state - mean)/std\n",
    "\t\tself.next_state = (self.next_state - mean)/std\n",
    "\t\treturn mean, std\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\t\t\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n",
    "\n",
    "\n",
    "class TD3_BC(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tdiscount=0.99,\n",
    "\t\ttau=0.005,\n",
    "\t\tpolicy_noise=0.2,\n",
    "\t\tnoise_clip=0.5,\n",
    "\t\tpolicy_freq=2,\n",
    "\t\talpha=2.5,\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\t\tself.policy_noise = policy_noise\n",
    "\t\tself.noise_clip = noise_clip\n",
    "\t\tself.policy_freq = policy_freq\n",
    "\t\tself.alpha = alpha\n",
    "\n",
    "\t\tself.total_it = 0\n",
    "\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=256):\n",
    "\t\tself.total_it += 1\n",
    "\n",
    "\t\t# Sample replay buffer \n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Select action according to policy and add clipped noise\n",
    "\t\t\tnoise = (\n",
    "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
    "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
    "\t\t\t\n",
    "\t\t\tnext_action = (\n",
    "\t\t\t\tself.actor_target(next_state) + noise\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t\t# Compute the target Q value\n",
    "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t#if (self.total_it) % 5 == 0:\n",
    "\t\t\t#print(\"iteration \", self.total_it, \" critic_loss: \", critic_loss)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "\t\tif self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor loss\n",
    "\t\t\tpi = self.actor(state)\n",
    "\t\t\tQ = self.critic.Q1(state, pi)\n",
    "\t\t\tlmbda = self.alpha/Q.abs().mean().detach()\n",
    "\n",
    "\t\t\tactor_loss = -lmbda * Q.mean() + F.mse_loss(pi, action) \n",
    "\t\t\t\n",
    "\t\t\t#print(\"iteration \", self.total_it, \" actor_loss: \", actor_loss)\n",
    "\n",
    "\t\t\t# Optimize the actor \n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "\tdef save(self, filename):\n",
    "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\t\t\n",
    "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "\tdef load(self, filename):\n",
    "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        log_prob = (dist.log_prob(e) - torch.log(1 - action.pow(2) + epsilon)).sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
    "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
    "        \"\"\"\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        return action.detach().cpu()\n",
    "    \n",
    "    def get_det_action(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        return torch.tanh(mu).detach().cpu()\n",
    "\n",
    "class DeepActor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, device, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepActor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.device = device\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        in_dim = hidden_size+state_size\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(in_dim, hidden_size)\n",
    "        self.fc3 = nn.Linear(in_dim, hidden_size)\n",
    "        self.fc4 = nn.Linear(in_dim, hidden_size)\n",
    "\n",
    "\n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "        #self.reset_parameters() # check if this improves training\n",
    "\n",
    "    def reset_parameters(self, init_w=3e-3):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.mu.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state: torch.tensor):\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc4(x))  \n",
    "\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        log_prob = (dist.log_prob(e) - torch.log(1 - action.pow(2) + epsilon)).sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
    "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
    "        \"\"\"\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        return action.detach().cpu()\n",
    "    \n",
    "    def get_det_action(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        return torch.tanh(mu).detach().cpu()\n",
    "    \n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256, seed=1, N=32, device=\"cuda:0\"):\n",
    "        super(IQN, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = hidden_size\n",
    "        self.pis = torch.FloatTensor([np.pi * i for i in range(1, self.n_cos + 1)]).view(1, 1, self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "        self.head = nn.Linear(self.action_size + self.input_shape, hidden_size) \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, hidden_size)\n",
    "        self.ff_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ff_2 = nn.Linear(hidden_size, 1)    \n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device)\n",
    "        cos = torch.cos(taus * self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        x = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(x  ))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication > reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1) * cos_x).view(batch_size * num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        #action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        #x = torch.cat((x,action),dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n",
    "\n",
    "class DeepIQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, device=\"cuda:0\"):\n",
    "        super(DeepIQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.input_dim = action_size+state_size+layer_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.ff_1 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_2 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_3 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_4 = nn.Linear(self.layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])  \n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        xs = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(xs))\n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_1(x))   \n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_2(x))\n",
    "\n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication > reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        state = input.repeat(num_tau,1).reshape(num_tau,batch_size*self.input_shape).transpose(0,1).reshape(batch_size*num_tau,self.input_shape)\n",
    "        \n",
    "        x = torch.cat((x,action,state),dim=1)\n",
    "        x = torch.relu(self.ff_3(x))\n",
    "\n",
    "        out = self.ff_4(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n",
    "\n",
    "\n",
    "\n",
    "class CQLSAC(nn.Module):\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                        state_size,\n",
    "                        action_size,\n",
    "                        device\n",
    "                ):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        super(CQLSAC, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.tau = 1e-2\n",
    "        hidden_size = 256\n",
    "        learning_rate = 5e-4\n",
    "        self.clip_grad_param = 1\n",
    "\n",
    "        self.target_entropy = -action_size  # -dim(A)\n",
    "\n",
    "        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "        self.alpha_optimizer = optim.Adam(params=[self.log_alpha], lr=learning_rate) \n",
    "        \n",
    "        # CQL params\n",
    "        self.with_lagrange = False\n",
    "        self.temp = 1.0\n",
    "        self.cql_weight = 1.0\n",
    "        self.target_action_gap = 0.0\n",
    "        self.cql_log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.cql_alpha_optimizer = optim.Adam(params=[self.cql_log_alpha], lr=learning_rate) \n",
    "        \n",
    "        # Actor Network \n",
    "\n",
    "        self.actor_local = Actor(state_size, action_size, hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=learning_rate)     \n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "\n",
    "        self.critic1 = IQN(state_size, action_size, hidden_size, seed=1).to(device)\n",
    "        self.critic2 = IQN(state_size, action_size, hidden_size, seed=2).to(device)\n",
    "        \n",
    "        assert self.critic1.parameters() != self.critic2.parameters()\n",
    "        \n",
    "        self.critic1_target = IQN(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "\n",
    "        self.critic2_target = IQN(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=learning_rate) \n",
    "\n",
    "    def get_action(self, state, eval=False):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if eval:\n",
    "                action = self.actor_local.get_det_action(state)\n",
    "            else:\n",
    "                action = self.actor_local.get_action(state)\n",
    "        return action.numpy()\n",
    "\n",
    "    def calc_policy_loss(self, states, alpha):\n",
    "        actions_pred, log_pis = self.actor_local.evaluate(states)\n",
    "\n",
    "        q1 = self.critic1.get_qvalues(states, actions_pred.squeeze(0))   \n",
    "        q2 = self.critic2.get_qvalues(states, actions_pred.squeeze(0))\n",
    "        min_Q = torch.min(q1,q2).cpu()\n",
    "        actor_loss = ((alpha * log_pis.cpu() - min_Q )).mean()\n",
    "        return actor_loss, log_pis\n",
    "\n",
    "    def _compute_policy_values(self, state_pi, state_q):\n",
    "        with torch.no_grad():\n",
    "            actions_pred, log_pis = self.actor_local.evaluate(state_pi)\n",
    "        \n",
    "        qs1 = self.critic1.get_qvalues(state_q, actions_pred)\n",
    "        qs2 = self.critic2.get_qvalues(state_q, actions_pred)\n",
    "        \n",
    "        return qs1-log_pis, qs2-log_pis\n",
    "    \n",
    "    def _compute_random_values(self, state, actions, critic):\n",
    "        random_values = critic.get_qvalues(state, actions)\n",
    "        random_log_prstate = math.log(0.5 ** self.action_size)\n",
    "        return random_values - random_log_prstate\n",
    "    \n",
    "    def train(self, step, experiences, gamma, d=1):\n",
    "        \"\"\"Updates actor, critics and entropy_alpha parameters using given batch of experience tuples.\n",
    "        Q_targets = r +  * (min_critic_target(next_state, actor_target(next_state)) -  *log_pi(next_action|next_state))\n",
    "        Critic_loss = MSE(Q, Q_target)\n",
    "        Actor_loss =  * log_pi(a|s) - Q(s,a)\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        current_alpha = copy.deepcopy(self.alpha)\n",
    "        actor_loss, log_pis = self.calc_policy_loss(states, current_alpha)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Compute alpha loss\n",
    "        alpha_loss = - (self.log_alpha.exp() * (log_pis.cpu() + self.target_entropy).detach().cpu()).mean()\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        with torch.no_grad():\n",
    "            next_action, _ = self.actor_local.evaluate(next_states)\n",
    "            #next_action = next_action.unsqueeze(1).repeat(1, 10, 1).view(next_action.shape[0] * 10, next_action.shape[1])\n",
    "            #temp_next_states = next_states.unsqueeze(1).repeat(1, 10, 1).view(next_states.shape[0] * 10, next_states.shape[1])\n",
    "            \n",
    "            Q_target1_next, _ = self.critic1_target(next_states, next_action) #.view(states.shape[0], 10, 1).max(1)[0].view(-1, 1)\n",
    "            # batch_size, num_tau, 1    \n",
    "            Q_target2_next, _ = self.critic2_target(next_states, next_action) #.view(states.shape[0], 10, 1).max(1)[0].view(-1, 1)\n",
    "            Q_target_next = torch.min(Q_target1_next, Q_target2_next).transpose(1,2)\n",
    "\n",
    "            # Compute Q targets for current states (y_i)\n",
    "            Q_targets = rewards.cpu().unsqueeze(-1) + (gamma * (1 - dones.cpu().unsqueeze(-1)) * Q_target_next.cpu()) \n",
    "\n",
    "\n",
    "        # Compute critic loss\n",
    "        q1, taus1 = self.critic1(states, actions)\n",
    "        q2, taus2 = self.critic2(states, actions)\n",
    "        assert Q_targets.shape == (256, 1, 32), \"have shape: {}\".format(Q_targets.shape)\n",
    "        assert q1.shape == (256, 32, 1)\n",
    "        \n",
    "        # Quantile Huber loss\n",
    "        td_error1 = Q_targets - q1.cpu()\n",
    "        td_error2 = Q_targets - q2.cpu()\n",
    "        \n",
    "        assert td_error1.shape == (256, 32, 32), \"wrong td error shape\"\n",
    "        huber_l_1 = calculate_huber_loss(td_error1, 1.0)\n",
    "        huber_l_2 = calculate_huber_loss(td_error2, 1.0)\n",
    "        \n",
    "        quantil_l_1 = abs(taus1.cpu() - (td_error1.detach() < 0).float()) * huber_l_1 / 1.0\n",
    "        quantil_l_2 = abs(taus2.cpu() - (td_error2.detach() < 0).float()) * huber_l_2 / 1.0\n",
    "\n",
    "        critic1_loss = quantil_l_1.sum(dim=1).mean(dim=1).mean()\n",
    "        critic2_loss = quantil_l_2.sum(dim=1).mean(dim=1).mean()\n",
    "\n",
    "        \n",
    "        # CQL addon\n",
    "\n",
    "        random_actions = torch.FloatTensor(q1.shape[0] * 10, actions.shape[-1]).uniform_(-1, 1).to(self.device)\n",
    "        num_repeat = int (random_actions.shape[0] / states.shape[0])\n",
    "        temp_states = states.unsqueeze(1).repeat(1, num_repeat, 1).view(states.shape[0] * num_repeat, states.shape[1])\n",
    "        temp_next_states = next_states.unsqueeze(1).repeat(1, num_repeat, 1).view(next_states.shape[0] * num_repeat, next_states.shape[1])\n",
    "        \n",
    "        current_pi_values1, current_pi_values2  = self._compute_policy_values(temp_states, temp_states)\n",
    "        next_pi_values1, next_pi_values2 = self._compute_policy_values(temp_next_states, temp_states)\n",
    "        \n",
    "        random_values1 = self._compute_random_values(temp_states, random_actions, self.critic1).reshape(states.shape[0], num_repeat, 1)\n",
    "        random_values2 = self._compute_random_values(temp_states, random_actions, self.critic2).reshape(states.shape[0], num_repeat, 1)\n",
    "\n",
    "        current_pi_values1 = current_pi_values1.reshape(states.shape[0], num_repeat, 1)\n",
    "        current_pi_values2 = current_pi_values2.reshape(states.shape[0], num_repeat, 1)\n",
    "        next_pi_values1 = next_pi_values1.reshape(states.shape[0], num_repeat, 1)\n",
    "        next_pi_values2 = next_pi_values2.reshape(states.shape[0], num_repeat, 1)      \n",
    "        \n",
    "        cat_q1 = torch.cat([random_values1, current_pi_values1, next_pi_values1], 1)\n",
    "        cat_q2 = torch.cat([random_values2, current_pi_values2, next_pi_values2], 1)\n",
    "        \n",
    "        assert cat_q1.shape == (states.shape[0], 3 * num_repeat, 1), f\"cat_q1 instead has shape: {cat_q1.shape}\"\n",
    "        assert cat_q2.shape == (states.shape[0], 3 * num_repeat, 1), f\"cat_q2 instead has shape: {cat_q2.shape}\"\n",
    "        \n",
    "\n",
    "        cql1_scaled_loss = (torch.logsumexp(cat_q1 / self.temp, dim=1).mean() * self.cql_weight * self.temp - q1.mean()) * self.cql_weight\n",
    "        cql2_scaled_loss = (torch.logsumexp(cat_q2 / self.temp, dim=1).mean() * self.cql_weight * self.temp - q2.mean()) * self.cql_weight\n",
    "        \n",
    "        cql_alpha_loss = torch.FloatTensor([0.0])\n",
    "        cql_alpha = torch.FloatTensor([0.0])\n",
    "        if self.with_lagrange:\n",
    "            cql_alpha = torch.clamp(self.cql_log_alpha.exp(), min=0.0, max=1000000.0).to(self.device)\n",
    "            cql1_scaled_loss = cql_alpha * (cql1_scaled_loss - self.target_action_gap)\n",
    "            cql2_scaled_loss = cql_alpha * (cql2_scaled_loss - self.target_action_gap)\n",
    "\n",
    "            self.cql_alpha_optimizer.zero_grad()\n",
    "            cql_alpha_loss = (- cql1_scaled_loss - cql2_scaled_loss) * 0.5 \n",
    "            cql_alpha_loss.backward(retain_graph=True)\n",
    "            self.cql_alpha_optimizer.step()\n",
    "        \n",
    "        total_c1_loss = critic1_loss + cql1_scaled_loss\n",
    "        total_c2_loss = critic2_loss + cql2_scaled_loss\n",
    "        \n",
    "        \n",
    "        # Update critics\n",
    "        # critic 1\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        total_c1_loss.backward(retain_graph=True)\n",
    "        clip_grad_norm_(self.critic1.parameters(), self.clip_grad_param)\n",
    "        self.critic1_optimizer.step()\n",
    "        # critic 2\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        total_c2_loss.backward()\n",
    "        clip_grad_norm_(self.critic2.parameters(), self.clip_grad_param)\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic1, self.critic1_target)\n",
    "        self.soft_update(self.critic2, self.critic2_target)\n",
    "        \n",
    "        return actor_loss.item(), alpha_loss.item(), critic1_loss.item(), critic2_loss.item(), cql1_scaled_loss.item(), cql2_scaled_loss.item(), current_alpha, cql_alpha_loss.item(), cql_alpha.item()\n",
    "\n",
    "    def soft_update(self, local_model , target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        _target = *_local + (1 - )*_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    assert loss.shape == (td_errors.shape[0], 32, 32), \"huber loss has wrong shape\"\n",
    "    return loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "        # Experiment\n",
    "        \"policy\": \"TD3_BC\",\n",
    "        \"seed\": 0, \n",
    "        \"eval_freq\": 5e3,\n",
    "        \"max_timesteps\": 250,   #1e6,\n",
    "        \"save_model\": \"store_true\",\n",
    "        \"load_model\": \"\",                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "\t    # TD3\n",
    "\t    \"expl_noise\": 0.1,\n",
    "        \"batch_size\": 256,\n",
    "        \"discount\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"policy_noise\": 0.2,\n",
    "        \"noise_clip\": 0.5,\n",
    "        \"policy_freq\": 2,\n",
    "        # TD3 + BC\n",
    "\t    \"alpha\": 2.5,\n",
    "        \"normalize\": True,\n",
    "        \"state_dim\": 18,\n",
    "\t\t\"action_dim\": 3,\n",
    "\t\t\"max_action\": 1,\n",
    "\t\t\"discount\": 0.99,\n",
    "\t\t\"tau\": 0.005,\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "        \"state_dim\": 18,\n",
    "\t\t\"action_dim\": 3,\n",
    "\t\t\"max_action\": 1,\n",
    "\t\t\"discount\": 0.99,\n",
    "\t\t\"tau\": 0.005,\n",
    "\t\t# TD3\n",
    "\t\t\"policy_noise\": 0.2,    #args.policy_noise * max_action,\n",
    "\t\t\"noise_clip\": 0.5,  #args.noise_clip * max_action,\n",
    "\t\t\"policy_freq\": 2,\n",
    "\t\t# TD3 + BC\n",
    "\t\t\"alpha\": 2.5\n",
    "\t}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, **kwargs):   #config\n",
    "    #np.random.seed(config.seed)\n",
    "    #random.seed(config.seed)\n",
    "    #torch.manual_seed(config.seed)      \n",
    "\n",
    "    ############################# Define Path and Load Dataset ######################################\n",
    "\n",
    "\tpath = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\"\n",
    "\tpath2 = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 2/\"\n",
    "    ## WARNING check which type of feedback in which round, and which corner for each subject\n",
    "\t#corner =  \"left\"\n",
    "\t#data = resultsSingleSubject(initial, \"reward\", path, corner)\n",
    "\t'''dic for one subject composed of ~1000 timepoints for one shot, 25 shots in one block, and 10 blocks\n",
    "    First 3 blocks are baseline learning, then 6 blocks of adaptation to perturbation, and one final washout block\n",
    "    That is 250 shots per subjects, 300'564 points in the dictionnary'''\n",
    "\n",
    "\t# Environment State Properties\n",
    "\tcorner = \"all\"\n",
    "\tstate_dim=14\n",
    "\taction_dim=2\n",
    "\tmax_action = 1\n",
    "\tnormalize = True\n",
    "    # Agent parameters\n",
    "\tkwargs = {\n",
    "\t\t\"state_dim\": state_dim,\n",
    "\t\t\"action_dim\": action_dim,\n",
    "\t\t\"max_action\": max_action,\n",
    "\t\t\"discount\": args['discount'],\n",
    "\t\t\"tau\": args['tau'],\n",
    "\t\t# TD3\n",
    "\t\t\"policy_noise\": args['policy_noise'] * max_action,\n",
    "\t\t\"noise_clip\": args['noise_clip'] * max_action,\n",
    "\t\t\"policy_freq\": args['policy_freq'],\n",
    "\t\t# TD3 + BC\n",
    "\t\t\"alpha\": args['alpha']\n",
    "\t    }\n",
    "    \n",
    "\t# Initialize Agent\n",
    "\tif args['policy'] == \"TD3_BC\":\n",
    "\t\tpolicy = TD3_BC(**kwargs) \n",
    "\telif args['policy'] == \"CQL_SAC\":\n",
    "\t\tpolicy = CQLSAC(state_size=state_dim, action_size=action_dim, device=device)\n",
    "\telse:\n",
    "\t\traise ValueError(\"Chose Agent between [TD3_BC, CQL_SAC]\")\n",
    "\n",
    "\tif args['load_model'] != \"\":\n",
    "\t\tpolicy_file = file_name if args['load_model'] == \"default\" else args['load_model']\n",
    "\t\tpolicy.load(f\"./models/{policy_file}\")\n",
    "    \n",
    "\t'''\n",
    "    # Dataframe for all subjects\n",
    "\tinitial = \"AAB\"\n",
    "\tdata = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "\t#data = resultsMultipleSubjects([path, path2], 'reward', 'all')\n",
    "\n",
    "\t#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "\t#pd_dataset = pd.DataFrame.from_dict(dataset)\n",
    "\t#pd_dataset.to_csv(\"RL_dataset/AAB.csv\")\n",
    "\t#print(\"AAB saved\")\n",
    "\treturn data\n",
    "\t'''\n",
    "\t#dataset = pd.read_csv(\"RL_dataset/AAB.csv\")\n",
    "\tinitial = \"AAB\"\n",
    "\tdata = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "\t#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "\t#replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "\t#replay_buffer.convert_D4RL(dataset)\n",
    "\treturn data\n",
    "\t\"\"\"\n",
    "\tif normalize:\n",
    "\t\tmean,std = replay_buffer.normalize_states() \n",
    "\telse:\n",
    "\t\tmean,std = 0,1\n",
    "\t\n",
    "\t################## Training #######################\n",
    "\t#for t in range(int(args.max_timesteps)):\n",
    "\t\t#policy.train(replay_buffer, args.batch_size)\n",
    "\n",
    "\n",
    "\tsteps = 0\n",
    "\taverage10 = deque(maxlen=10)\n",
    "\ttotal_steps = 0\n",
    "\tbatch_size = 64 #256\n",
    "\n",
    "\tfor i in range(1, 100):\n",
    "\t\tepisode_steps = 0\n",
    "\t\trewards = 0\n",
    "\t\twhile True:\n",
    "\t\t\tsteps += 1\n",
    "\t\t\tprint(\"step: \", steps)\n",
    "\t\t\tpolicy.train(replay_buffer)\n",
    "\n",
    "\t\taverage10.append(rewards)\n",
    "\t\ttotal_steps += episode_steps\n",
    "\t\tprint(\"Episode: {} | Reward: {} | Polciy Loss: {} | Steps: {}\".format(i, reward, policy_loss, steps))\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_390/1458895838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTD3_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{args['policy']}_{args['seed']}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_390/4170048367.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_dim, action_dim, max_action, discount, tau, policy_noise, noise_clip, policy_freq, alpha)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize policy\n",
    "policy = TD3_BC(**kwargs)\n",
    "\n",
    "print(args[\"policy\"])\n",
    "file_name = f\"{args['policy']}_{args['seed']}\"\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Policy: {args['policy']}, Seed: {args['seed']}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "\n",
    "if args['save_model'] and not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "    \n",
    "#dataset = train(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5158/3957423419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph Analysis 1 subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = \"AAB\"\n",
    "df = pd.read_csv(\"RL_dataset/Offline_reduced/\"+initial+\"_Offline_reduced.csv\", header = 0, \\\n",
    "        names = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]', r'\\r'], '', regex=True) \n",
    "states = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "actions= pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "new_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "trial = df['trial'].astype(int)\n",
    "terminals = df['terminals'].astype(bool) #=='True'    #Dataframe Object type converts to string (if convert using astype(bool), all values are True)\n",
    "#print(df['rewards'])\n",
    "rewards = df['rewards'].astype(float)\n",
    "#print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "\n",
    "successful_rewards_ind = []\n",
    "for i in range(len(rewards)):\n",
    "        if terminals.iloc[i] == True:\n",
    "                if rewards.iloc[i] != -10.0:\n",
    "                        successful_rewards_ind = np.append(successful_rewards_ind, i)\n",
    "successful_trial_list = trial.iloc[successful_rewards_ind]\n",
    "\n",
    "\n",
    "fig1, action_pattern_x = plt.subplots()\n",
    "fig2, action_pattern_z = plt.subplots()\n",
    "\n",
    "for i in successful_trial_list:\n",
    "        action_pattern_x.plot(np.arange(1, len(actions[trial == i])+1), actions[trial == i][0])\n",
    "        action_pattern_z.plot(np.arange(1, len(actions[trial == i])+1), actions[trial == i][1])\n",
    "\n",
    "action_pattern_x.set_xlabel(\"timesteps\")\n",
    "action_pattern_x.set_ylabel(\"Impulse force\")\n",
    "fig1.suptitle('Impulse force along x-axis at each timestep \\n (30 timesteps before hit, 20 timesteps after)', fontsize=16, y=1.04)\n",
    "action_pattern_z.set_xlabel(\"timesteps\")\n",
    "action_pattern_z.set_ylabel(\"Impulse force\")\n",
    "fig2.suptitle('Impulse force along z-axis at each timestep \\n (30 timesteps before hit, 20 timesteps after)', fontsize=16, y=1.04)\n",
    "\n",
    "\n",
    "\n",
    "#unsuccessful_trial_list = trial.drop(a.index, axis=0)\n",
    "successful_actions = actions.iloc[successful_trial_list.index]\n",
    "\n",
    "#unsuccessful_actions = actions.iloc[unsuccessful_trial_list.index]\n",
    "#successful_actions = pd.DataFrame(index=range(len(successful_trial_list)*states[trial==1].shape[0]),columns=range(actions.shape[1]))\n",
    "#unsuccessful_actions = pd.DataFrame(index=range(len(unsuccessful_trial_list)*states[trial==1].shape[0]),columns=range(actions.shape[1]))\n",
    "\"\"\"for i in successful_trial_list:\n",
    "        successful_actions.iloc[i:i+states[trial==1].shape[0]] = actions[trial==i]\n",
    "for i in unsuccessful_trial_list:\n",
    "        unsuccessful_actions.iloc[i:i+states[trial==1].shape[0]]  = actions[trial==i]\"\"\"\n",
    "\n",
    "s_a = np.zeros((len(successful_actions), states[trial==1].shape[0], actions.shape[1]))  #trial 1 is always complete (250 elements)\n",
    "u_a = np.zeros((len(trial.unique())-len(successful_actions), states[trial==1].shape[0], actions.shape[1]))\n",
    "j = 0\n",
    "k = 0\n",
    "print(\"a shape: \", actions.shape[1])\n",
    "\n",
    "episode_len = 250 #50\n",
    "for i, trial_ind in enumerate(trial.unique()):\n",
    "        pad_len = episode_len - actions[trial==trial_ind].shape[0]\n",
    "        a = actions[trial==trial_ind]\n",
    "        if np.isin(trial_ind, successful_trial_list):\n",
    "                if pad_len > 0:\n",
    "                        pad_array = np.array([pad_len*[np.nan]])\n",
    "                        a = np.pad(a, pad_width = ((pad_len,0), (0,0)),  mode = 'constant', constant_values = np.nan)\n",
    "\n",
    "                s_a[j][:][:] = a\n",
    "                j+=1\n",
    "        else:\n",
    "                if pad_len > 0:\n",
    "                        pad_array = np.array([pad_len*[np.nan]])\n",
    "                        a = np.pad(a, pad_width = ((pad_len,0), (0,0)),  mode = 'constant', constant_values = np.nan) \n",
    "                u_a[k][:][:] = a\n",
    "                k+=1 \n",
    "successful_mean = np.nanmean(s_a, axis=0, keepdims=True)      #numpy.nanmean()\n",
    "successful_std = np.nanstd(s_a, axis=0)\n",
    "\n",
    "unsuccessful_mean = np.nanmean(u_a, axis=0)     #unsuccessful_trial_list\n",
    "unsuccessful_std = np.nanstd(u_a, axis=0)         #unsuccessful_trial_list\n",
    "\n",
    "#print(successful_mean.shape[0], successful_mean[:][1])\n",
    "successful_mean = successful_mean.reshape(-1)\n",
    "successful_mean = successful_mean.reshape(250,2) #(50,2)\n",
    "\n",
    "unsuccessful_mean = unsuccessful_mean.reshape(-1)\n",
    "unsuccessful_mean = unsuccessful_mean.reshape(250,2)      #(50,2)\n",
    "\n",
    "\n",
    "fig3, mean_x = plt.subplots()\n",
    "#mean_x.plot(np.arange(1, len(actions[trial == 1])+1), successful_mean[:,0])   #linestyle='None', marker='^')\n",
    "mean_x.set_xlabel(\"timesteps\")\n",
    "mean_x.set_ylabel(\"Impulse force\")\n",
    "fig3.suptitle('Mean of Successful trial x-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_x.errorbar(np.arange(1, len(actions[trial == 1])+1), successful_mean[:,0], successful_std[:,0], color='blue', ecolor='orange')#, linestyle='None', marker='^')\n",
    "mean_x.plot(np.arange(1, len(actions[trial == 1])+1), unsuccessful_mean[:,0],  color='green') \n",
    "\n",
    "fig4, mean_x = plt.subplots()\n",
    "mean_x.set_xlabel(\"timesteps\")\n",
    "mean_x.set_ylabel(\"Impulse force\")\n",
    "fig4.suptitle('Mean of Unsuccessful trial x-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_x.errorbar(np.arange(1, len(actions[trial == 1])+1), unsuccessful_mean[:,0], unsuccessful_std[:,0], color='green', ecolor='red')  # linestyle='None', marker='^')\n",
    "\n",
    "fig5, mean_z = plt.subplots()\n",
    "mean_z.set_xlabel(\"timesteps\")\n",
    "mean_z.set_ylabel(\"Impulse force\")\n",
    "fig5.suptitle('Mean of Successful trial z-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_z.errorbar(np.arange(1, len(actions[trial == 1])+1), successful_mean[:,1], successful_std[:,1], color='blue', ecolor='orange')     #, linestyle='None', marker='^')\n",
    "mean_z.plot(np.arange(1, len(actions[trial == 1])+1), unsuccessful_mean[:,1], color='green')\n",
    "\n",
    "fig6, mean_z = plt.subplots()\n",
    "mean_z.set_xlabel(\"timesteps\")\n",
    "mean_z.set_ylabel(\"Impulse force\")\n",
    "fig6.suptitle('Mean of Unsuccessful trial z-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_z.errorbar(np.arange(1, len(actions[trial == 1])+1), unsuccessful_mean[:,1], unsuccessful_std[:,1], color='green', ecolor='red')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "fig1.savefig('analysis/'+initial+'_30_action_Succesful_pattern_x.png', bbox_inches='tight')\n",
    "fig2.savefig('analysis/'+initial+'_30_action_Successful_pattern_z.png', bbox_inches='tight')\n",
    "\n",
    "fig3.savefig('analysis/'+initial+'_30_action_Successful_pattern_mean_x.png', facecolor='w',bbox_inches='tight')\n",
    "fig4.savefig('analysis/'+initial+'_30_action_Unuccessful_pattern_mean_x.png', facecolor='w',bbox_inches='tight')\n",
    "\n",
    "fig5.savefig('analysis/'+initial+'_30_action_Successful_pattern_mean_z.png', facecolor='w',bbox_inches='tight')\n",
    "fig6.savefig('analysis/'+initial+'_30_action_Unsuccessful_pattern_mean_z.png', facecolor='w',bbox_inches='tight')\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "actions[trial == 1][0].hist(bins=15, color='steelblue', edgecolor='black', linewidth=1.0,\n",
    "           xlabelsize=8, ylabelsize=8, grid=False)    \n",
    "plt.tight_layout(rect=(0, 0, 1.2, 1.2))  \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "g = sns.FacetGrid(df_monthly, col=\"Year\", row=\"Month\", height=4.2, aspect=1.9)\n",
    "g = g.map(sns.barplot, 'District', 'PM2.5', palette='viridis', ci=None, order = list_district)\n",
    "\n",
    "g.set_xticklabels(rotation = 90)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Analysis all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/TD3_BC/RL_dataset/Offline_reduced/\"\n",
    "filename = []\n",
    "for file in sorted(os.listdir(path)):\n",
    "\tfilename.append(path+file)\n",
    "\n",
    "df = pd.read_csv(filename[0], header = 0, \\\n",
    "\t\t\tnames = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]', r'\\r'], '', regex=True) \n",
    "\n",
    "states = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "actions = pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "new_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "trial = df['trial'].astype(int)\n",
    "terminals = df['terminals'].astype(bool)\n",
    "\n",
    "#Train/Test split\n",
    "trial_ind = np.arange(1,len(trial.unique()))\t#+1 if we want 250 index as well\n",
    "train_trial = np.random.choice(trial_ind, size=int(0.8*len(trial_ind)), replace=False)  #distrib proba for each value, could be useful to weight more \"important\" trajectories\n",
    "test_trial = np.delete(trial_ind, train_trial-1)\n",
    "\n",
    "train_ind = trial.isin(train_trial)\n",
    "test_ind = trial.isin(test_trial)\n",
    "\n",
    "train_set = {'trial': trial[train_ind],\n",
    "\t\t\t\t'states': states[train_ind],\n",
    "\t\t\t\t'actions': actions[train_ind],\n",
    "\t\t\t\t'new_states': new_states[train_ind],\n",
    "\t\t\t\t'rewards': df['rewards'][train_ind],\n",
    "\t\t\t\t'terminals': terminals[train_ind]}\n",
    "\n",
    "test_set = {'trial': trial[test_ind],\n",
    "\t\t\t\t'states': states[test_ind],\n",
    "\t\t\t\t'actions': actions[test_ind],\n",
    "\t\t\t\t'new_states': new_states[test_ind],\n",
    "\t\t\t\t'rewards': df['rewards'][test_ind],\n",
    "\t\t\t\t'terminals': terminals[test_ind]}\n",
    "\n",
    "#print(filename[0], len(train_set['trial'].unique()), train_set['terminals'].value_counts())\n",
    "\n",
    "\n",
    "### If multiple files are passed ###\n",
    "if len(filename) > 1:\n",
    "\tfor i, file in enumerate(filename):\n",
    "\t\tif i > 0:\n",
    "\t\t\tdf = pd.read_csv(file, header = 0, \\\n",
    "\t\t\t\t\tnames = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "\t\t\tdf = df.replace([r'\\n', r'\\[', r'\\]', r'\\r'], '', regex=True) \n",
    "\t\t\n",
    "\t\t\tstates = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "\t\t\tactions = pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "\t\t\tnew_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "\t\t\ttrial = df['trial'].astype(int)\n",
    "\t\t\tterminals = df['terminals'].astype(bool)\n",
    "\t\t\t#Train/Test split\n",
    "\t\t\ttrial_ind = np.arange(1,len(trial.unique()))\n",
    "\t\t\ttrain_trial = np.random.choice(trial_ind, size=int(0.8*len(trial_ind)), replace=False)  #distrib proba for each value, could be useful to weight more \"important\" trajectories\n",
    "\t\t\ttest_trial = np.delete(trial_ind, train_trial-1)\n",
    "\n",
    "\t\t\ttrain_ind = trial.isin(train_trial)\n",
    "\t\t\ttest_ind = trial.isin(test_trial)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\ttrain_set['trial'] = pd.concat([train_set['trial'],trial[train_ind]+ (i)*250 ], axis=0)\n",
    "\t\t\ttrain_set['states'] = pd.concat([train_set['states'], states[train_ind]], axis=0)\n",
    "\t\t\ttrain_set['actions'] = pd.concat([train_set['actions'], actions[train_ind]], axis=0)\n",
    "\t\t\ttrain_set['new_states'] = pd.concat([train_set['new_states'], new_states[train_ind]], axis=0)\n",
    "\t\t\ttrain_set['rewards'] = pd.concat([train_set['rewards'], df['rewards'][train_ind]], axis=0)\n",
    "\t\t\ttrain_set['terminals'] = pd.concat([train_set['terminals'], terminals[train_ind]], axis=0)\t\n",
    "\n",
    "\t\t\ttest_set['trial'] = pd.concat([test_set['trial'], trial[test_ind]+ (i)*250 ], axis=0)\n",
    "\t\t\ttest_set['states'] = pd.concat([test_set['states'], states[test_ind]], axis=0)\n",
    "\t\t\ttest_set['actions'] = pd.concat([test_set['actions'], actions[test_ind]], axis=0)\n",
    "\t\t\ttest_set['new_states'] = pd.concat([test_set['new_states'], new_states[test_ind]], axis=0)\n",
    "\t\t\ttest_set['rewards'] = pd.concat([test_set['rewards'], df['rewards'][test_ind]], axis=0)\n",
    "\t\t\ttest_set['terminals'] = pd.concat([test_set['terminals'], terminals[test_ind]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = test_set['rewards']\n",
    "terminals = test_set['terminals']\n",
    "trial = test_set['trial']\n",
    "states = test_set['states']\n",
    "actions = test_set['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "#for i in np.array(trial.unique()):\n",
    "'''\n",
    "ind = actions[terminals == True]\n",
    "for i in ind.index:\n",
    "        print(\"rewards terminal state: \", rewards[i])\n",
    "print(\"reward ind.index[12]: \", rewards[ind.index[12]])\n",
    "a = ind[rewards != -10.0]\n",
    "successful_trial_list = trial[a.index]\n",
    "'''\n",
    "\"\"\"\n",
    "terminal_rewards = test_set['rewards'][test_set['terminals'] == True]\n",
    "successful_rewards = terminal_rewards[terminal_rewards != -10.0]\n",
    "successful_trial_list = test_set['trial'].iloc[successful_rewards.index]\"\"\"\n",
    "#terminal_rewards = rewards[terminals == True]\n",
    "successful_rewards_ind = []\n",
    "for i in range(len(rewards)):\n",
    "        if terminals.iloc[i] == True:\n",
    "                if rewards.iloc[i] != -10.0:\n",
    "                        successful_rewards_ind = np.append(successful_rewards_ind, i)\n",
    "successful_trial_list = trial.iloc[successful_rewards_ind]\n",
    "\n",
    "fig1, action_pattern_x = plt.subplots()\n",
    "fig2, action_pattern_z = plt.subplots()\n",
    "\n",
    "for i in successful_trial_list:\n",
    "        action_pattern_x.plot(np.arange(1, len(actions[trial == i])+1), actions[trial == i][0])\n",
    "        action_pattern_z.plot(np.arange(1, len(actions[trial == i])+1), actions[trial == i][1])\n",
    "\n",
    "action_pattern_x.set_xlabel(\"timesteps\")\n",
    "action_pattern_x.set_ylabel(\"Impulse force\")\n",
    "fig1.suptitle('Impulse force along x-axis at each timestep \\n (30 timesteps before hit, 20 timesteps after)', fontsize=16, y=1.04)\n",
    "action_pattern_z.set_xlabel(\"timesteps\")\n",
    "action_pattern_z.set_ylabel(\"Impulse force\")\n",
    "fig2.suptitle('Impulse force along z-axis at each timestep \\n (30 timesteps before hit, 20 timesteps after)', fontsize=16, y=1.04)\n",
    "\n",
    "\n",
    "\n",
    "#unsuccessful_trial_list = trial.drop(a.index, axis=0)\n",
    "successful_actions = np.zeros((len(successful_trial_list), len(actions[trial == 19]), actions[trial==1].shape[1]))\n",
    "for i, trial_ind in enumerate(successful_trial_list):\n",
    "        successful_actions[i][:][:] = actions[trial == trial_ind]\n",
    "\n",
    "s_a = np.zeros((len(successful_actions), 50, actions.shape[1]))  #trial 1 is always complete (250 elements)\n",
    "u_a = np.zeros((len(trial.unique())-len(successful_actions), 50, actions.shape[1]))\n",
    "j = 0\n",
    "k = 0\n",
    "\n",
    "episode_len = 50\n",
    "for i, trial_ind in enumerate(trial.unique()):\n",
    "        pad_len = episode_len - actions[trial==trial_ind].shape[0]\n",
    "        if np.isin(trial_ind, successful_trial_list):\n",
    "                a = actions.iloc[i*episode_len:(i+1)*episode_len][:]\n",
    "                if pad_len > 0:\n",
    "                        pad_array = np.array([pad_len*[np.nan]])\n",
    "                        a = np.pad(a, pad_width = ((pad_len,0), (0,0)),  mode = 'constant', constant_values = np.nan)\n",
    "                s_a[j][:][:] = a\n",
    "                j+=1\n",
    "        else:\n",
    "                a = actions.iloc[i*episode_len:(i+1)*episode_len][:]\n",
    "                if pad_len > 0:\n",
    "                        pad_array = np.array([pad_len*[np.nan]])\n",
    "                        a = np.pad(a, pad_width = ((pad_len,0), (0,0)),  mode = 'constant', constant_values = np.nan) \n",
    "                u_a[k][:][:] = a\n",
    "                k+=1 \n",
    "successful_mean = np.nanmean(s_a, axis=0, keepdims=True)      #numpy.nanmean()\n",
    "successful_std = np.nanstd(s_a, axis=0)\n",
    "unsuccessful_mean = np.nanmean(u_a, axis=0)     #unsuccessful_trial_list\n",
    "unsuccessful_std = np.nanstd(u_a, axis=0)         #unsuccessful_trial_list\n",
    "\n",
    "#print(successful_mean.shape[0], successful_mean[:][1])\n",
    "successful_mean = successful_mean.reshape(-1)\n",
    "successful_mean = successful_mean.reshape(50,2)\n",
    "\n",
    "unsuccessful_mean = unsuccessful_mean.reshape(-1)\n",
    "unsuccessful_mean = unsuccessful_mean.reshape(50,2)\n",
    "\n",
    "\n",
    "fig3, mean_x = plt.subplots()\n",
    "#mean_x.plot(np.arange(1, len(actions[trial == 1])+1), successful_mean[:,0])   #linestyle='None', marker='^')\n",
    "mean_x.set_xlabel(\"timesteps\")\n",
    "mean_x.set_ylabel(\"Impulse force\")\n",
    "fig3.suptitle('Mean of Successful trial x-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_x.errorbar(np.arange(1, 51), successful_mean[:,0], successful_std[:,0], color='blue', ecolor='orange')#, linestyle='None', marker='^')\n",
    "\n",
    "fig4, mean_x = plt.subplots()\n",
    "mean_x.set_xlabel(\"timesteps\")\n",
    "mean_x.set_ylabel(\"Impulse force\")\n",
    "fig4.suptitle('Mean of Unsuccessful trial x-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_x.errorbar(np.arange(1, 51), unsuccessful_mean[:,0], unsuccessful_std[:,0], color='blue', ecolor='red')  # linestyle='None', marker='^')\n",
    "\n",
    "fig5, mean_z = plt.subplots()\n",
    "mean_z.set_xlabel(\"timesteps\")\n",
    "mean_z.set_ylabel(\"Impulse force\")\n",
    "fig5.suptitle('Mean of Successful trial z-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_z.errorbar(np.arange(1, 51), successful_mean[:,1], successful_std[:,1], color='blue', ecolor='orange')     #, linestyle='None', marker='^')\n",
    "\n",
    "fig6, mean_z = plt.subplots()\n",
    "mean_z.set_xlabel(\"timesteps\")\n",
    "mean_z.set_ylabel(\"Impulse force\")\n",
    "fig6.suptitle('Mean of Unsuccessful trial z-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_z.errorbar(np.arange(1, 51), unsuccessful_mean[:,1], unsuccessful_std[:,1], color='blue', ecolor='red')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "fig1.savefig('analysis/'+initial+'_30_action_Succesful_pattern_x.png', bbox_inches='tight')\n",
    "fig2.savefig('analysis/'+initial+'_30_action_Successful_pattern_z.png', bbox_inches='tight')\n",
    "\n",
    "fig3.savefig('analysis/'+initial+'_30_action_Successful_pattern_mean_x.png', facecolor='w',bbox_inches='tight')\n",
    "fig4.savefig('analysis/'+initial+'_30_action_Unuccessful_pattern_mean_x.png', facecolor='w',bbox_inches='tight')\n",
    "\n",
    "fig5.savefig('analysis/'+initial+'_30_action_Successful_pattern_mean_z.png', facecolor='w',bbox_inches='tight')\n",
    "fig6.savefig('analysis/'+initial+'_30_action_Unsuccessful_pattern_mean_z.png', facecolor='w',bbox_inches='tight')\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ AAB\n",
      "Result Single Subject function\n",
      "Success Derivation function\n",
      "9 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n",
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported AAB as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n"
     ]
    }
   ],
   "source": [
    "############################# Define Path and Load Dataset ######################################\n",
    "path = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\"\n",
    "path2 = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 2/\"\n",
    "initial = \"AAB\"\n",
    "data = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "#save_raw_data(data, initial)\n",
    "#save_RL_reduced_dataset(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced raw data  AAB  saved\n"
     ]
    }
   ],
   "source": [
    "save_raw_data(data, initial)    #print(\"Success \", i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline RL Dataset function\n",
      "61214\n",
      "AAB reduced Offline dataset saved\n"
     ]
    }
   ],
   "source": [
    "save_RL_reduced_dataset(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Define Path and Load Dataset ######################################\n",
    "\n",
    "path = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\"\n",
    "path2 = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 2/\"\n",
    "pathlist = [path, path2]\n",
    "## WARNING check which type of feedback in which round, and which corner for each subject\n",
    "#corner =  \"left\"\n",
    "#data = resultsSingleSubject(initial, \"reward\", path, corner)\n",
    "'''dic for one subject composed of ~1000 timepoints for one shot, 25 shots in one block, and 10 blocks\n",
    "First 3 blocks are baseline learning, then 6 blocks of adaptation to perturbation, and one final washout block\n",
    "That is 250 shots per subjects, 300'564 points in the dictionnary'''\n",
    "\n",
    "# Environment State Properties\n",
    "'''\n",
    "corner = \"all\"\n",
    "state_dim=14\n",
    "action_dim=2\n",
    "max_action = 1\n",
    "normalize = True\n",
    "# Agent parameters\n",
    "kwargs = {\n",
    "\t\"state_dim\": state_dim,\n",
    "\t\"action_dim\": action_dim,\n",
    "\t\"max_action\": max_action,\n",
    "\t\"discount\": args['discount'],\n",
    "\t\"tau\": args['tau'],\n",
    "\t# TD3\n",
    "\t\"policy_noise\": args['policy_noise'] * max_action,\n",
    "\t\"noise_clip\": args['noise_clip'] * max_action,\n",
    "\t\"policy_freq\": args['policy_freq'],\n",
    "\t# TD3 + BC\n",
    "\t\"alpha\": args['alpha']\n",
    "\t}\n",
    "'''\n",
    "#dataset = pd.read_csv(\"RL_dataset/AAB.csv\")\n",
    "for i, initial in enumerate(sorted(os.listdir(path2))):\n",
    "\tpathSubj = path2 + str(initial)\n",
    "\tfor fil in range(len(sorted(os.listdir(pathSubj + '/Game/')))):\n",
    "\t\tif sorted(os.listdir(pathSubj + '/Game/'))[fil].find(\"Block2\") > -1:\n",
    "\t\t\tblockFile = sorted(os.listdir(pathSubj + '/Game/'))[fil]\n",
    "\t\t\t\n",
    "\t\t\tif blockFile.find('Reward') > -1:\n",
    "\t\t\t\tprint(path, initial)\n",
    "\t\t\t\tdata = resultsMultipleSubjects(path2, initial, 'reward', 'all')\n",
    "\t\t\t\tsave_raw_data(data, initial)\n",
    "\t\t\t\tsave_RL_reduced_dataset(initial)\n",
    "\"\"\"\t\t\t\t\t\t\n",
    "for path in pathlist:\n",
    "\tfor i, initial in enumerate(sorted(os.listdir(path))):\n",
    "\t\tpathSubj = path + str(initial)\n",
    "\t\tfor fil in range(len(sorted(os.listdir(pathSubj + '/Game/')))):\n",
    "\t\t\tif sorted(os.listdir(pathSubj + '/Game/'))[fil].find(\"Block2\") > -1:\n",
    "\t\t\t\tblockFile = sorted(os.listdir(pathSubj + '/Game/'))[fil]\n",
    "\t\t\t\t\n",
    "\t\t\t\tif blockFile.find('Reward') > -1:\n",
    "\t\t\t\t\tprint(path, initial)\n",
    "\t\t\t\t\tdata = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "\t\t\t\t\tsave_raw_data(data, initial)\n",
    "\t\t\t\t\tsave_RL_reduced_dataset(initial)\"\"\"\n",
    "\n",
    "#initial = \"BL\"\n",
    "#data = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "#data = resultsMultipleSubjects([path, path2], 'reward', 'all')\n",
    "\n",
    "#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "#replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "#replay_buffer.convert_D4RL(dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_data(data, initial):\n",
    "    reward_ = []\n",
    "    #rewards = []\n",
    "    cueballpos = []\n",
    "    cueballvel = []\n",
    "    redballpos = []\n",
    "    targetcornerpos = []\n",
    "    cueposfront = []\n",
    "    cueposback = []\n",
    "    cuedirection = []\n",
    "    cuevel = []\n",
    "\n",
    "    total_len_trajectories = 0\n",
    "    \n",
    "    k=0\n",
    "    for i in range(data[initial][\"start_ind\"].shape[0]):\n",
    "        #reward_ = []\n",
    "        for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]):\n",
    "            total_len_trajectories += 1\n",
    "            if j == data[initial][\"hit_ind\"][i]-1:\n",
    "                #if i == 12 :\n",
    "                    #print(\"right hit index wtf is going on\", j, k)\n",
    "                #done_bool = True\n",
    "                reward = data[initial][\"rewards\"][i]\n",
    "            else:\n",
    "                #if i == 12:\n",
    "                    #print(\"wrong hit index\", j, k)\n",
    "                #done_bool = False\n",
    "                ## Discounted reward ##\n",
    "                #gamma = 0.99\n",
    "                #reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "                reward = 0\n",
    "            #if i == 12:\n",
    "                #print(reward)\n",
    "            k+=1\n",
    "            reward_.append(reward)\n",
    "        #rewards.append(np.array(reward_))\n",
    "        cueballpos.append(np.array((data[initial][\"cueballpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cueballvel.append(np.array((data[initial][\"cueballvel\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballvel\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballvel\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballvel\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        redballpos.append(np.array((data[initial][\"redballpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"redballpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"redballpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"redballpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        targetcornerpos.append(np.array((data[initial][\"targetcornerpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"targetcornerpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"targetcornerpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"targetcornerpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cueposfront.append(np.array((data[initial][\"cueposfront\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposfront\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposfront\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposfront\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cueposback.append(np.array((data[initial][\"cueposback\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposback\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposback\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposback\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cuedirection.append(np.array((data[initial][\"cuedirection\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuedirection\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuedirection\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuedirection\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cuevel.append(np.array((data[initial][\"cuevel\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuevel\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuevel\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuevel\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "\n",
    "    #for i in range(len(reward_)):\n",
    "        #print(reward_[i])\n",
    "    dic = {'rewards': np.array(reward_),    #rewards,  \n",
    "    'cueballpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cueballvel': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'redballpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'targetcornerpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cueposfront': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cueposback': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cuedirection': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cuevel': np.zeros(total_len_trajectories, dtype=object)}\n",
    "\n",
    "    transition_num = 0\n",
    "    #print(\"cuballpos shape 1: \", cueballpos[1].shape[1], cueballpos[1].shape, data[initial][\"start_ind\"][1]-data[initial][\"hit_ind\"][1], rewards[1].shape)\n",
    "    for i in range(data[initial][\"start_ind\"].shape[0]):\n",
    "            for j in range(cueballpos[i].shape[1]):\n",
    "                    dic['cueballpos'][transition_num] = [cueballpos[i][0][j],cueballpos[i][1][j], cueballpos[i][2][j], cueballpos[i][3][j]]\n",
    "                    dic['cueballvel'][transition_num] = [cueballvel[i][0][j],cueballvel[i][1][j], cueballvel[i][2][j], cueballvel[i][3][j]]\n",
    "                    dic['redballpos'][transition_num] = [redballpos[i][0][j], redballpos[i][1][j], redballpos[i][2][j], redballpos[i][3][j]]\n",
    "                    dic['targetcornerpos'][transition_num] = [targetcornerpos[i][0][j], targetcornerpos[i][1][j], targetcornerpos[i][2][j], targetcornerpos[i][3][j]]\n",
    "                    dic['cueposfront'][transition_num] = [cueposfront[i][0][j], cueposfront[i][1][j], cueposfront[i][2][j], cueposfront[i][3][j]]\n",
    "                    dic['cueposback'][transition_num] = [cueposback[i][0][j], cueposback[i][1][j], cueposback[i][2][j], cueposback[i][3][j]]\n",
    "                    dic['cuedirection'][transition_num] = [cuedirection[i][0][j], cuedirection[i][1][j], cuedirection[i][2][j], cuedirection[i][3][j]]\n",
    "                    dic['cuevel'][transition_num] = [cuevel[i][0][j], cuevel[i][1][j], cuevel[i][2][j], cuevel[i][3][j]]\n",
    "                    transition_num += 1\n",
    "            #if i%50 == 0:\n",
    "                    #print(i)\n",
    "    pd_dataset = pd.DataFrame.from_dict(dic)\n",
    "    #pd_dataset.to_csv(\"RL_dataset/reduced_data/\"+initial+\"_reduced_data.csv\")\n",
    "    pd_dataset.to_csv(\"RL_dataset/raw_data/\"+initial+\"_raw_data.csv\")\n",
    "    print(\"reduced raw data \", initial, \" saved\")\n",
    "    #return pd_dataset, dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save RL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_RL_reduced_dataset(initial):\n",
    "        #df = pd.read_csv(\"RL_dataset/reduced_data/\"+initial+\"_reduced_data.csv\", header = 0, \\\n",
    "        df = pd.read_csv(\"RL_dataset/raw_data/\"+initial+\"_raw_data.csv\", header = 0, \\\n",
    "                names = ['rewards','cueballpos', 'cueballvel','redballpos', 'targetcornerpos', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel'], usecols = [1,2,3,4,5,6,7,8,9], lineterminator = \"\\n\")\n",
    "        df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "        rewards= pd.DataFrame.from_records(np.array(df['rewards'].astype(str).str.split(','))).astype(float)\n",
    "        cueballpos = pd.DataFrame.from_records(np.array(df['cueballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cueballvel = pd.DataFrame.from_records(np.array(df['cueballvel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        redballpos = pd.DataFrame.from_records(np.array(df['redballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        targetcornerpos = pd.DataFrame.from_records(np.array(df['targetcornerpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cueposfront = pd.DataFrame.from_records(np.array(df['cueposfront'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cueposback = pd.DataFrame.from_records(np.array(df['cueposback'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cuedirection = pd.DataFrame.from_records(np.array(df['cuedirection'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cuevel = pd.DataFrame.from_records(np.array(df['cuevel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "\n",
    "        ball_data = {'cueballpos': cueballpos,\n",
    "             'cueballvel': cueballvel,\n",
    "            'redballpos': redballpos,\n",
    "            'targetcornerpos': targetcornerpos\n",
    "            }\n",
    "        cue_data = {'cueposfront': cueposfront,\n",
    "                'cueposback': cueposback,\n",
    "                'cuedirection': cuedirection\n",
    "                }\n",
    "        dataset = Offline_Reduced(ball_data, cue_data, cuevel, rewards[0])\n",
    "        #dataset = Offline_One(ball_data, cue_data, cuevel, rewards[0])  #rewars[0] to get float value from dataframe\n",
    "        # Dataframe does not accept 2-d arrays\n",
    "        # transform 2-d array (n states times 14 dimensions (state)) to 1-d array of list (of length 14)\n",
    "        #dataset_RL = Offline_RL_dataset(data,rewards,cuedirection, cuevel,cueballpos, terminate_on_end=True)\n",
    "        #Offline_RL_load(rewards,cueballpos,redballpos, targetcornerpos, cueposfront, cueposback, cuedirection, cuevel, terminate_on_end=True)\n",
    "        #dataset = Offline_One_big_dict(data, initial)\n",
    "\n",
    "        new_d = {'trial': dataset[\"trial\"],\n",
    "                'states': np.zeros(dataset[\"states\"].shape[0], dtype=object),\n",
    "                'actions': np.zeros(dataset[\"actions\"].shape[0], dtype=object),\n",
    "                'new_states': np.zeros(dataset[\"new_states\"].shape[0], dtype=object),\n",
    "                'rewards': dataset[\"rewards\"],\n",
    "                'terminals': dataset[\"terminals\"]}\n",
    "        for i in range(dataset[\"states\"].shape[0]):\n",
    "                new_d['states'][i] = dataset[\"states\"][i][:].tolist()\n",
    "                new_d['actions'][i] = dataset[\"actions\"][i][:].tolist()\n",
    "                new_d['new_states'][i] = dataset[\"new_states\"][i][:].tolist()\n",
    "\n",
    "        pd_dataset = pd.DataFrame.from_dict(new_d)\n",
    "        pd_dataset.to_csv(\"RL_dataset/Offline_reduced/\"+initial+\"_Offline_reduced.csv\")\n",
    "        #pd_dataset.to_csv(\"RL_dataset/\"+initial+\".csv\")\n",
    "        print(initial, \"reduced Offline dataset saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "initial=\"AS\"\n",
    "df = pd.read_csv(\"RL_dataset/reduced_data/\"+initial+\"_reduced_data.csv\", header = 0, \\\n",
    "        names = ['rewards','cueballpos','redballpos', 'targetcornerpos', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel'], usecols = [1,2,3,4,5,6,7,8], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "rewards= pd.DataFrame.from_records(np.array(df['rewards'].astype(str).str.split(','))).astype(float)\n",
    "cueballpos = pd.DataFrame.from_records(np.array(df['cueballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "redballpos = pd.DataFrame.from_records(np.array(df['redballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "targetcornerpos = pd.DataFrame.from_records(np.array(df['targetcornerpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposfront = pd.DataFrame.from_records(np.array(df['cueposfront'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposback = pd.DataFrame.from_records(np.array(df['cueposback'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuedirection = pd.DataFrame.from_records(np.array(df['cuedirection'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuevel = pd.DataFrame.from_records(np.array(df['cuevel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = {'cueballpos': cueballpos,\n",
    "             'redballpos': redballpos, \n",
    "             'targetcornerpos': targetcornerpos,\n",
    "             'cueposfront': cueposfront, \n",
    "             'cueposback': cueposback,\n",
    "             'cuedirection': cuedirection,\n",
    "              'cuevel': cuevel}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "initial = \"AAB\"\n",
    "df = pd.read_csv(\"RL_dataset/Offline_reduced/\"+initial+\"_Offline_reduced.csv\", header = 0, \\\n",
    "        names = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "states = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "actions= pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "new_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "trial = df['trial'].astype(int)\n",
    "#Train/Test split\n",
    "trial_ind = np.arange(1,len(trial.unique())+1)\n",
    "train_trial = np.random.choice(trial_ind, size=200, replace=False)  #distrib proba for each value, could be useful to weight more \"important\" trajectories\n",
    "test_trial = np.delete(trial_ind, train_trial-1)\n",
    "\n",
    "train_ind = trial.isin(train_trial)\n",
    "test_ind = trial.isin(test_trial)\n",
    "\n",
    "train_set = {'trial': trial[train_ind],\n",
    "                'states': states[train_ind],\n",
    "                'actions': actions[train_ind],\n",
    "                'new_states': new_states[train_ind],\n",
    "                'rewards': df['rewards'][train_ind],\n",
    "                'terminals': df['terminals'][train_ind]}\n",
    "\n",
    "test_set = {'trial': trial[test_ind],\n",
    "                'states': states[test_ind],\n",
    "                'actions': actions[test_ind],\n",
    "                'new_states': new_states[test_ind],\n",
    "                'rewards': df['rewards'][test_ind],\n",
    "                'terminals': df['terminals'][test_ind]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dic):\n",
    "    ind = np.random.randint(1, 250) \n",
    "    return (torch.FloatTensor(dic['states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['actions'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.FloatTensor(dic['new_states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['rewards'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.Tensor(dic['terminals'][dic['trial'] == ind].to_numpy()).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(dic, ind):\n",
    "    return (torch.FloatTensor(dic['states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['actions'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.FloatTensor(dic['new_states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['rewards'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.Tensor(dic['terminals'][dic['trial'] == ind].to_numpy()).to(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool Graphic Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 14)\n"
     ]
    }
   ],
   "source": [
    "print(states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved GIF\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "states,_,_,_,_ = get_trajectory(train_set, train_set[\"trial\"].iloc[0])\n",
    "states = states.detach().numpy()\n",
    "\n",
    "#cueposfront = policy.select_action(states).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "\tax.clear()\n",
    "\t#cueposfront = policy.select_action(states)\n",
    "\t#line.set_xdata(cueposfront[i,0].detach().numpy())\n",
    "\t#line.set_ydata(cueposfront[i,1].detach().numpy())  # update the data.\n",
    "\t#line.set_data(cueposfront[i,0].detach().numpy()+i/10, cueposfront[i,1].detach().numpy()+i/10)\n",
    "\tx_values = [states[i,8], states[i,10]]\n",
    "\tz_values = [states[i,9], states[i,11]]\n",
    "\t#line.set_data(x_values, z_values)\n",
    "\tax.plot(x_values, z_values, color=\"blue\")\n",
    "\n",
    "\tx_val = states[i,0]\n",
    "\tz_val = states[i,1]\n",
    "\t#point.set_data(x_values, z_values)\n",
    "\tax.plot(x_val, z_val, ms=7, color='black', marker='o')\n",
    "\t\n",
    "\t\n",
    "\tax.plot(states[0][4], states[0][5], ms=7, color=\"red\", marker='o')\n",
    "\tax.plot(states[0][6], states[0][7], ms=12, color='green', marker='o')\n",
    "\tax.set(xlim=(-2, 2), ylim=(-4, 2))\n",
    "\t#return line, point,\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate,  frames = len(states), interval=20, repeat=False)\t#init_func=init, blit=True\n",
    "plt.close()\n",
    "#from matplotlib.animation import PillowWriter\n",
    "anim.save('training_plots/Agent_policy3.gif', writer='imagemagick')\t#dpi=300, writer=PillowWriter(fps=1))\t#imagemagick\n",
    "print(\"saved GIF\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Raw dataset and RL dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Offline_RL_dataset(nb_trials=list_data['cueballpos'][\"trial\"].iloc[-1])\n",
    "train_set.get_trajectories(list_data, rewards)\n",
    "#train_set.compute_mean_std(list_data)\n",
    "#train_set.normalize_states()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "states, actions, new_states, rewards, terminals = sample(train_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "for t in range(int(args[\"max_timesteps\"])):\n",
    "\tpolicy.train(replay_buffer, args[\"batch_size\"])\n",
    "\t'''\n",
    "\t# Evaluate episode\n",
    "\tif (t + 1) % args[\"eval_freq\"] == 0:\n",
    "\t\tprint(f\"Time steps: {t+1}\")\n",
    "\t\tevaluations.append(eval_policy(policy, args.env, args.seed, mean, std))\n",
    "\t\t#np.save(f\"./results/{file_name}\", evaluations)\n",
    "\t\t#if args.save_model: policy.save(f\"./models/{file_name}\")\n",
    "\t'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns score\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, eval_dataset, mean, std, eval_episodes=2):\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, done = eval_env.reset(), False\n",
    "\t\twhile not done:\n",
    "\t\t\tstate = (np.array(state).reshape(1,-1) - mean)/std\n",
    "\t\t\taction = policy.select_action(state)\n",
    "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\td4rl_score = eval_env.get_normalized_score(avg_reward) * 100\n",
    "\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}, D4RL score: {d4rl_score:.3f}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\treturn d4rl_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_j_steps = 25\n",
    "gamma = 0.98\n",
    "mag = MAGIC(gamma)\n",
    "info = [data.actions(),\n",
    "                data.rewards(),\n",
    "                data.base_propensity(),\n",
    "                data.target_propensity(),\n",
    "                Qs\n",
    "                ]\n",
    "magic_evaluation = mag.evaluate(info, num_j_steps, True)\n",
    "print(magic_evaluation[0], (magic_evaluation[0] - true )**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import _collections_abc\n",
    "\n",
    "def _count_elements(mapping, iterable):\n",
    "    'Tally elements from the iterable.'\n",
    "    mapping_get = mapping.get\n",
    "    for elem in iterable:\n",
    "        mapping[elem] = mapping_get(elem, 0) + 1\n",
    "'''\n",
    "try:                                    # Load C helper function if available\n",
    "    from _collections import _count_elements\n",
    "except ImportError:\n",
    "    pass\n",
    "'''\n",
    "\n",
    "class itemgetter:\n",
    "    \"\"\"\n",
    "    Return a callable object that fetches the given item(s) from its operand.\n",
    "    After f = itemgetter(2), the call f(r) returns r[2].\n",
    "    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\n",
    "    \"\"\"\n",
    "    __slots__ = ('_items', '_call')\n",
    "\n",
    "    def __init__(self, item, *items):\n",
    "        if not items:\n",
    "            self._items = (item,)\n",
    "            def func(obj):\n",
    "                return obj[item]\n",
    "            self._call = func\n",
    "        else:\n",
    "            self._items = items = (item,) + items\n",
    "            def func(obj):\n",
    "                return tuple(obj[i] for i in items)\n",
    "            self._call = func\n",
    "\n",
    "    def __call__(self, obj):\n",
    "        return self._call(obj)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s.%s(%s)' % (self.__class__.__module__,\n",
    "                              self.__class__.__name__,\n",
    "                              ', '.join(map(repr, self._items)))\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, self._items\n",
    "\n",
    "\n",
    "class Counter(dict):\n",
    "    '''Dict subclass for counting hashable items.  Sometimes called a bag\n",
    "    or multiset.  Elements are stored as dictionary keys and their counts\n",
    "    are stored as dictionary values.\n",
    "    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n",
    "    >>> c.most_common(3)                # three most common elements\n",
    "    [('a', 5), ('b', 4), ('c', 3)]\n",
    "    >>> sorted(c)                       # list all unique elements\n",
    "    ['a', 'b', 'c', 'd', 'e']\n",
    "    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n",
    "    'aaaaabbbbcccdde'\n",
    "    >>> sum(c.values())                 # total of all counts\n",
    "    15\n",
    "    >>> c['a']                          # count of letter 'a'\n",
    "    5\n",
    "    >>> for elem in 'shazam':           # update counts from an iterable\n",
    "    ...     c[elem] += 1                # by adding 1 to each element's count\n",
    "    >>> c['a']                          # now there are seven 'a'\n",
    "    7\n",
    "    >>> del c['b']                      # remove all 'b'\n",
    "    >>> c['b']                          # now there are zero 'b'\n",
    "    0\n",
    "    >>> d = Counter('simsalabim')       # make another counter\n",
    "    >>> c.update(d)                     # add in the second counter\n",
    "    >>> c['a']                          # now there are nine 'a'\n",
    "    9\n",
    "    >>> c.clear()                       # empty the counter\n",
    "    >>> c\n",
    "    Counter()\n",
    "    Note:  If a count is set to zero or reduced to zero, it will remain\n",
    "    in the counter until the entry is deleted or the counter is cleared:\n",
    "    >>> c = Counter('aaabbc')\n",
    "    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n",
    "    >>> c.most_common()                 # 'b' is still in, but its count is zero\n",
    "    [('a', 3), ('c', 1), ('b', 0)]\n",
    "    '''\n",
    "    # References:\n",
    "    #   http://en.wikipedia.org/wiki/Multiset\n",
    "    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n",
    "    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n",
    "    #   http://code.activestate.com/recipes/259174/\n",
    "    #   Knuth, TAOCP Vol. II section 4.6.3\n",
    "\n",
    "    def __init__(self, iterable=None, /, **kwds):\n",
    "        '''Create a new, empty Counter object.  And if given, count elements\n",
    "        from an input iterable.  Or, initialize the count from another mapping\n",
    "        of elements to their counts.\n",
    "        >>> c = Counter()                           # a new, empty counter\n",
    "        >>> c = Counter('gallahad')                 # a new counter from an iterable\n",
    "        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n",
    "        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.update(iterable, **kwds)\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        'The count of elements not in the Counter is zero.'\n",
    "        # Needed so that self[missing_item] does not raise KeyError\n",
    "        return 0\n",
    "\n",
    "    def total(self):\n",
    "        'Sum of the counts'\n",
    "        return sum(self.values())\n",
    "\n",
    "    def most_common(self, n=None):\n",
    "        '''List the n most common elements and their counts from the most\n",
    "        common to the least.  If n is None, then list all element counts.\n",
    "        >>> Counter('abracadabra').most_common(3)\n",
    "        [('a', 5), ('b', 2), ('r', 2)]\n",
    "        '''\n",
    "        # Emulate Bag.sortedByCount from Smalltalk\n",
    "        if n is None:\n",
    "            return sorted(self.items(), key=_itemgetter(1), reverse=True)\n",
    "\n",
    "        # Lazy import to speedup Python startup time\n",
    "        import heapq\n",
    "        return heapq.nlargest(n, self.items(), key=_itemgetter(1))\n",
    "\n",
    "    def elements(self):\n",
    "        '''Iterator over elements repeating each as many times as its count.\n",
    "        >>> c = Counter('ABCABC')\n",
    "        >>> sorted(c.elements())\n",
    "        ['A', 'A', 'B', 'B', 'C', 'C']\n",
    "        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n",
    "        >>> import math\n",
    "        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n",
    "        >>> math.prod(prime_factors.elements())\n",
    "        1836\n",
    "        Note, if an element's count has been set to zero or is a negative\n",
    "        number, elements() will ignore it.\n",
    "        '''\n",
    "        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n",
    "        return _chain.from_iterable(_starmap(_repeat, self.items()))\n",
    "\n",
    "    # Override dict methods where necessary\n",
    "\n",
    "    @classmethod\n",
    "    def fromkeys(cls, iterable, v=None):\n",
    "        # There is no equivalent method for counters because the semantics\n",
    "        # would be ambiguous in cases such as Counter.fromkeys('aaabbc', v=2).\n",
    "        # Initializing counters to zero values isn't necessary because zero\n",
    "        # is already the default value for counter lookups.  Initializing\n",
    "        # to one is easily accomplished with Counter(set(iterable)).  For\n",
    "        # more exotic cases, create a dictionary first using a dictionary\n",
    "        # comprehension or dict.fromkeys().\n",
    "        raise NotImplementedError(\n",
    "            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')\n",
    "\n",
    "    def update(self, iterable=None, /, **kwds):\n",
    "        '''Like dict.update() but add counts instead of replacing them.\n",
    "        Source can be an iterable, a dictionary, or another Counter instance.\n",
    "        >>> c = Counter('which')\n",
    "        >>> c.update('witch')           # add elements from another iterable\n",
    "        >>> d = Counter('watch')\n",
    "        >>> c.update(d)                 # add elements from another counter\n",
    "        >>> c['h']                      # four 'h' in which, witch, and watch\n",
    "        4\n",
    "        '''\n",
    "        # The regular dict.update() operation makes no sense here because the\n",
    "        # replace behavior results in the some of original untouched counts\n",
    "        # being mixed-in with all of the other counts for a mismash that\n",
    "        # doesn't have a straight-forward interpretation in most counting\n",
    "        # contexts.  Instead, we implement straight-addition.  Both the inputs\n",
    "        # and outputs are allowed to contain zero and negative counts.\n",
    "\n",
    "        if iterable is not None:\n",
    "            if isinstance(iterable, _collections_abc.Mapping):\n",
    "                if self:\n",
    "                    self_get = self.get\n",
    "                    for elem, count in iterable.items():\n",
    "                        self[elem] = count + self_get(elem, 0)\n",
    "                else:\n",
    "                    # fast path when counter is empty\n",
    "                    super().update(iterable)\n",
    "            else:\n",
    "                _count_elements(self, iterable)\n",
    "        if kwds:\n",
    "            self.update(kwds)\n",
    "\n",
    "    def subtract(self, iterable=None, /, **kwds):\n",
    "        '''Like dict.update() but subtracts counts instead of replacing them.\n",
    "        Counts can be reduced below zero.  Both the inputs and outputs are\n",
    "        allowed to contain zero and negative counts.\n",
    "        Source can be an iterable, a dictionary, or another Counter instance.\n",
    "        >>> c = Counter('which')\n",
    "        >>> c.subtract('witch')             # subtract elements from another iterable\n",
    "        >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n",
    "        >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n",
    "        0\n",
    "        >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n",
    "        -1\n",
    "        '''\n",
    "        if iterable is not None:\n",
    "            self_get = self.get\n",
    "            if isinstance(iterable, _collections_abc.Mapping):\n",
    "                for elem, count in iterable.items():\n",
    "                    self[elem] = self_get(elem, 0) - count\n",
    "            else:\n",
    "                for elem in iterable:\n",
    "                    self[elem] = self_get(elem, 0) - 1\n",
    "        if kwds:\n",
    "            self.subtract(kwds)\n",
    "\n",
    "    def copy(self):\n",
    "        'Return a shallow copy.'\n",
    "        return self.__class__(self)\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (dict(self),)\n",
    "\n",
    "    def __delitem__(self, elem):\n",
    "        'Like dict.__delitem__() but does not raise KeyError for missing values.'\n",
    "        if elem in self:\n",
    "            super().__delitem__(elem)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self:\n",
    "            return f'{self.__class__.__name__}()'\n",
    "        try:\n",
    "            # dict() preserves the ordering returned by most_common()\n",
    "            d = dict(self.most_common())\n",
    "        except TypeError:\n",
    "            # handle case where values are not orderable\n",
    "            d = dict(self)\n",
    "        return f'{self.__class__.__name__}({d!r})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        'True if all counts agree. Missing counts are treated as zero.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] == other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        'True if any counts disagree. Missing counts are treated as zero.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return not self == other\n",
    "\n",
    "    def __le__(self, other):\n",
    "        'True if all counts in self are a subset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] <= other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        'True if all counts in self are a proper subset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return self <= other and self != other\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        'True if all counts in self are a superset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] >= other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        'True if all counts in self are a proper superset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return self >= other and self != other\n",
    "\n",
    "    def __add__(self, other):\n",
    "        '''Add counts from two counters.\n",
    "        >>> Counter('abbb') + Counter('bcc')\n",
    "        Counter({'b': 4, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            newcount = count + other[elem]\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        ''' Subtract count, but keep only results with positive counts.\n",
    "        >>> Counter('abbbc') - Counter('bccd')\n",
    "        Counter({'b': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            newcount = count - other[elem]\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count < 0:\n",
    "                result[elem] = 0 - count\n",
    "        return result\n",
    "\n",
    "    def __or__(self, other):\n",
    "        '''Union is the maximum of value in either of the input counters.\n",
    "        >>> Counter('abbb') | Counter('bcc')\n",
    "        Counter({'b': 3, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            newcount = other_count if count < other_count else count\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __and__(self, other):\n",
    "        ''' Intersection is the minimum of corresponding counts.\n",
    "        >>> Counter('abbb') & Counter('bcc')\n",
    "        Counter({'b': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            newcount = count if count < other_count else other_count\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        return result\n",
    "\n",
    "    def __pos__(self):\n",
    "        'Adds an empty counter, effectively stripping negative and zero counts'\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            if count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __neg__(self):\n",
    "        '''Subtracts from an empty counter.  Strips positive and zero counts,\n",
    "        and flips the sign on negative counts.\n",
    "        '''\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            if count < 0:\n",
    "                result[elem] = 0 - count\n",
    "        return result\n",
    "\n",
    "    def _keep_positive(self):\n",
    "        '''Internal method to strip elements with a negative or zero count'''\n",
    "        nonpositive = [elem for elem, count in self.items() if not count > 0]\n",
    "        for elem in nonpositive:\n",
    "            del self[elem]\n",
    "        return self\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        '''Inplace add from another counter, keeping only positive counts.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c += Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 4, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, count in other.items():\n",
    "            self[elem] += count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        '''Inplace subtract counter, but keep only results with positive counts.\n",
    "        >>> c = Counter('abbbc')\n",
    "        >>> c -= Counter('bccd')\n",
    "        >>> c\n",
    "        Counter({'b': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, count in other.items():\n",
    "            self[elem] -= count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __ior__(self, other):\n",
    "        '''Inplace union is the maximum of value from either counter.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c |= Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 3, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, other_count in other.items():\n",
    "            count = self[elem]\n",
    "            if other_count > count:\n",
    "                self[elem] = other_count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __iand__(self, other):\n",
    "        '''Inplace intersection is the minimum of corresponding counts.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c &= Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 1})\n",
    "        '''\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            if other_count < count:\n",
    "                self[elem] = other_count\n",
    "        return self._keep_positive()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "class getQs(object):\n",
    "\tdef __init__(self, data, pi_e, processor, action_space_dim = 3):\n",
    "\t\tself.data = data\n",
    "\t\tself.pi_e = pi_e\n",
    "\t\tself.processor = processor\n",
    "\t\tself.action_space_dim = action_space_dim\n",
    "\n",
    "\tdef get(self, model):\n",
    "\t\tQs = []\n",
    "\t\tbatchsize = 1\n",
    "\t\tnum_batches = int(np.ceil(len(self.data)/batchsize))\n",
    "\t\t# frames = np.array([x['frames'] for x in self.trajectories])\n",
    "\t\tfor batchnum in trange(num_batches, desc='Batch'):\n",
    "\t\t\tlow_ = batchsize*batchnum\n",
    "\t\t\thigh_ = min(batchsize*(batchnum+1), len(self.data))\n",
    "\n",
    "\t\t\tpos = self.data.states(False, low_=low_,high_=high_)\n",
    "\t\t\tacts = self.data.actions()[low_:high_]\n",
    "\n",
    "\n",
    "\t\t\t# episodes = self.trajectories[low_:high_]\n",
    "\t\t\t# pos = np.vstack([np.vstack(x['x']) for x in episodes])\n",
    "\t\t\t# N = np.hstack([[low_ + n]*len(x['x']) for n,x in enumerate(episodes)])\n",
    "\t\t\t# acts = np.hstack([x['a'] for x in episodes])\n",
    "\t\t\t# pos = np.array([np.array(frames[int(N[idx])])[pos[idx].astype(int)] for idx in range(len(pos))])\n",
    "\t\t\ttraj_Qs = model.Q(self.pi_e, self.processor(pos))\n",
    "\n",
    "\t\t\ttraj_Qs = traj_Qs.reshape(-1, self.action_space_dim)\n",
    "\t\t\t# lengths = self.data.lengths()\n",
    "\n",
    "\t\t\t# endpts = np.cumsum(np.hstack([[0], lengths]))\n",
    "\t\t\t# for start,end in zip(endpts[:-1], endpts[1:]):\n",
    "\t\t\t# \tQs.append(traj_Qs[start:end])\n",
    "\t\t\tQs.append(traj_Qs)\n",
    "\n",
    "\t\treturn Qs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class defaultCNN(nn.Module):\n",
    "    def __init__(self, shape, action_space_dim):\n",
    "        super(defaultCNN, self).__init__()\n",
    "        self.c, self.h, self.w = shape\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(self.c, 16, (2,2)),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*(self.h-1)*(self.w-1), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=.001)\n",
    "            torch.nn.init.normal_(m.bias, mean=0.0, std=.001)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        output = self.net(state)\n",
    "        return torch.masked_select(output, action)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def predict_w_softmax(self, state):\n",
    "        return nn.Softmax()(self.net(state))\n",
    "\n",
    "class defaultModelBasedCNN(nn.Module):\n",
    "    def __init__(self, shape, action_space_dim):\n",
    "        super(defaultModelBasedCNN, self).__init__()\n",
    "        self.c, self.h, self.w = shape\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.c, 4, (5, 5)),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(4, 8, (3, 3)),\n",
    "        )\n",
    "\n",
    "        self.states_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, (3, 3)),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(16, action_space_dim, (5, 5)),\n",
    "        )\n",
    "        \n",
    "        self.rewards_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*(self.h-4-2)*(self.w-4-2), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim),\n",
    "        )\n",
    "\n",
    "        self.dones_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*(self.h-4-2)*(self.w-4-2), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=.001)\n",
    "            torch.nn.init.normal_(m.bias, mean=0.0, std=.001)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        T, R, D = self.states_head(self.features(state)), self.rewards_head(self.features(state)), self.dones_head(self.features(state))\n",
    "        return T[np.arange(len(action)), action.float().argmax(1), ...][:,None,:,:], torch.masked_select(R, action), torch.masked_select(D, action)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.states_head(self.features(state)), self.rewards_head(self.features(state)), self.dones_head(self.features(state))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std):\n",
    "        super(PropensityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "        self.std = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x):\n",
    "        # Sample from the Gaussian distribution with mean predicted by the model and fixed standard deviation\n",
    "        mean = self.forward(x)\n",
    "        std = torch.tensor(self.std).expand_as(mean)\n",
    "        return torch.normal(mean, std)\n",
    "\n",
    "# Define the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim, action_dim, std=0.1)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop through the dataset and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the loss and backpropagate\n",
    "    loss = criterion(model(state), action)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std):\n",
    "        super(PropensityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "        self.std = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x):\n",
    "        # Sample from the Gaussian distribution with mean predicted by the model and fixed standard deviation\n",
    "        mean = self.forward(x)\n",
    "        std = torch.tensor(self.std).expand_as(mean)\n",
    "        return torch.normal(mean, std)\n",
    "\n",
    "# Define the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim, action_dim, std=0.1)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop through the dataset and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the loss and backpropagate\n",
    "    loss = criterion(model(state), action)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std=1.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_dim)\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x).numpy()\n",
    "    \n",
    "    def log_prob(self, x, actions):\n",
    "        means = self.forward(x)\n",
    "        log_prstate = -0.5 * ((actions - means) / self.std) ** 2 - 0.5 * np.log(2 * np.pi) - np.log(self.std)\n",
    "        return log_prstate.sum(1, keepdim=True)\n",
    "\n",
    "# Create the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim=state_dim, action_dim=action_dim)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Loop over the data and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Compute the log probabilities of the actions\n",
    "    log_prstate = model.log_prob(state, action)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = -log_prstate.mean()\n",
    "    \n",
    "    # Perform backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Use the model to predict the propensity of an action given a state\n",
    "#state = np.array([[1, 2, 3, 4]])\n",
    "#print(model.predict(torch.from_numpy(state).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_propensity(self, behavior_data):    #cfg):\n",
    "    # WARN: Only works in tabular env with discrete action space. Current implementation is a max likelihood\n",
    "\n",
    "    model = defaultCNN(self.states()[0][0].shape, self.n_actions) #cfg.to_regress_pi_b['model']\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    self.processed_data = self.fill()\n",
    "\n",
    "    batch_size = 32 #cfg.to_regress_pi_b['batch_size']\n",
    "    dataset_length = self.num_tuples()\n",
    "    perm = np.random.permutation(range(dataset_length))\n",
    "    eighty_percent_of_set = int(.8*len(perm))\n",
    "    training_idxs = perm[:eighty_percent_of_set]\n",
    "    validation_idxs = perm[eighty_percent_of_set:]\n",
    "    training_steps_per_epoch = int(np.ceil(len(training_idxs)/float(batch_size)))\n",
    "    validation_steps_per_epoch = int(np.ceil(len(validation_idxs)/float(batch_size)))\n",
    "\n",
    "    for k in tqdm(range(100)):#cfg.to_regress_pi_b['max_epochs']\n",
    "        \n",
    "        train_gen = self.generator(training_idxs, fixed_permutation=True, batch_size=batch_size)\n",
    "        val_gen = self.generator(validation_idxs, fixed_permutation=True, batch_size=batch_size)\n",
    "\n",
    "        # TODO: earlyStopping, LR reduction\n",
    "\n",
    "        for step in range(training_steps_per_epoch):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                (inp, out) = next(train_gen)\n",
    "                states = torch.from_numpy(inp).float()\n",
    "                actions = torch.from_numpy(out).float().argmax(1)\n",
    "\n",
    "            prediction = model.predict_w_softmax(states)\n",
    "            \n",
    "            loss = nn.NLLLoss()(torch.log(prediction), actions)\n",
    "                                        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), 1.0)   #cfg.to_regress_pi_b['clipnorm']\n",
    "            optimizer.step()\n",
    "\n",
    "    for episode_num, states in enumerate(np.squeeze(self.states())):\n",
    "        base_propensity = []\n",
    "        for state in states:\n",
    "            base_propensity.append(model.predict_w_softmax(torch.from_numpy(state[None,None,...]).float()).detach().numpy()[0].tolist())\n",
    "\n",
    "        self.trajectories[episode_num]['base_propensity'] = base_propensity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAGIC algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omega(self):\n",
    "        return np.array([[episode['target_propensity'][idx][int(act)]/episode['base_propensity'][idx][int(act)] for idx,act in enumerate(episode['a'])] for episode in self.trajectories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGIC(object):\n",
    "    \"\"\"Algorithm: MAGIC.\n",
    "    \"\"\"\n",
    "    NUM_SUBSETS_FOR_CB_ESTIMATES = 25\n",
    "    CONFIDENCE_INTERVAL = 0.9\n",
    "    NUM_BOOTSTRAP_SAMPLES = 50\n",
    "    BOOTSTRAP_SAMPLE_PCT = 0.5\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "            Discount factor.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def evaluate(self, info, num_j_steps, is_wdr, return_Qs = False):\n",
    "        \"\"\"Get MAGIC estimate from Q + IPS.\n",
    "        Parameters\n",
    "        ----------\n",
    "        info : list\n",
    "            [list of actions, list of rewards, list of base propensity, list of target propensity, list of Qhat]\n",
    "        num_j_steps : int\n",
    "            Parameter to MAGIC algorithm\n",
    "        is_wdr : bool\n",
    "            Use Weighted Doubly Robust?\n",
    "        return_Qs : bool\n",
    "            Return trajectory-wise estimate alongside full DR estimate?\n",
    "            Default: False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            [MAGIC estimate, normalized MAGIC, std error, normalized std error]\n",
    "            If return_Qs is true, also returns trajectory-wise estimate\n",
    "        \"\"\"\n",
    "\n",
    "        (actions,\n",
    "        rewards,\n",
    "        base_propensity,\n",
    "        target_propensities,\n",
    "        estimated_q_values) = MAGIC.transform_to_equal_length_trajectories(*info)\n",
    "\n",
    "        num_trajectories = actions.shape[0]\n",
    "        trajectory_length = actions.shape[1]\n",
    "\n",
    "        j_steps = [float(\"inf\")]\n",
    "\n",
    "        if num_j_steps > 1:\n",
    "            j_steps.append(-1)\n",
    "        if num_j_steps > 2:\n",
    "            interval = trajectory_length // (num_j_steps - 1)\n",
    "            j_steps.extend([i * interval for i in range(1, num_j_steps - 1)])\n",
    "\n",
    "        base_propensity_for_logged_action = np.sum(\n",
    "            np.multiply(base_propensity, actions), axis=2\n",
    "        )\n",
    "        target_propensity_for_logged_action = np.sum(\n",
    "            np.multiply(target_propensities, actions), axis=2\n",
    "        )\n",
    "        estimated_q_values_for_logged_action = np.sum(\n",
    "            np.multiply(estimated_q_values, actions), axis=2\n",
    "        )\n",
    "        estimated_state_values = np.sum(\n",
    "            np.multiply(target_propensities, estimated_q_values), axis=2\n",
    "        )\n",
    "\n",
    "        importance_weights = target_propensity_for_logged_action / base_propensity_for_logged_action\n",
    "        importance_weights[np.isnan(importance_weights)] = 0.\n",
    "        importance_weights = np.cumprod(importance_weights, axis=1)\n",
    "        importance_weights = MAGIC.normalize_importance_weights(\n",
    "            importance_weights, is_wdr\n",
    "        )\n",
    "\n",
    "        importance_weights_one_earlier = (\n",
    "            np.ones([num_trajectories, 1]) * 1.0 / num_trajectories\n",
    "        )\n",
    "        importance_weights_one_earlier = np.hstack(\n",
    "            [importance_weights_one_earlier, importance_weights[:, :-1]]\n",
    "        )\n",
    "\n",
    "        discounts = np.logspace(\n",
    "            start=0, stop=trajectory_length - 1, num=trajectory_length, base=self.gamma\n",
    "        )\n",
    "\n",
    "        j_step_return_trajectories = []\n",
    "        for j_step in j_steps:\n",
    "            j_step_return_trajectories.append(\n",
    "                MAGIC.calculate_step_return(\n",
    "                    rewards,\n",
    "                    discounts,\n",
    "                    importance_weights,\n",
    "                    importance_weights_one_earlier,\n",
    "                    estimated_state_values,\n",
    "                    estimated_q_values_for_logged_action,\n",
    "                    j_step,\n",
    "                )\n",
    "            )\n",
    "        j_step_return_trajectories = np.array(j_step_return_trajectories)\n",
    "\n",
    "        j_step_returns = np.sum(j_step_return_trajectories, axis=1)\n",
    "\n",
    "        if len(j_step_returns) == 1:\n",
    "            weighted_doubly_robust = j_step_returns[0]\n",
    "            weighted_doubly_robust_std_error = 0.0\n",
    "        else:\n",
    "            # break trajectories into several subsets to estimate confidence bounds\n",
    "            infinite_step_returns = []\n",
    "            num_subsets = int(\n",
    "                min(\n",
    "                    num_trajectories / 2,\n",
    "                    MAGIC.NUM_SUBSETS_FOR_CB_ESTIMATES,\n",
    "                )\n",
    "            )\n",
    "            interval = num_trajectories / num_subsets\n",
    "            for i in range(num_subsets):\n",
    "                trajectory_subset = np.arange(\n",
    "                    int(i * interval), int((i + 1) * interval)\n",
    "                )\n",
    "                importance_weights = (\n",
    "                    target_propensity_for_logged_action[trajectory_subset]\n",
    "                    / base_propensity_for_logged_action[trajectory_subset]\n",
    "                )\n",
    "                importance_weights[np.isnan(importance_weights)] = 0.\n",
    "                importance_weights = np.cumprod(importance_weights, axis=1)\n",
    "                importance_weights = MAGIC.normalize_importance_weights(\n",
    "                    importance_weights, is_wdr\n",
    "                )\n",
    "                importance_weights_one_earlier = (\n",
    "                    np.ones([len(trajectory_subset), 1]) * 1.0 / len(trajectory_subset)\n",
    "                )\n",
    "                importance_weights_one_earlier = np.hstack(\n",
    "                    [importance_weights_one_earlier, importance_weights[:, :-1]]\n",
    "                )\n",
    "                infinite_step_return = np.sum(\n",
    "                    MAGIC.calculate_step_return(\n",
    "                        rewards[trajectory_subset],\n",
    "                        discounts,\n",
    "                        importance_weights,\n",
    "                        importance_weights_one_earlier,\n",
    "                        estimated_state_values[trajectory_subset],\n",
    "                        estimated_q_values_for_logged_action[trajectory_subset],\n",
    "                        float(\"inf\"),\n",
    "                    )\n",
    "                )\n",
    "                infinite_step_returns.append(infinite_step_return)\n",
    "\n",
    "            # Compute weighted_doubly_robust mean point estimate using all data\n",
    "            weighted_doubly_robust, xs = self.compute_weighted_doubly_robust_point_estimate(\n",
    "                j_steps,\n",
    "                num_j_steps,\n",
    "                j_step_returns,\n",
    "                infinite_step_returns,\n",
    "                j_step_return_trajectories,\n",
    "            )\n",
    "\n",
    "            # Use bootstrapping to compute weighted_doubly_robust standard error\n",
    "            bootstrapped_means = []\n",
    "            sample_size = int(\n",
    "                MAGIC.BOOTSTRAP_SAMPLE_PCT\n",
    "                * num_subsets\n",
    "            )\n",
    "            for _ in range(\n",
    "                MAGIC.NUM_BOOTSTRAP_SAMPLES\n",
    "            ):\n",
    "                random_idxs = np.random.choice(num_j_steps, sample_size, replace=False)\n",
    "                random_idxs.sort()\n",
    "                wdr_estimate = self.compute_weighted_doubly_robust_point_estimate(\n",
    "                    j_steps=[j_steps[i] for i in random_idxs],\n",
    "                    num_j_steps=sample_size,\n",
    "                    j_step_returns=j_step_returns[random_idxs],\n",
    "                    infinite_step_returns=infinite_step_returns,\n",
    "                    j_step_return_trajectories=j_step_return_trajectories[random_idxs],\n",
    "                )\n",
    "                bootstrapped_means.append(wdr_estimate)\n",
    "            weighted_doubly_robust_std_error = np.std(bootstrapped_means)\n",
    "\n",
    "        episode_values = np.sum(np.multiply(rewards, discounts), axis=1)\n",
    "        denominator = np.nanmean(episode_values)\n",
    "        if abs(denominator) < 1e-6:\n",
    "            return [0]*4\n",
    "\n",
    "        # print (weighted_doubly_robust,\n",
    "        #         weighted_doubly_robust / denominator,\n",
    "        #         weighted_doubly_robust_std_error,\n",
    "        #         weighted_doubly_robust_std_error / denominator)\n",
    "\n",
    "        if return_Qs:\n",
    "            return [weighted_doubly_robust,\n",
    "                    weighted_doubly_robust / denominator,\n",
    "                    weighted_doubly_robust_std_error,\n",
    "                    weighted_doubly_robust_std_error / denominator], np.dot(xs, j_step_return_trajectories)\n",
    "        else:\n",
    "            return [weighted_doubly_robust,\n",
    "                    weighted_doubly_robust / denominator,\n",
    "                    weighted_doubly_robust_std_error,\n",
    "                    weighted_doubly_robust_std_error / denominator]\n",
    "\n",
    "    def compute_weighted_doubly_robust_point_estimate(\n",
    "        self,\n",
    "        j_steps,\n",
    "        num_j_steps,\n",
    "        j_step_returns,\n",
    "        infinite_step_returns,\n",
    "        j_step_return_trajectories,\n",
    "    ):\n",
    "        low_bound, high_bound = MAGIC.confidence_bounds(\n",
    "            infinite_step_returns,\n",
    "            MAGIC.CONFIDENCE_INTERVAL,\n",
    "        )\n",
    "        # decompose error into bias + variance\n",
    "        j_step_bias = np.zeros([num_j_steps])\n",
    "        where_lower = np.where(j_step_returns < low_bound)[0]\n",
    "        j_step_bias[where_lower] = low_bound - j_step_returns[where_lower]\n",
    "        where_higher = np.where(j_step_returns > high_bound)[0]\n",
    "        j_step_bias[where_higher] = j_step_returns[where_higher] - high_bound\n",
    "\n",
    "        covariance = np.cov(j_step_return_trajectories)\n",
    "        error = covariance + j_step_bias.T * j_step_bias\n",
    "\n",
    "        # minimize mse error\n",
    "        constraint = {\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - 1.0}\n",
    "\n",
    "        x = np.zeros([len(j_steps)])\n",
    "        res = sp.optimize.minimize(\n",
    "            mse_loss,\n",
    "            x,\n",
    "            args=error,\n",
    "            constraints=constraint,\n",
    "            bounds=[(0, 1) for _ in range(x.shape[0])],\n",
    "        )\n",
    "        x = np.array(res.x)\n",
    "        return float(np.dot(x, j_step_returns)), x\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_to_equal_length_trajectories(\n",
    "        actions,\n",
    "        rewards,\n",
    "        logged_propensities,\n",
    "        target_propensities,\n",
    "        estimated_q_values,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Take in samples (action, rewards, propensities, etc.) and output lists\n",
    "        of equal-length trajectories (episodes) according to terminals.\n",
    "        As the raw trajectories are of various lengths, the shorter ones are\n",
    "        filled with zeros(ones) at the end.\n",
    "        \"\"\"\n",
    "        num_actions = len(target_propensities[0][0])\n",
    "\n",
    "        def to_equal_length(x, fill_value):\n",
    "            x_equal_length = np.array(\n",
    "                list(itertools.zip_longest(*x, fillvalue=fill_value))\n",
    "            ).swapaxes(0, 1)\n",
    "            return x_equal_length\n",
    "\n",
    "        action_trajectories = to_equal_length(\n",
    "            [np.eye(num_actions)[act] for act in actions], np.zeros([num_actions])\n",
    "        )\n",
    "        reward_trajectories = to_equal_length(rewards, 0)\n",
    "        logged_propensity_trajectories = to_equal_length(\n",
    "            logged_propensities, np.zeros([num_actions])\n",
    "        )\n",
    "        target_propensity_trajectories = to_equal_length(\n",
    "            target_propensities, np.zeros([num_actions])\n",
    "        )\n",
    "\n",
    "        # Hack for now. Delete.\n",
    "        estimated_q_values = [[np.hstack(y).tolist() for y in x] for x in estimated_q_values]\n",
    "\n",
    "        Q_value_trajectories = to_equal_length(\n",
    "            estimated_q_values, np.zeros([num_actions])\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            action_trajectories,\n",
    "            reward_trajectories,\n",
    "            logged_propensity_trajectories,\n",
    "            target_propensity_trajectories,\n",
    "            Q_value_trajectories,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_importance_weights(\n",
    "        importance_weights, is_wdr\n",
    "    ):\n",
    "        if is_wdr:\n",
    "            sum_importance_weights = np.sum(importance_weights, axis=0)\n",
    "            where_zeros = np.where(sum_importance_weights == 0.0)[0]\n",
    "            sum_importance_weights[where_zeros] = len(importance_weights)\n",
    "            importance_weights[:, where_zeros] = 1.0\n",
    "            importance_weights /= sum_importance_weights\n",
    "            return importance_weights\n",
    "        else:\n",
    "            importance_weights /= importance_weights.shape[0]\n",
    "            return importance_weights\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_step_return(\n",
    "        rewards,\n",
    "        discounts,\n",
    "        importance_weights,\n",
    "        importance_weights_one_earlier,\n",
    "        estimated_state_values,\n",
    "        estimated_q_values,\n",
    "        j_step,\n",
    "    ):\n",
    "        trajectory_length = len(rewards[0])\n",
    "        num_trajectories = len(rewards)\n",
    "        j_step = int(min(j_step, trajectory_length - 1))\n",
    "\n",
    "        weighted_discounts = np.multiply(discounts, importance_weights)\n",
    "        weighted_discounts_one_earlier = np.multiply(\n",
    "            discounts, importance_weights_one_earlier\n",
    "        )\n",
    "\n",
    "        importance_sampled_cumulative_reward = np.sum(\n",
    "            np.multiply(weighted_discounts[:, : j_step + 1], rewards[:, : j_step + 1]),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        if j_step < trajectory_length - 1:\n",
    "            direct_method_value = (\n",
    "                weighted_discounts_one_earlier[:, j_step + 1]\n",
    "                * estimated_state_values[:, j_step + 1]\n",
    "            )\n",
    "        else:\n",
    "            direct_method_value = np.zeros([num_trajectories])\n",
    "\n",
    "        control_variate = np.sum(\n",
    "            np.multiply(\n",
    "                weighted_discounts[:, : j_step + 1], estimated_q_values[:, : j_step + 1]\n",
    "            )\n",
    "            - np.multiply(\n",
    "                weighted_discounts_one_earlier[:, : j_step + 1],\n",
    "                estimated_state_values[:, : j_step + 1],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        j_step_return = (\n",
    "            importance_sampled_cumulative_reward + direct_method_value - control_variate\n",
    "        )\n",
    "\n",
    "        return j_step_return\n",
    "\n",
    "    @staticmethod\n",
    "    def confidence_bounds(x, confidence):\n",
    "        n = len(x)\n",
    "        m, se = np.mean(x), sp.stats.sem(x)\n",
    "        h = se * sp.stats.t._ppf((1 + confidence) / 2.0, n - 1)\n",
    "        return m - h, m + h\n",
    "\n",
    "\n",
    "def mse_loss(x, error):\n",
    "    return np.dot(np.dot(x, error), x.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
