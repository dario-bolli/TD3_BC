{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "'''\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import scipy.stats as ss\n",
    "import pingouin as pg\n",
    "\n",
    "import scipy.signal as sci\n",
    "import math\n",
    "import scipy as sp\n",
    "\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "\n",
    "import re\n",
    "\n",
    "import copy\n",
    "import argparse\n",
    "import d4rl\n",
    "from collections import deque'''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actions(cuevel, cueDirection):\n",
    "    cueAngle = compute_angle(cuevel, cueDirection)  #min_max_standardisation()\n",
    "\n",
    "    #cueacceleration = compute_acceleration(cuevel.to_numpy())\n",
    "    '''cueMagnitude,_ = compute_impulseForce(cuevel, cueDirection)\n",
    "    cueMagnitude = min_max_standardisation(cueMagnitude)\n",
    "    actions = np.hstack((cueAngle.reshape(-1,1), cueMagnitude.to_numpy().reshape(-1,1)))'''\n",
    "    actions = np.hstack((cueAngle.reshape(-1,1), cuevel.to_numpy())) #, cueacceleration))\n",
    "    return actions  #cueAngle, cueMagnitude\n",
    "\n",
    "def compute_acceleration(cuevel):\n",
    "    dt = 0.0111\n",
    "    acc = np.zeros(cuevel.shape)\n",
    "    for i in range(len(cuevel)):\n",
    "        if i == 0:\n",
    "            acc[i][:] = (np.abs(cuevel[i][:]-0))/dt\n",
    "            if acc[i][0] > 3 or acc[i][1] > 3:\n",
    "                print(\"cuevel values for big acc: \", cuevel[i][:])\n",
    "        else:\n",
    "            acc[i][:] = (np.abs(cuevel[i][:]-cuevel[i-1][:]))/dt\n",
    "            if acc[i][0] > 8 or acc[i][1] > 8:\n",
    "                print(\"cuevel values for big acc: \", cuevel[i][:], cuevel[i-1][:], i)\n",
    "    \n",
    "    return acc\n",
    "        \n",
    "def min_max_standardisation(array):\n",
    "    #array must be 1-dimensional\n",
    "    array = (array - array.min()) / (array.max() - array.min())\n",
    "    return array\n",
    "\n",
    "def compute_angle(cuevel, cueDirection):\n",
    "    #cueAngle1 = np.rad2deg(np.arctan2(cuevel['z'].values, cuevel['x'].values))\n",
    "    #print(\"cue Angle vel: \", cueAngle1)\n",
    "    cueAngle = np.rad2deg(np.arctan2(cueDirection['z'].values, cueDirection['x'].values))\n",
    "    #print(\"cue Angle Direction: \", cueAngle)\n",
    "    return cueAngle\n",
    "\n",
    "def compute_impulseForce(cuevel, cuedirection):\n",
    "    impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "    shotMagnitude = np.zeros(1)\n",
    "    shotDir = np.zeros(cuedirection.shape)\n",
    "    #Reward: magnitude range\n",
    "    lbMagnitude = 0.1   #0.516149\n",
    "    ubMagnitude = 3 #0.882607\n",
    "\n",
    "    shotMagnitude = np.sqrt(np.square(cuevel).sum(axis=1))\n",
    "    #np.linalg.norm(cuevel, axis=1)\n",
    "    for i in range(cuevel.shape[0]):\n",
    "        if shotMagnitude[i] > ubMagnitude:\n",
    "            shotMagnitude[i] = ubMagnitude\n",
    "        elif shotMagnitude[i] < lbMagnitude:\n",
    "            shotMagnitude[i] = 0\n",
    "\n",
    "        shotDir[i][0] = cuedirection[\"x\"].iloc[i]\n",
    "        shotDir[i][1] = cuedirection[\"z\"].iloc[i]\n",
    "        if shotMagnitude[i] == 0:\n",
    "            impulseForce[i][:] = 0\n",
    "        else:\n",
    "            impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "    return shotMagnitude, impulseForce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_reward(Angle_data): #Success, SuccessFunnel_table, SuccessMedian_table):\n",
    "    print(\"Reward function\")\n",
    "    reward = np.zeros(len(Angle_data.Trial))\n",
    "    \n",
    "    for i in range(len(Angle_data.Trial)):\n",
    "        if Angle_data['Success'][i] == 1:\n",
    "            reward[i] = 100\n",
    "        elif Angle_data['SuccessFunnel'][i] == 1:\n",
    "            reward[i] = 100\n",
    "        #elif SuccessMedian_table.iloc[i] == 1:     #Agent will not understand reward for trial better than past 10 trials\n",
    "            #reward[i] = 20\n",
    "        else:\n",
    "            reward[i] = -10\n",
    "\n",
    "    return reward\n",
    "\n",
    "def static_for_n_timesteps(cueballvel, redballvel, timestep, n):\n",
    "    err = 0.001\n",
    "    count=0\n",
    "    for i in range(timestep, timestep+n, 1):\n",
    "        if np.linalg.norm(cueballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) < err and np.linalg.norm(redballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) < err:\n",
    "            count += 1\n",
    "    if count == n:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def start_hit_timesteps(subjData): #, TrialNumber):\n",
    "    \n",
    "    print(\"start hit timesteps function\")\n",
    "    cueballPos = subjData[\"cbpos\"]\n",
    "    cueballvel = subjData[\"cbvel\"]\n",
    "    redballvel = subjData[\"rbvel\"]\n",
    "    cuevel = subjData[\"cuevel\"]\n",
    "    #Find when cue hits cueball\n",
    "    hit_ind=[]\n",
    "    start_ind=[]\n",
    "    n = 10\n",
    "    threshold = 0.1\n",
    "\n",
    "    miss_hit = False\n",
    "\n",
    "    prev_trial = 0\n",
    "    block_until_next_trial = True\n",
    "    for i in range(len(cueballPos)):\n",
    "        #The first 200 timesteps approximately are calibration and parasite movements\n",
    "        if i > 200:   #!= 0:\n",
    "            #New trial started\n",
    "            if cueballPos[\"trial\"].iloc[i] > prev_trial:\n",
    "                if miss_hit == True and len(hit_ind) < len(start_ind):   \n",
    "                    hit_ind = np.append(hit_ind, start_ind[-1]+350)#on average hitting cueball after 350 timesteps\n",
    "                    miss_hit = False \n",
    "                if static_for_n_timesteps(cueballvel, redballvel, i, n):\n",
    "                    #Wait for cueball vel y-axis and redball vel y-axis to be zero after new trial started\n",
    "                    start_ind = np.append(start_ind, i)\n",
    "                    prev_trial = cueballPos[\"trial\"].iloc[i]\n",
    "                    block_until_next_trial = False\n",
    "                    miss_hit = True\n",
    "            elif cueballPos[\"trial\"].iloc[i] == prev_trial:\n",
    "                if np.linalg.norm(cueballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) > threshold and block_until_next_trial == False:\n",
    "                    #Add 6 timesteps for margin\n",
    "                    hit_ind = np.append(hit_ind, i+5)\n",
    "                    block_until_next_trial = True\n",
    "                    miss_hit = False\n",
    "\n",
    "        #if last Trial is missed\n",
    "        if i == len(cueballPos.index) and miss_hit:\n",
    "            hit_ind = np.append(hit_ind, start_ind[-1]+350)#on average hitting cueball after 350 timesteps\n",
    "\n",
    "    if len(start_ind) != 250 or len(hit_ind) != 250:\n",
    "        print(\"WARNING: either missed a start-hit index, or a trial was not properly recorded\")\n",
    "        #raise ValueError( \"Missed an index. start ind size\", len(start_ind), \"and hit ind size: \", len(hit_ind))\n",
    "    for i in range(len(start_ind)):\n",
    "        if start_ind[i] >= hit_ind[i]:\n",
    "            raise ValueError(\"start ind > hit_ind\", i , start_ind, hit_ind)\n",
    "    return start_ind.astype(int), hit_ind.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_hit_reduced_1_timesteps(subjData):\n",
    "    \n",
    "    print(\"start hit timesteps function\")\n",
    "    cueballPos = subjData[\"cbpos\"]\n",
    "    cueballvel = subjData[\"cbvel\"]\n",
    "    redballvel = subjData[\"rbvel\"]\n",
    "    cuevel = subjData[\"cuevel\"]\n",
    "    #Find when cue hits cueball\n",
    "    hit_ind=[]\n",
    "    start_ind=[]\n",
    "    n = 10\n",
    "    threshold = 0.06\n",
    "\n",
    "    start_movement = False\n",
    "\n",
    "    prev_trial = 0\n",
    "    start_movement_ind = 0\n",
    "    window_before_hit = 1   #30  #200\n",
    "    window_after_hit = 1    #20   #50\n",
    "\n",
    "    for i in range(len(cueballPos[\"trial\"])):\n",
    "        #The first 200 timesteps approximately are calibration and parasite movements\n",
    "        if i > 200:   #!= 0:\n",
    "            #New trial started\n",
    "            if cueballPos[\"trial\"].iloc[i] > prev_trial:\n",
    "                if static_for_n_timesteps(cueballvel, redballvel, i, n):\n",
    "                    #Wait for cueball vel y-axis and redball vel y-axis to be zero after new trial started\n",
    "                    start_movement_ind = i\n",
    "                    start_movement = True\n",
    "                    prev_trial = cueballPos[\"trial\"].iloc[i]\n",
    "\n",
    "            elif cueballPos[\"trial\"].iloc[i] == prev_trial and start_movement == True:        \n",
    "                if np.linalg.norm(cueballvel[[\"x\", \"z\"]].iloc[i].to_numpy()) > threshold:\n",
    "                    if len(start_ind) > 0:\n",
    "                        if cueballPos[\"trial\"][i-window_before_hit] != cueballPos[\"trial\"][start_ind[-1]]:\n",
    "                            start_ind = np.append(start_ind, i-window_before_hit)\n",
    "                        else:\n",
    "                            start_ind = np.append(start_ind, start_movement_ind)\n",
    "                    else:\n",
    "                        start_ind = np.append(start_ind, i-window_before_hit)\n",
    "                    #if cueballPos[\"trial\"][start_ind[i]] == cueballPos[\"trial\"][start_ind[i+1]]:\n",
    "                        #print(\"same trial 2 index\", start_movement, cueballPos[\"trial\"].iloc[i], prev_trial)\n",
    "                    hit_ind = np.append(hit_ind, i+window_after_hit)   #+1 when selecting from trajectory\n",
    "                    start_movement = False\n",
    "                        \n",
    "    if len(start_ind) != 250 or len(hit_ind) != 250:\n",
    "        print(\"WARNING: either missed a start-hit index, or a trial was not properly recorded\")\n",
    "    for i in range(len(start_ind)-1):\n",
    "        if cueballPos[\"trial\"][start_ind[i]] == cueballPos[\"trial\"][start_ind[i+1]]:\n",
    "            print(\"start index \", i, \" and start index \", i+1, \"have same trial number\")\n",
    "        if start_ind[i] >= hit_ind[i]:\n",
    "            raise ValueError(\"start ind > hit_ind\", i , start_ind[i], hit_ind[i])\n",
    "    return start_ind, hit_ind#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_hit_reduced_timesteps(subjData):\n",
    "    \n",
    "    print(\"start hit timesteps function\")\n",
    "    cueballPos = subjData[\"cbpos\"]\n",
    "    cueballvel = subjData[\"cbvel\"]\n",
    "    redballvel = subjData[\"rbvel\"]\n",
    "    cuevel = subjData[\"cuevel\"]\n",
    "    #Find when cue hits cueball\n",
    "    hit_ind=[]\n",
    "    start_ind=[]\n",
    "    n = 10\n",
    "    threshold = 0.06\n",
    "\n",
    "    start_movement = False\n",
    "\n",
    "    prev_trial = 0\n",
    "    start_movement_ind = 0\n",
    "    window_before_hit = 100  #200\n",
    "    window_after_hit = 40   #50\n",
    "    count_miss = 1\n",
    "\n",
    "    for i in range(len(cueballPos[\"trial\"])):\n",
    "        #The first 200 timesteps approximately are calibration and parasite movements\n",
    "        if i > 200:   #!= 0:\n",
    "            #New trial started\n",
    "            if cueballPos[\"trial\"].iloc[i] > prev_trial:\n",
    "                if cueballPos[\"trial\"].iloc[i]== (len(start_ind) - count_miss):\n",
    "                    print(\"WARNING: no start and hit index defined for trial \", prev_trial)\n",
    "                    count_miss += 1\n",
    "                if static_for_n_timesteps(cueballvel, redballvel, i, n):\n",
    "                    #Wait for cueball vel y-axis and redball vel y-axis to be zero after new trial started\n",
    "                    start_movement_ind = i\n",
    "                    start_movement = True\n",
    "                    prev_trial = cueballPos[\"trial\"].iloc[i]\n",
    "\n",
    "            elif cueballPos[\"trial\"].iloc[i] == prev_trial and start_movement == True:        \n",
    "                if np.linalg.norm(cueballvel[[\"x\", \"z\"]].iloc[i].to_numpy()) > threshold:\n",
    "                    if len(start_ind) > 0:\n",
    "                        if cueballPos[\"trial\"][i-window_before_hit] != cueballPos[\"trial\"][start_ind[-1]]:\n",
    "                            start_ind = np.append(start_ind, i-window_before_hit)\n",
    "                        else:\n",
    "                            start_ind = np.append(start_ind, start_movement_ind)\n",
    "                    else:\n",
    "                        start_ind = np.append(start_ind, i-window_before_hit)\n",
    "                    #if cueballPos[\"trial\"][start_ind[i]] == cueballPos[\"trial\"][start_ind[i+1]]:\n",
    "                        #print(\"same trial 2 index\", start_movement, cueballPos[\"trial\"].iloc[i], prev_trial)\n",
    "                    hit_ind = np.append(hit_ind, i+window_after_hit)   #+1 when selecting from trajectory\n",
    "                    start_movement = False\n",
    "                        \n",
    "    if len(start_ind) < 250 or len(hit_ind) < 250:\n",
    "        print(\"WARNING: either missed a start-hit index, or a trial was not properly recorded\")\n",
    "    for i in range(len(start_ind)-1):\n",
    "        if cueballPos[\"trial\"][start_ind[i]] == cueballPos[\"trial\"][start_ind[i+1]]:\n",
    "            print(\"start index \", i, \" and start index \", i+1, \"have same trial number\")\n",
    "        if start_ind[i] >= hit_ind[i]:\n",
    "            raise ValueError(\"start ind > hit_ind\", i , start_ind[i], hit_ind[i])\n",
    "    return start_ind, hit_ind#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Warning normalization mean and std should be the same for all positions (otherwise substract mean and we have multiple zero locations)\n",
    "def normalize(self, eps = 1e-3):\n",
    "\t\tmean_x = self[\"x\"].mean(0,keepdims=True)\n",
    "\t\tstd_x = self[\"x\"].std(0,keepdims=True) + eps\n",
    "\t\tmean_y = self[\"y\"].mean(0,keepdims=True)\n",
    "\t\tstd_y = self[\"y\"].std(0,keepdims=True) + eps\n",
    "\t\tmean_z = self[\"z\"].mean(0,keepdims=True)\n",
    "\t\tstd_z = self[\"z\"].std(0,keepdims=True) + eps\n",
    "\t\tself[\"x\"] = (self[\"x\"] - mean_x)/std_x\n",
    "\t\tself[\"y\"] = (self[\"y\"] - mean_x)/std_x\n",
    "\t\tself[\"z\"] = (self[\"z\"] - mean_x)/std_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error task\n",
    "\n",
    "def importDataError (initials, path, pocket, lbThreshold = -9.364594, ubThreshold = 4.675092):\n",
    "\n",
    "    '''Imports Behavioural Data for a subject performing Error task\n",
    "\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right corresponding to the pocket\n",
    "    - lbThreshold (float), ubThreshold (float): \n",
    "        lower and upper bounds of the interval of possible angles (out of the interval it will be considered outlier)\n",
    "    \n",
    "    Output:\n",
    "    - errorSubjectData (dict): combinations initials (key) - dataframe (value) for each subject with chosen pocket for error task.\n",
    "        Every entry of the dictionary stores three data frames:\n",
    "            1. VRData: behavioural data from the unity AAB\n",
    "            2. angleData: angle of the shot and corresponding trial numbers (250x2)\n",
    "            3. successData: binary variable for success and corresponding trial numbers (250x2)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Declare possible pocket and raise error otherwise\n",
    "    pockets = ['left', 'right']\n",
    "    if pocket.lower() in pockets:\n",
    "        pocket = pocket.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid AAB type. Expected one of: %s\" % pockets)\n",
    "\n",
    "\n",
    "    ### Definition of ideal angle and funnel of pocketing ###\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "\n",
    "    ### Import angle data ###\n",
    "    angleData = pd.read_csv(path + initials + \"/AAB/\" + initials + \"_Angle.txt\", header=None, names = ['Angle'])\n",
    "    angleData['Block'] = np.repeat(range(1,11),25)\n",
    "    # Remove outliers setting value to nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) > ubThreshold, 'Angle'] = np.nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) < lbThreshold, 'Angle'] = np.nan   \n",
    "\n",
    "\n",
    "    ### Import success data ###\n",
    "    successData = pd.read_csv(path + initials + \"/AAB/\" + initials + \"_Success.txt\",  sep = '\\t', header = None, \\\n",
    "        names = ['Block','Trial','Angle','Magnitude','RBPosition'], usecols = [0,1,2,3,5])\n",
    "\n",
    "\n",
    "\n",
    "    ### Import Behavioural Data from Blocks ###\n",
    "    VRData = pd.DataFrame()\n",
    "\n",
    "    # Merge all blocks together\n",
    "    for bl in range(1,11):\n",
    "        block = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Error\" +  pocket + \"_Block\" + str(bl) + \".txt\", sep = '\\t')\n",
    "        # remove shots after 25 trials (done by mistake)\n",
    "        while block['TrialNumber'].iloc[-1]!=25:\n",
    "            block.drop(block.tail(1).index,inplace=True)\n",
    "\n",
    "        block['TrialNumber'] = block['TrialNumber'] + (bl-1)*25  \n",
    "        VRData = pd.concat([VRData, block])\n",
    "\n",
    "\n",
    "    ### Store data in a dictionary ###\n",
    "    errorSubjectData = {}\n",
    "    names = ['VRData','Angle','Success']\n",
    "    dfs = [VRData, angleData, successData]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        errorSubjectData[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###\n",
    "    return(errorSubjectData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reward Task\n",
    "def importDataReward (initials, path, pocket, lbThreshold = -9.364594, ubThreshold = 4.675092):\n",
    "\n",
    "    '''Imports Behavioural Data for a subject performing Reward task\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right corresponding to the pocket\n",
    "    - lbThreshold (float), ubThreshold (float): \n",
    "        lower and upper bounds of the interval of possible angles (out of the interval it will be considered outlier)\n",
    "    \n",
    "    Output:\n",
    "    - rewardSubjectData (dict): combinations initials (key) - dataframe (value) for each subject with chosen pocket for reward task\n",
    "        Every entry of the dictionary stores four data frames:\n",
    "            1. VRData: behavioural data from the unity game\n",
    "            2. angleData: angle of the shot and corresponding trial numbers (250x2)\n",
    "            3. successData: binary variable for success and corresponding trial numbers (250x2)\n",
    "            4. rewardMotivation: reward motivation (Funnel or Median) and corresponding trial numbers (250x2)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Declare possible pocket and raise error otherwise\n",
    "    pockets = ['left', 'right']\n",
    "    if pocket.lower() in pockets:\n",
    "        pocket = pocket.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % pockets)\n",
    "\n",
    "    ### Definition of ideal angle and funnel ###\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "    ### Import angle data ###\n",
    "    angleData = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Angle.txt\", header=None, names = ['Angle'])\n",
    "    angleData['Block'] = np.repeat(range(1,11),25)\n",
    "    # Remove outliers setting values to nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) > ubThreshold, 'Angle'] = np.nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) < lbThreshold, 'Angle'] = np.nan   \n",
    "\n",
    "    ### Import success data ###\n",
    "    successData = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Success.txt\",  sep = '\\t', header = None, names = ['Block','Trial','Angle','Magnitude','RBPosition'], usecols = [0,1,2,3,5])\n",
    "\n",
    "    ### Import Reward Motivation (funnel or improvement) ###\n",
    "    rewardMotivation = pd.read_csv(path + initials + \"/Game/\" + initials + \"_RewardMotivation.txt\",  sep = '\\t', header = None, names = ['Block', 'Trial','Motivation'])\n",
    "    rewardMotivation['Trial'] = (rewardMotivation['Block']-1)*25 + rewardMotivation['Trial']\n",
    "\n",
    "\n",
    "    ### Import Behavioural Data from Blocks ###\n",
    "    VRData = pd.DataFrame()\n",
    "\n",
    "    for bl in range(1,11):\n",
    "        block = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Reward\" + pocket + \"_Block\" + str(bl) + \".txt\", sep = '\\t')\n",
    "        while block['TrialNumber'].iloc[-1]!=25:\n",
    "            block.drop(block.tail(1).index,inplace=True)\n",
    "        if block['TrialNumber'].iloc[-1]==26:\n",
    "            print(\"block: \", bl,block.tail(1).index)\n",
    "        block['TrialNumber'] = block['TrialNumber'] + (bl-1)*25  \n",
    "        VRData = pd.concat([VRData, block])\n",
    "        #print(\"Block\" + str(bl))\n",
    "\n",
    "    \n",
    "    ### Store data in a dictionary ###\n",
    "    rewardSubjectData = {}\n",
    "    names = ['VRData','Angle','Success','Motivation']\n",
    "    dfs = [VRData, angleData, successData, rewardMotivation]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        rewardSubjectData[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###    \n",
    "    return(rewardSubjectData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "    '''Preprocess data from Pool VR returning position of cue ball, red ball, stick and gaze\n",
    "    \n",
    "    Args:\n",
    "    - dataset (pd.Dataframe): VRData (1st dictionary entry of the output of importDataError or importDataReward)\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): four dataframes for one single subject\n",
    "        1. cbpos: position of the cue ball (X, Y, Z), trial number, time (in the game) and real time (N x 6)\n",
    "        2. rbpos: position of the red ball, trial number, time (in the game) and real time (N x 6)\n",
    "        3. stick: position of the stick, trial number, time (in the game) and real time (N x 6)\n",
    "        4. gaze: position of the gaze, trial number, time (in the game) and real time (N x 6)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    ### Create string variables from 3D vectors ###\n",
    "    \n",
    "    dataset['cueballpos_str'] = dataset['cueballpos'].str.split(',')\n",
    "    dataset['cueballvel_str'] = dataset['cueballvel'].str.split(',')\n",
    "    dataset['redballpos_str'] = dataset['redballpos'].str.split(',')\n",
    "    dataset['redballvel_str'] = dataset['redballvel'].str.split(',')\n",
    "    dataset['cueposfront_str'] = dataset['cueposfront'].str.split(',')\n",
    "    dataset['cueposback_str'] = dataset['cueposback'].str.split(',')\n",
    "    dataset['cuevel_str'] = dataset['cuevel'].str.split(',')\n",
    "    dataset['cuedirection_str'] = dataset['cuedirection'].str.split(',')\n",
    "    dataset['corner1pos_str'] = dataset['corner1pos'].str.split(',')\n",
    "    dataset['corner5pos_str'] = dataset['corner5pos'].str.split(',')\n",
    "    dataset['corner6pos_str'] = dataset['corner6pos'].str.split(',')\n",
    "    \n",
    "    dataset['stick'] = dataset['optifront'].str.split(',')\n",
    "    dataset['gaze_str'] = dataset['gaze'].str.split(',')\n",
    "\n",
    "    ### Create datasets from string variables ###\n",
    "    cbpos = pd.DataFrame.from_records(np.array(dataset['cueballpos_str']), columns=['x','y','z']).astype(float)\n",
    "    cbvel = pd.DataFrame.from_records(np.array(dataset['cueballvel_str']), columns=['x','y','z']).astype(float)\n",
    "    rbpos = pd.DataFrame.from_records(np.array(dataset['redballpos_str']), columns=['x','y','z']).astype(float)\n",
    "    rbvel = pd.DataFrame.from_records(np.array(dataset['redballvel_str']), columns=['x','y','z']).astype(float)\n",
    "    cueposfront = pd.DataFrame.from_records(np.array(dataset['cueposfront_str']), columns=['x','y','z']).astype(float)\n",
    "    cueposback = pd.DataFrame.from_records(np.array(dataset['cueposback_str']), columns=['x','y','z']).astype(float)\n",
    "    cuedirection = pd.DataFrame.from_records(np.array(dataset['cuedirection_str']), columns=['x','y','z']).astype(float)\n",
    "    cuevel = pd.DataFrame.from_records(np.array(dataset['cuevel_str']), columns=['x','y','z']).astype(float)\n",
    "    corner1pos = pd.DataFrame.from_records(np.array(dataset['corner1pos_str']), columns=['x','y','z']).astype(float)\n",
    "    corner5pos = pd.DataFrame.from_records(np.array(dataset['corner5pos_str']), columns=['x','y','z']).astype(float)\n",
    "    corner6pos = pd.DataFrame.from_records(np.array(dataset['corner6pos_str']), columns=['x','y','z']).astype(float)\n",
    "\n",
    "    stick = pd.DataFrame.from_records(np.array(dataset['stick']), columns=['x','y','z']).astype(float)\n",
    "    gaze = pd.DataFrame.from_records(np.array(dataset['gaze_str']), columns=['x','y','z']).astype(float)\n",
    "\n",
    "\n",
    "    ### Standardise w.r.t cue ball initial position and add time and trial number ###\n",
    "    #x_std, y_std, z_std = cbpos.iloc[0]\n",
    "    x_ref, y_ref, z_ref = corner1pos.iloc[100]  #to be sure position has been calibrated\n",
    "    x_scale, y_scale, z_scale = np.abs(corner1pos.iloc[100]-corner6pos.iloc[100])\n",
    "    print(\"scaling params: \", (x_ref, y_ref, z_ref), (x_scale, y_scale, z_scale))\n",
    "    for df in (cbpos, rbpos, cueposfront, cueposback, corner5pos, corner6pos, stick, gaze):    # cbvel, rbvel, cuedirection, cuevel, \n",
    "        df -= (x_ref, y_ref, z_ref)\n",
    "        df /= (x_scale, y_scale, z_scale)\n",
    "        df['trial'] = np.array(dataset['TrialNumber'])\n",
    "        #df['time'] = np.array(dataset['TrialTime']) \n",
    "        #df['timeReal'] = np.array(dataset['GlobalTime'])  \n",
    "    \n",
    "    #We don't want to standardize the velocities with respect to cue ball initial position\n",
    "    for df in (cbvel, rbvel, cuedirection, cuevel):\n",
    "        df['trial'] = np.array(dataset['TrialNumber'])\n",
    "\n",
    "    ### Create a dictionary for saving dataframes and save them ###\n",
    "    subject = {}\n",
    "    names = ['cbpos', 'cbvel', 'rbpos', 'rbvel', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel', 'corner5pos', 'corner6pos', 'stick','gaze']\n",
    "    dfs = [cbpos, cbvel, rbpos, rbvel, cueposfront, cueposback, cuedirection, cuevel, corner5pos, corner6pos, stick, gaze]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        subject[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###\n",
    "    return(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successTrialsError(subject):\n",
    "\n",
    "    '''Preprocesses and derive successful trials from VR Data correcting the output for error subjects\n",
    "    \n",
    "    Args:\n",
    "    - subject (dict): output of importDataError or importDataReward\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): copy of the input with correct success definition\n",
    "    \n",
    "    '''\n",
    "\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "    \n",
    "    ### Derive Successful trials from Angle dataset\n",
    "    subject['Angle']['Success'] = subject['Angle'].Angle.isin(subject['Success']['Angle']).astype(int)\n",
    "    subject['Angle']['Trial'] = range(1,251)\n",
    "    subject['Angle']['AngleStd'] =  subject['Angle']['Angle'] - idealAngle\n",
    "\n",
    "    gameData = preprocess(subject['VRData'])\n",
    "\n",
    "    succTrials = subject['Angle'].Success.index[subject['Angle'].Success == 1] + 1\n",
    "\n",
    "    for tr in succTrials:\n",
    "        cbTrial = gameData['cbpos'][gameData['cbpos'].trial == tr]\n",
    "\n",
    "        if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n",
    "            subject['Angle'].Success.iloc[tr-1] = 0\n",
    "    print(str(subject['Success'].shape[0]-sum(subject['Angle'].Success)) + \" fake successes removed\")\n",
    "\n",
    "\n",
    "    # Correction if missing successes in perturbation phase for error\n",
    "    df = subject['Angle'][subject['Angle']['Block'] > 3]\n",
    "    df = df[df['Block'] < 10]\n",
    "    if df['Success'].sum()==0:\n",
    "      for tr in range(76, 226):\n",
    "          rbTrial = gameData['rbpos'][gameData['rbpos'].trial == tr]\n",
    "          if (np.abs(np.median(rbTrial['y'].tail(10)) - np.median(rbTrial['y'].head(rbTrial.shape[0]-10))) > 0.02) and (np.abs(np.median(rbTrial['y'].tail(10)) - np.median(rbTrial['y'].head(rbTrial.shape[0]-10))) < 0.5):\n",
    "              subject['Angle'].Success.iloc[tr-1] = 1\n",
    "    \n",
    "    ### Returns preprocessed data with correct success\n",
    "    return(subject)\n",
    "\n",
    "\n",
    "def successTrialsReward(subjData, preprocessData):\n",
    "\n",
    "  '''Preprocesses and derive successful trials from VR Data correcting the output for reward subjects, \\\n",
    "    creating three new variables in the Angle dataset: SuccessMedian (binary variable for success attributable to the median), \\\n",
    "    SuccessFunnel (binary variable for success attributable to the funnel) and Target (reference angle to be rewarded)\n",
    "  \n",
    "    Args:\n",
    "    - subject (dict): output of importDataError or importDataReward\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): copy of the input with correct success definition and three new variables in the Angle dataset\n",
    "   \n",
    "  '''\n",
    "  \n",
    "  print(\"Success Derivation function\")\n",
    "  subject = subjData\n",
    "  ### Set threshold for funnel ###\n",
    "  idealAngle = 105.0736\n",
    "  lb_idealAngle = 104.5401 - idealAngle\n",
    "  ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "  ### Derive Successful trials from Angle dataset ###\n",
    "  ind = subject['Angle']['Block'].isin([1,2,3,10])\n",
    "\n",
    "  ### Create new variables in Angle dataset: success overall, success due to funnel and success due to improvement ###\n",
    "  subject['Angle']['Success'] = 0\n",
    "  subject['Angle']['SuccessFunnel'] = 0\n",
    "  subject['Angle']['SuccessMedian'] = 0\n",
    "\n",
    "  ### Add success to angle dataset ###\n",
    "  subject['Angle']['Success'][ind] = subject['Angle'].Angle[ind].isin(subject['Success']['Angle']).astype(int)\n",
    "\n",
    "  ### Add trial and standardised angle ###\n",
    "  subject['Angle']['Trial'] = range(1,251)\n",
    "  subject['Angle']['AngleStd'] =  subject['Angle']['Angle'] - idealAngle\n",
    "\n",
    "\n",
    "\n",
    "  ### Preprocess game data ###\n",
    "  #gameData = preprocess(subject['VRData'])\n",
    "\n",
    "  ### True Success for baseline blocks ###\n",
    "  succTrials = subject['Angle'].Success.index[subject['Angle'].Success == 1] + 1\n",
    "  \n",
    "  for tr in succTrials:\n",
    "      cbTrial = preprocessData['cbpos'][preprocessData['cbpos'].trial == tr]\n",
    "\n",
    "      if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n",
    "          subject['Angle'].Success.iloc[tr-1] = 0\n",
    "\n",
    "  print(str(succTrials.shape[0]-sum(subject['Angle'].Success)) + \" fake successes removed\")\n",
    "\n",
    "  ### Success for perturbation blocks ###\n",
    "  for tr in subject['Motivation'].Trial:\n",
    "    #subject['Angle']['Success'].iloc[tr] = 1\n",
    "\n",
    "    if (subject['Motivation'].Motivation[subject['Motivation'].Trial == tr] == 'Median').all():\n",
    "      subject['Angle']['SuccessMedian'][tr] = 1\n",
    "    else:\n",
    "      subject['Angle']['SuccessFunnel'][tr] = 1 \n",
    "\n",
    "  ### Derive target for reward (median of the past 10 successful trials) ###\n",
    "\n",
    "  target = []\n",
    "  vec_median = list(subject['Angle']['AngleStd'].iloc[range(66,76)])\n",
    "\n",
    "  for tr in range(76, 226):\n",
    "    if subject['Angle']['Success'].iloc[tr]==1:\n",
    "        target.append(min(ub_idealAngle, max(np.median(vec_median), ub_idealAngle-5)))\n",
    "        vec_median.remove(max(vec_median))\n",
    "        vec_median.append(subject['Angle']['AngleStd'].iloc[tr])\n",
    "    else:\n",
    "        target.append(min(ub_idealAngle, max(np.median(vec_median), ub_idealAngle-5)))\n",
    "\n",
    "  target_overall = pd.Series(np.concatenate([np.repeat(np.nan, 75), target, np.repeat(np.nan, 25)]))\n",
    "\n",
    "  subject['Angle']['Target'] = target_overall\n",
    "  ### Returns preprocessed data with correct success ###\n",
    "  return(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Single Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsSingleSubject(initials, gameType, path, pocket):\n",
    "    '''\n",
    "    Produces results for single subject starting from raw data\n",
    "\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - gameType (str): error/reward - game mode\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right - corresponding to the pocket\n",
    "    \n",
    "    Output:\n",
    "    - outputDict (dict): dictionary with two entries:\n",
    "        1. Game (dict): game data (same output as importDataError/importDataReward) with success corrected\n",
    "            - VRData: data from the Unity Game\n",
    "            - Angle: shot directional angles for all trials\n",
    "            - Success: features of all successful trials\n",
    "            - Motivation (only for reward task): reason of successful trial (if funnel or improvement of the median)\n",
    "\n",
    "        2. PreProcessed (dict): pre-processed game data\n",
    "            - cbpos: position of the cue ball for all timeframes and all trials\n",
    "            - rbpos: position of the red ball for all timeframes and all trials\n",
    "            - stick: position of the stick for all timeframes and all trials\n",
    "            - gaze: position of the gaze\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print(\"Result Single Subject function\")\n",
    "    # Declare possible gameType and raise error otherwise\n",
    "    game_types = ['error', 'reward']\n",
    "    if gameType.lower() in game_types:\n",
    "        mode = gameType.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % game_types)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Error Mode #####\n",
    "    if mode == 'error':\n",
    "\n",
    "        # Import Data\n",
    "        gameData = importDataError(initials, path, pocket) \n",
    "        ''' \n",
    "        gameData is a dictionary with keys:\n",
    "        - VRData: data from the Unity Game\n",
    "        - Angle: shot directional angles for all trials\n",
    "        - Success: features of all successful trials\n",
    "        '''\n",
    "\n",
    "        # Preprocess Data\n",
    "        preprocData = preprocess(gameData['VRData'])\n",
    "        '''\n",
    "        preprocData is a dictionary with keys:\n",
    "        - cbpos: position of the cue ball for all timeframes and all trials\n",
    "        - rbpos: position of the red ball for all timeframes and all trials\n",
    "        - stick: position of the stick for all timeframes and all trials\n",
    "        - gaze: position of the gaze\n",
    "        '''\n",
    "\n",
    "        # Update of Success in Import Data\n",
    "        gameData = successTrialsError(gameData)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Reward Mode #####\n",
    "\n",
    "    elif mode == 'reward':\n",
    "\n",
    "        # Import Data\n",
    "        gameData = importDataReward(initials, path, pocket) \n",
    "        ''' \n",
    "        gameData is a dictionary with keys:\n",
    "        - VRData: data from the Unity Game\n",
    "        - Angle: shot directional angles for all trials\n",
    "        - Success: features of all successful trials\n",
    "        - Motivation: reason of successful trial (if funnel or improvement of the median)\n",
    "        '''\n",
    "\n",
    "        # Preprocess Data\n",
    "        preprocData = preprocess(gameData['VRData'])\n",
    "        '''\n",
    "        preprocData is a dictionary with keys:\n",
    "        - cbpos: position of the cue ball for all timeframes and all trials\n",
    "        - rbpos: position of the red ball for all timeframes and all trials\n",
    "        - stick: position of the stick for all timeframes and all trials\n",
    "        - gaze: position of the gaze\n",
    "        '''\n",
    "\n",
    "        # Update of Success in Import Data\n",
    "        angleData = successTrialsReward(gameData, preprocData)\n",
    "\n",
    "\n",
    "    ### List of outputs ###\n",
    "    outputDict = {}\n",
    "    outputDict['Game'] = gameData \n",
    "    outputDict['PreProcessed'] = preprocData\n",
    "    outputDict['Angle'] = angleData\n",
    "    return(outputDict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Multiple Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsMultipleSubjects(path, sub, gameType, pocketSide):\n",
    "    '''\n",
    "    Args:\n",
    "    - pathList (list): list of all paths of the folder with raw data\n",
    "    - gameType (str): error/reward - game type \n",
    "    - pocketSide (str): left/right - game pocket\n",
    " \n",
    "    '''\n",
    "\t\n",
    "    print(\"result Multiple Subject function\")\n",
    "    # Declare possible gameType and raise error otherwise\n",
    "    game_types = ['error', 'reward']\n",
    "    if gameType.lower() in game_types:\n",
    "        mode = gameType.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % game_types)\n",
    "\n",
    "    # Call dataframe for both rounds together\n",
    "    dataset = {}\n",
    "    \n",
    "    # Declare possible pocketSide and raise error otherwise\n",
    "    pocketChoice = ['left', 'right', 'all']\n",
    "    if pocketSide.lower() in pocketChoice:\n",
    "        pocket = pocketSide.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid pocket. Expected one of: %s\" % pocketChoice)\n",
    "\n",
    "\n",
    "    print(path, sub)\n",
    "    if mode == 'reward':\n",
    "\n",
    "        # Derive path for a specific subject and game type\n",
    "        pathSubj = path + str(sub)\n",
    "        for fil in range(len(sorted(os.listdir(pathSubj + '/Game/')))):\n",
    "            if sorted(os.listdir(pathSubj + '/Game/'))[fil].find(\"Block2\") > -1:\n",
    "                blockFile = sorted(os.listdir(pathSubj + '/Game/'))[fil]\n",
    "\n",
    "\n",
    "        if (blockFile.find('Left') > -1 and pocket != 'right'):\n",
    "            pocketSub = 'Left'\n",
    "        elif (blockFile.find('Right') > -1 and pocket != 'left'):\n",
    "            pocketSub = 'Right'\n",
    "        else:\n",
    "            pocketSub = 'None'\n",
    "\n",
    "        if blockFile.find('Reward') > -1:\n",
    "            # Derive initials of the subject\n",
    "            initials = os.path.basename(pathSubj)\n",
    "\n",
    "            # Derive All Results for One Subject\n",
    "            subjData = resultsSingleSubject(initials, 'reward', path, pocketSub)\n",
    "\n",
    "            # Add data to dataframes previously defined\n",
    "            dataset[str(initials)]={}\n",
    "\n",
    "            dataset[str(initials)][\"rewards\"] = def_reward(subjData[\"Angle\"][\"Angle\"])  #[\"Success\"], subjData[\"Angle\"][\"Angle\"][\"SuccessMedian\"], subjData[\"Angle\"][\"Angle\"][\"SuccessFunnel\"])\n",
    "\n",
    "\n",
    "            dataset[str(initials)][\"cueballpos\"] = subjData[\"PreProcessed\"][\"cbpos\"]\n",
    "            dataset[str(initials)][\"cueballvel\"] = subjData[\"PreProcessed\"][\"cbvel\"]\n",
    "            \n",
    "            dataset[str(initials)][\"start_ind\"], dataset[str(initials)][\"hit_ind\"] = start_hit_reduced_timesteps(subjData[\"PreProcessed\"])  # start_hit_timesteps(subjData[\"PreProcessed\"])     \n",
    "            dataset[str(initials)][\"start_ind\"] = dataset[str(initials)][\"start_ind\"].astype(int)\n",
    "            dataset[str(initials)][\"hit_ind\"] = dataset[str(initials)][\"hit_ind\"].astype(int)\n",
    "            dataset[str(initials)][\"redballpos\"] = subjData[\"PreProcessed\"][\"rbpos\"]\n",
    "            \n",
    "            if pocketSub == 'Left':\n",
    "                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner5pos\"]\n",
    "            else:\n",
    "                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner6pos\"]\n",
    "\n",
    "            dataset[str(initials)][\"cueposfront\"] = subjData[\"PreProcessed\"][\"cueposfront\"]\n",
    "\n",
    "            dataset[str(initials)][\"cueposback\"] = subjData[\"PreProcessed\"][\"cueposback\"]\n",
    "\n",
    "            dataset[str(initials)][\"cuedirection\"] = subjData[\"PreProcessed\"][\"cuedirection\"]\n",
    "\n",
    "            dataset[str(initials)][\"cuevel\"] = subjData[\"PreProcessed\"][\"cuevel\"]\n",
    "\n",
    "            #dataset[str(initials)][\"rewards\"] = def_reward(subjData['Game'][\"Angle\"][\"Success\"], subjData[\"Game\"][\"Angle\"][\"SuccessFunnel\"], subjData[\"Game\"][\"Angle\"][\"SuccessMedian\"])\n",
    "\n",
    "            print(\"Imported \" + initials + \" as reward subject \" + \"for \" + pocketSub + \" pocket\" + \" in round: \" + path)\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Reduced timesteps Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_Reduced(data, cue_data, cuevel, rewards, state_dim=14, action_dim=2):   #state_dim should be a multiple of 6\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = len(data[\"cueballpos\"][\"trial\"])-1   #len(data[initial][\"start_ind\"])\n",
    "    print(N)\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    new_state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    #action = np.zeros(action_dim)\n",
    "    #reward = np.zeros(1)\n",
    "    #ball_states=6\n",
    "    #cue_states=6\n",
    "    #cue_states_iterations = int((state_dim-ball_states)/(cue_states))\n",
    "\n",
    "    #Action Velocity, Force?\n",
    "    #See car 2D -> action space acceleration and cueDirection\n",
    "    actions = compute_actions(cuevel[[\"x\", \"z\"]], cue_data[\"cuedirection\"][[\"x\", \"z\"]])\n",
    "    #actions =  np.stack((cue_data[\"cueposfront\"][\"x\"], cue_data[\"cueposfront\"][\"z\"]))#, cue_data[\"cueposback\"][\"x\"], cue_data[\"cueposback\"][\"z\"])) \n",
    "    #actions = compute_actions(cue_data, cuevel)\n",
    "    for i in range(N):\n",
    "        count=0\n",
    "        for x in data:\n",
    "            state[count] = data[str(x)][\"x\"].iloc[i]\n",
    "            state[count+1] = data[str(x)][\"z\"].iloc[i]\n",
    "            new_state[count] = data[str(x)][\"x\"].iloc[i+1]\n",
    "            new_state[count+1] = data[str(x)][\"z\"].iloc[i+1]\n",
    "            count+=2\n",
    "\n",
    "        #Add last j timesteps of the cuepos to states\n",
    "        #3 observations on x and z axis makes 6 observations\n",
    "        #for j in range(cue_states_iterations): #Warning if state_dim not multiple of 6, division fail for loop\n",
    "        for x in cue_data:\n",
    "            state[count] = cue_data[str(x)][\"x\"].iloc[i]#-j]\n",
    "            state[count+1] = cue_data[str(x)][\"z\"].iloc[i]#-j]\n",
    "            new_state[count] = cue_data[str(x)][\"x\"].iloc[i+1]#-j]\n",
    "            new_state[count+1] = cue_data[str(x)][\"z\"].iloc[i+1]#-j]\n",
    "            count+=2\n",
    "   \n",
    "        #Action Velocity, Force?\n",
    "        action = actions[i][:]#actions[:,i+1] #action is cuepos at t+1\n",
    "        reward = rewards.iloc[i+1]\n",
    "        \n",
    "        if data['cueballpos']['trial'].iloc[i+1] != data['cueballpos']['trial'].iloc[i] or i == N-1:\n",
    "            done_bool = True\n",
    "            #reward = rewards.iloc[i]\n",
    "        else:\n",
    "            done_bool = False\n",
    "            #reward=0\n",
    "\n",
    "        trial_.append(data[\"cueballpos\"][\"trial\"].iloc[i])\n",
    "        state_.append(state.copy())\n",
    "        new_state_.append(new_state.copy())\n",
    "        action_.append(action.copy())\n",
    "        reward_.append(reward)\n",
    "        done_.append(done_bool)\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline dict One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_One(data, cue_data, cuevel, rewards, state_dim=24, action_dim=2):   #state_dim should be a multiple of 6\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = len(data[\"cueballpos\"][\"trial\"]) -1   #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    new_state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    action = np.zeros(action_dim)\n",
    "    #reward = np.zeros(1)\n",
    "    #gamma = 0.6\n",
    "    ball_states=6\n",
    "    cue_states=6\n",
    "    cue_states_iterations = int((state_dim-ball_states)/(cue_states))\n",
    "    #Action Velocity, Force?\n",
    "    #See car 2D -> action space acceleration and cueDirection\n",
    "    actions = compute_impulseForce(cuevel[[\"x\", \"z\"]], cue_data[\"cuedirection\"][[\"x\", \"z\"]])\n",
    "        \n",
    "    for i in range(N):\n",
    "        count=0\n",
    "        for x in data:\n",
    "            state[count] = data[str(x)][\"x\"].iloc[i]\n",
    "            state[count+1] = data[str(x)][\"z\"].iloc[i]\n",
    "            new_state[count] = data[str(x)][\"x\"].iloc[i+1]\n",
    "            new_state[count+1] = data[str(x)][\"z\"].iloc[i+1]\n",
    "            count+=2\n",
    "\n",
    "        #Add last j timesteps of the cuepos to states\n",
    "        #3 observations on x and z axis makes 6 observations\n",
    "        for j in range(cue_states_iterations): #Warning if state_dim not multiple of 6, division fail for loop\n",
    "            for x in cue_data:\n",
    "                state[count] = cue_data[str(x)][\"x\"].iloc[i-j]\n",
    "                state[count+1] = cue_data[str(x)][\"z\"].iloc[i-j]\n",
    "                new_state[count] = cue_data[str(x)][\"x\"].iloc[i+1-j]\n",
    "                new_state[count+1] = cue_data[str(x)][\"z\"].iloc[i+1-j]\n",
    "                count+=2\n",
    "   \n",
    "        #Action Velocity, Force?\n",
    "        action = actions[i][:]\n",
    "        #reward = rewards[i]\n",
    "        \n",
    "        if data['cueballpos']['trial'].iloc[i+1] != data['cueballpos']['trial'].iloc[i]:\n",
    "            done_bool = True\n",
    "            reward = rewards.iloc[i]\n",
    "        else:\n",
    "            done_bool = False\n",
    "            reward=0\n",
    "\n",
    "        trial_.append(data[\"cueballpos\"][\"trial\"].iloc[i])\n",
    "        state_.append(state.copy())\n",
    "        new_state_.append(new_state.copy())\n",
    "        action_.append(action.copy())\n",
    "        reward_.append(reward)\n",
    "        done_.append(done_bool)\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }\n",
    "\n",
    "def compute_impulseForce_load(self, cuevel, cuedirection):\n",
    "    impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "    shotMagnitude = np.zeros(1)\n",
    "    shotDir = np.zeros(cuedirection.shape)\n",
    "    #Reward: magnitude range\n",
    "    lbMagnitude = 0.4   #0.516149\n",
    "    ubMagnitude = 0.882607\n",
    "\n",
    "    shotMagnitude = np.linalg.norm(cuevel, axis=1)\n",
    "\n",
    "    for i in range(cuevel.shape[0]):\n",
    "        if shotMagnitude[i] > ubMagnitude:\n",
    "            shotMagnitude[i] = ubMagnitude\n",
    "            #print(\"upper bounded\")\n",
    "        #elif shotMagnitude[i] > lbMagnitude:\n",
    "            #print(i, shotMagnitude[i])\n",
    "        elif shotMagnitude[i] < lbMagnitude:\n",
    "            shotMagnitude[i] = 0\n",
    "\n",
    "        shotDir[i][:] = cuedirection[i][:]\n",
    "        if shotMagnitude[i] == 0:\n",
    "            impulseForce[i][:] = 0\n",
    "        else:\n",
    "            impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "    return impulseForce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL One dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_One_big_dict(data,initial, terminate_on_end=False):\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = data[initial][\"cueballpos\"][\"trial\"].iloc[-1]    #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(14)  #np.zeros(21)\n",
    "    new_state = np.zeros(14)  #np.zeros(21)\n",
    "    action = np.zeros(2)\n",
    "    reward = np.zeros(1)\n",
    "    #gamma = 0.6\n",
    "    episode_step=0\n",
    "    for i in range(N):\n",
    "        for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]+1, 1):\n",
    "            #state timestep t\n",
    "            state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j] \n",
    "            #state[0] = data[initial][\"cueballpos\"][y].iloc[j] \n",
    "            state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j] \n",
    "            state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j] \n",
    "            #state[2] = data[initial][\"redballpos\"][y].iloc[j]\n",
    "            state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j]\n",
    "            #state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j]\n",
    "            state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j]\n",
    "            state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j]\n",
    "            #state[6] = data[initial][\"cueposfront\"][y].iloc[j]\n",
    "            state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j]\n",
    "            state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j]\n",
    "            #state[8] = data[initial][\"cueposback\"][y].iloc[j]\n",
    "            state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j]\n",
    "            state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j]\n",
    "            #state[10] = data[initial][\"cueDirection\"][y].iloc[j]\n",
    "            state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j]\n",
    "            state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j]\n",
    "            state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j]\n",
    "\n",
    "            #state Timestep t+1\n",
    "            new_state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[0] = data[initial][\"cueballpos\"][y].iloc[j+1] \n",
    "            new_state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j+1] \n",
    "            new_state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[2] = data[initial][\"redballpos\"][y].iloc[j+1]\n",
    "            new_state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j+1]\n",
    "            #new_state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j+1]\n",
    "            new_state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j+1]\n",
    "            #new_state[6] = data[initial][\"cueposfront\"][y].iloc[j+1]\n",
    "            new_state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j+1]\n",
    "            new_state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j+1]\n",
    "            #new_state[8] = data[initial][\"cueposback\"][y].iloc[j+1]\n",
    "            new_state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j+1]\n",
    "            new_state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j+1]\n",
    "            #new_state[10] = data[initial][\"cueDirection\"][y].iloc[j+1]\n",
    "            new_state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j+1]\n",
    "            new_state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j+1]\n",
    "            new_state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j+1]\n",
    "\n",
    "            #Action Velocity, Force?\n",
    "            action = compute_impulseForce(data[initial][\"cuevel\"][[\"x\", \"z\"]].iloc[j], data[initial][\"cuedirection\"][[\"x\", \"z\"]].iloc[j])\n",
    "            #action[0] = data[initial][\"cuevel\"][\"y\"].iloc[j]\n",
    "            \n",
    "\n",
    "            if j == data[initial][\"hit_ind\"][i]:\n",
    "                done_bool = True\n",
    "                reward = data[initial][\"rewards\"][i]\n",
    "            else:\n",
    "                done_bool = False\n",
    "                ## Discounted reward ##\n",
    "                #reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "                reward = 0\n",
    "\n",
    "            final_timestep = (episode_step == data[initial][\"hit_ind\"][i]-1)\n",
    "            if (not terminate_on_end) and final_timestep:\n",
    "                # Skip this transition and don't apply terminals on the last step of an episode\n",
    "                episode_step = 0\n",
    "                continue\n",
    "            if done_bool or final_timestep:\n",
    "                episode_step = 0\n",
    "\n",
    "            trial_.append(data[initial][\"cueballpos\"][\"trial\"].iloc[j] )\n",
    "            state_.append(state)\n",
    "            new_state_.append(new_state)\n",
    "            action_.append(action)\n",
    "            reward_.append(reward)\n",
    "            done_.append(done_bool)\n",
    "            episode_step += 1\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline load class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Offline_RL_dataset(object): #rewards,cueballpos,redballpos, targetcornerpos, cueposfront, cueposback, cuedirection, cuevel,\n",
    "    def __init__(self, state_dim=14, action_dim=2, nb_trial=250):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #number of trial\n",
    "        self.N = nb_trial    #int(cueballpos[0].iloc[-1])\n",
    "        self.state_dim = state_dim  \n",
    "        self.action_dim = action_dim\n",
    "        self.trajectories = []\n",
    "        \n",
    "        self.mean = np.zeros(self.state_dim)\n",
    "        self.std = np.zeros(self.state_dim)\n",
    "\n",
    "    def get_trajectories(self, list_data, rewards):\n",
    "        \n",
    "        state_ = []\n",
    "        new_state_ = []\n",
    "        action_ = []\n",
    "        reward_ = []\n",
    "        done_ = []\n",
    "\n",
    "        state = np.zeros(self.state_dim)\n",
    "        new_state = np.zeros(self.state_dim) \n",
    "        action = np.zeros(self.action_dim)\n",
    "        reward = np.zeros(1)\n",
    "\n",
    "        actions = self.compute_impulseForce_load(np.transpose(np.array([list_data[\"cuevel\"][1], list_data[\"cuevel\"][3]])), np.transpose(np.array([list_data[\"cuedirection\"][1], list_data[\"cuedirection\"][3]])))\n",
    "\n",
    "        for i in range(len(list_data[\"cueballpos\"])-1):\n",
    "            count=0\n",
    "            for x in list_data:\n",
    "                state[count] = list_data[str(x)][1].iloc[i]\n",
    "                state[count+1] = list_data[str(x)][3].iloc[i]\n",
    "                new_state[count] = list_data[str(x)][1].iloc[i+1]\n",
    "                new_state[count+1] = list_data[str(x)][3].iloc[i+1]\n",
    "                count+=2\n",
    "            #Action Velocity, Force?\n",
    "            action = actions[i][:]\n",
    "            reward = rewards.iloc[i]\n",
    "            \n",
    "            if cueballpos[0].iloc[i+1] != cueballpos[0].iloc[i]:\n",
    "                done_bool = True\n",
    "            else:\n",
    "                done_bool = False\n",
    "\n",
    "            state_.append(state.copy())\n",
    "            new_state_.append(new_state.copy())\n",
    "            action_.append(action.copy())\n",
    "            reward_.append(reward.copy())\n",
    "            done_.append(done_bool)\n",
    "\n",
    "            if list_data[\"cueballpos\"][0].iloc[i+1] != list_data[\"cueballpos\"][0].iloc[i]:\n",
    "                self.trajectories.append({\n",
    "                'size': np.array(state_).shape[0],\n",
    "                'states': np.array(state_),\n",
    "                'actions': np.array(action_),\n",
    "                'new_states': np.array(new_state_),\n",
    "                'rewards': np.array(reward_),\n",
    "                'terminals': np.array(done_),\n",
    "                })\n",
    "\n",
    "    def sample(self):   #, batch_size = 4):   #bacth size is the number of trajectory processed before gradient update\n",
    "        max_size = self.N\n",
    "        ind = 1 #np.random.randint(0, max_size)    #, size=batch_size)\n",
    "        return (torch.FloatTensor(self.trajectories[ind]['size']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['states']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['actions']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['new_states']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['rewards']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['terminals']).to(self.device))\n",
    "\n",
    "    def compute_impulseForce_load(self, cuevel, cuedirection):\n",
    "        impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "        shotMagnitude = np.zeros(1)\n",
    "        shotDir = np.zeros(cuedirection.shape)\n",
    "        #Reward: magnitude range\n",
    "        lbMagnitude = 0.4   #0.516149\n",
    "        ubMagnitude = 0.882607\n",
    "\n",
    "        shotMagnitude = np.linalg.norm(cuevel, axis=1)\n",
    "\n",
    "        for i in range(cuevel.shape[0]):\n",
    "            if shotMagnitude[i] > ubMagnitude:\n",
    "                shotMagnitude[i] = ubMagnitude\n",
    "                #print(\"upper bounded\")\n",
    "            #elif shotMagnitude[i] > lbMagnitude:\n",
    "                #print(i, shotMagnitude[i])\n",
    "            elif shotMagnitude[i] < lbMagnitude:\n",
    "                shotMagnitude[i] = 0\n",
    "\n",
    "            shotDir[i][:] = cuedirection[i][:]\n",
    "            if shotMagnitude[i] == 0:\n",
    "                impulseForce[i][:] = 0\n",
    "            else:\n",
    "                impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "        return impulseForce\n",
    "\n",
    "    def compute_mean_std(self, list_data, eps = 1e-3):\n",
    "        count = 0\n",
    "        for i,x in enumerate(list_data):\n",
    "            self.mean[count] = list_data[str(x)][1].mean(0)  #, keepdim=True)\n",
    "            self.mean[count+1] = list_data[str(x)][3].mean(0)\n",
    "\n",
    "            self.std[count] = list_data[str(x)][1].std(0)  #, keepdim=True)\n",
    "            self.std[count+1] = list_data[str(x)][3].std(0)\n",
    "            count += 2\n",
    "\n",
    "    def normalize_states(self, eps = 1e-3):\n",
    "        for i in range(2):  #self.N\n",
    "            for j in range(self.state_dim):\n",
    "                self.trajectories[i]['states'][:][j] = (self.trajectories[i]['states'][:][j] - self.mean[j])/self.std[j]\n",
    "                self.trajectories[i]['new_states'][:][j] = (self.trajectories[i]['new_states'][:][j] - self.mean[j])/self.std[j]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_RL_dataset(data,initial, terminate_on_end=False):\n",
    "    \"\"\"\n",
    "        Returns datasets formatted for use by standard Q-learning algorithms,\n",
    "        with states, actions, next_states, rewards, and a terminal\n",
    "        flag.\n",
    "\n",
    "        Args:\n",
    "            cueballPos, redballPos, targetcornerPos, cuePosfront, cuePosback: Recorded stateervation States (for each subject)\n",
    "            cueVel: Recorded Action new_state (for each Subject)\n",
    "            reward: np.array with one reward per trial\n",
    "            start_ind: starting index of each trial\n",
    "            hit_ind: hitting the cue ball index for each trial\n",
    "            target_corner: string indicating the corner in which to pocket\n",
    "            terminate_on_end (bool): Set done=True on the last timestep\n",
    "            in a trajectory. Default is False, and will discard the\n",
    "            last timestep in each trajectory.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing keys:\n",
    "                states: An N x dim_state array of states.\n",
    "                actions: An N x dim_action array of actions.\n",
    "                next_states: An N x dim_state array of next states.\n",
    "                rewards: An N-dim float array of rewards.\n",
    "                terminals: An N-dim boolean array of \"done\" or episode termination flags.\n",
    "        \"\"\"\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = data[\"AAB\"][\"cueballpos\"][\"trial\"].iloc[-1]    #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "\n",
    "    state = np.zeros(14)  #np.zeros(21)\n",
    "    new_state = np.zeros(14)  #np.zeros(21)\n",
    "    action = np.zeros(2)\n",
    "    reward = np.zeros(1)\n",
    "    gamma = 0.6 #discounted reward\n",
    "    # The newer version of the dataset adds an explicit\n",
    "    # timeouts field. Keep old method for backwards compatability.\n",
    "    #use_timeouts = False\n",
    "    #if 'timeouts' in dataset:\n",
    "        #use_timeouts = True\n",
    "    \n",
    "    '''terminate_on_end (bool): Set done=True on the last timestep\n",
    "        in a trajectory. Default is False, and will discard the\n",
    "        last timestep in each trajectory.'''\n",
    "\n",
    "    episode_step = 0\n",
    "    trajectories = []\n",
    "    for i in range(N):\n",
    "        \"\"\"\n",
    "        episode = {'true_state': [],\n",
    "                   'true_next_state': [],\n",
    "                   'x': [],\n",
    "                   'a': [],\n",
    "                   'r': [],\n",
    "                   'x_prime': [],\n",
    "                   'done': [],\n",
    "                   'base_propensity': [],\n",
    "                   'target_propensity': [],\n",
    "                   'frames': [],\n",
    "                   'extra_propensity': []}\n",
    "        \"\"\"\n",
    "        for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]+1, 1):\n",
    "            #state timestep t\n",
    "            state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j] \n",
    "            #state[0] = data[initial][\"cueballpos\"][y].iloc[j] \n",
    "            state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j] \n",
    "            state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j] \n",
    "            #state[2] = data[initial][\"redballpos\"][y].iloc[j]\n",
    "            state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j]\n",
    "            state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j]\n",
    "            state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j]\n",
    "            #state[6] = data[initial][\"cueposfront\"][y].iloc[j]\n",
    "            state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j]\n",
    "            state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j]\n",
    "            #state[8] = data[initial][\"cueposback\"][y].iloc[j]\n",
    "            state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j]\n",
    "            state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j]\n",
    "            #state[10] = data[initial][\"cueDirection\"][y].iloc[j]\n",
    "            state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j]\n",
    "            state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j]\n",
    "            state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j]\n",
    "\n",
    "            #state Timestep t+1\n",
    "            new_state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[0] = data[initial][\"cueballpos\"][y].iloc[j+1] \n",
    "            new_state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j+1] \n",
    "            new_state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[2] = data[initial][\"redballpos\"][y].iloc[j+1]\n",
    "            new_state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j+1]\n",
    "            new_state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j+1]\n",
    "            #new_state[6] = data[initial][\"cueposfront\"][y].iloc[j+1]\n",
    "            new_state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j+1]\n",
    "            new_state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j+1]\n",
    "            #new_state[8] = data[initial][\"cueposback\"][y].iloc[j+1]\n",
    "            new_state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j+1]\n",
    "            new_state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j+1]\n",
    "            #new_state[10] = data[initial][\"cueDirection\"][y].iloc[j+1]\n",
    "            new_state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j+1]\n",
    "            new_state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j+1]\n",
    "            new_state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j+1]\n",
    "\n",
    "            #Action Velocity, Force?\n",
    "            action = compute_impulseForce(data[initial][\"cuevel\"][[\"x\", \"z\"]].iloc[j], data[initial][\"cuedirection\"][[\"x\", \"z\"]].iloc[j])\n",
    "            #action[0] = data[initial][\"cuevel\"][\"y\"].iloc[j]\n",
    "            \n",
    "\n",
    "            if j == data[initial][\"hit_ind\"][i]:\n",
    "                done_bool = True\n",
    "                reward = data[initial][\"rewards\"][i]\n",
    "            else:\n",
    "                done_bool = False\n",
    "                ## Discounted reward ##\n",
    "                #gamma = 0.9\n",
    "                #reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "                reward = 0\n",
    "\n",
    "            final_timestep = (episode_step == data[initial][\"hit_ind\"][i]-1)\n",
    "            if (not terminate_on_end) and final_timestep:\n",
    "                # Skip this transition and don't apply terminals on the last step of an episode\n",
    "                episode_step = 0\n",
    "                continue\n",
    "            if done_bool or final_timestep:\n",
    "                episode_step = 0\n",
    "            print(\"appending episode: \", j)\n",
    "            state_.append(state)\n",
    "            new_state_.append(new_state)\n",
    "            action_.append(action)\n",
    "            reward_.append(reward)\n",
    "            done_.append(done_bool)\n",
    "            episode_step += 1\n",
    "        \n",
    "        print(\"trajectory: \", i)\n",
    "        trajectories.append({\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_state': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        })\n",
    "    \n",
    "    return trajectories\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'states': state_,\n",
    "        'actions': action_,\n",
    "        'next_states': next_state_,\n",
    "        'rewards': reward_,\n",
    "        'terminals': done_,\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'next_states': np.array(next_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef convert_D4RL(self, dataset):\n",
    "\t\tself.state = np.array(dataset['states'])\n",
    "\t\tself.action = np.array(dataset['actions'])\n",
    "\t\tself.next_state = np.array(dataset['next_states'])\n",
    "\t\tself.reward = np.array(dataset['rewards']).reshape(-1,1)\n",
    "\t\tself.not_done = 1. - np.array(dataset['terminals']).reshape(-1,1)\n",
    "\t\tself.size = self.state.shape[0]\n",
    "\n",
    "\n",
    "\tdef normalize_states(self, eps = 1e-3):\n",
    "\t\tmean = self.state.mean(0,keepdims=True)\n",
    "\t\tstd = self.state.std(0,keepdims=True) + eps\n",
    "\t\tself.state = (self.state - mean)/std\n",
    "\t\tself.next_state = (self.next_state - mean)/std\n",
    "\t\treturn mean, std\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\t\t\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n",
    "\n",
    "\n",
    "class TD3_BC(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tdiscount=0.99,\n",
    "\t\ttau=0.005,\n",
    "\t\tpolicy_noise=0.2,\n",
    "\t\tnoise_clip=0.5,\n",
    "\t\tpolicy_freq=2,\n",
    "\t\talpha=2.5,\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\t\tself.policy_noise = policy_noise\n",
    "\t\tself.noise_clip = noise_clip\n",
    "\t\tself.policy_freq = policy_freq\n",
    "\t\tself.alpha = alpha\n",
    "\n",
    "\t\tself.total_it = 0\n",
    "\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=256):\n",
    "\t\tself.total_it += 1\n",
    "\n",
    "\t\t# Sample replay buffer \n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Select action according to policy and add clipped noise\n",
    "\t\t\tnoise = (\n",
    "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
    "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
    "\t\t\t\n",
    "\t\t\tnext_action = (\n",
    "\t\t\t\tself.actor_target(next_state) + noise\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t\t# Compute the target Q value\n",
    "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t#if (self.total_it) % 5 == 0:\n",
    "\t\t\t#print(\"iteration \", self.total_it, \" critic_loss: \", critic_loss)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "\t\tif self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor loss\n",
    "\t\t\tpi = self.actor(state)\n",
    "\t\t\tQ = self.critic.Q1(state, pi)\n",
    "\t\t\tlmbda = self.alpha/Q.abs().mean().detach()\n",
    "\n",
    "\t\t\tactor_loss = -lmbda * Q.mean() + F.mse_loss(pi, action) \n",
    "\t\t\t\n",
    "\t\t\t#print(\"iteration \", self.total_it, \" actor_loss: \", actor_loss)\n",
    "\n",
    "\t\t\t# Optimize the actor \n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "\tdef save(self, filename):\n",
    "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\t\t\n",
    "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "\tdef load(self, filename):\n",
    "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        log_prob = (dist.log_prob(e) - torch.log(1 - action.pow(2) + epsilon)).sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
    "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
    "        \"\"\"\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        return action.detach().cpu()\n",
    "    \n",
    "    def get_det_action(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        return torch.tanh(mu).detach().cpu()\n",
    "\n",
    "class DeepActor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, device, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepActor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.device = device\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        in_dim = hidden_size+state_size\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(in_dim, hidden_size)\n",
    "        self.fc3 = nn.Linear(in_dim, hidden_size)\n",
    "        self.fc4 = nn.Linear(in_dim, hidden_size)\n",
    "\n",
    "\n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "        #self.reset_parameters() # check if this improves training\n",
    "\n",
    "    def reset_parameters(self, init_w=3e-3):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.mu.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state: torch.tensor):\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc4(x))  \n",
    "\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        log_prob = (dist.log_prob(e) - torch.log(1 - action.pow(2) + epsilon)).sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
    "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
    "        \"\"\"\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        return action.detach().cpu()\n",
    "    \n",
    "    def get_det_action(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        return torch.tanh(mu).detach().cpu()\n",
    "    \n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256, seed=1, N=32, device=\"cuda:0\"):\n",
    "        super(IQN, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = hidden_size\n",
    "        self.pis = torch.FloatTensor([np.pi * i for i in range(1, self.n_cos + 1)]).view(1, 1, self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "        self.head = nn.Linear(self.action_size + self.input_shape, hidden_size) \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, hidden_size)\n",
    "        self.ff_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ff_2 = nn.Linear(hidden_size, 1)    \n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device)\n",
    "        cos = torch.cos(taus * self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        x = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(x  ))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication > reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1) * cos_x).view(batch_size * num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        #action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        #x = torch.cat((x,action),dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n",
    "\n",
    "class DeepIQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, device=\"cuda:0\"):\n",
    "        super(DeepIQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.input_dim = action_size+state_size+layer_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.ff_1 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_2 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_3 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_4 = nn.Linear(self.layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])  \n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        xs = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(xs))\n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_1(x))   \n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_2(x))\n",
    "\n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication > reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        state = input.repeat(num_tau,1).reshape(num_tau,batch_size*self.input_shape).transpose(0,1).reshape(batch_size*num_tau,self.input_shape)\n",
    "        \n",
    "        x = torch.cat((x,action,state),dim=1)\n",
    "        x = torch.relu(self.ff_3(x))\n",
    "\n",
    "        out = self.ff_4(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n",
    "\n",
    "\n",
    "\n",
    "class CQLSAC(nn.Module):\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                        state_size,\n",
    "                        action_size,\n",
    "                        device\n",
    "                ):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        super(CQLSAC, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.tau = 1e-2\n",
    "        hidden_size = 256\n",
    "        learning_rate = 5e-4\n",
    "        self.clip_grad_param = 1\n",
    "\n",
    "        self.target_entropy = -action_size  # -dim(A)\n",
    "\n",
    "        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "        self.alpha_optimizer = optim.Adam(params=[self.log_alpha], lr=learning_rate) \n",
    "        \n",
    "        # CQL params\n",
    "        self.with_lagrange = False\n",
    "        self.temp = 1.0\n",
    "        self.cql_weight = 1.0\n",
    "        self.target_action_gap = 0.0\n",
    "        self.cql_log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.cql_alpha_optimizer = optim.Adam(params=[self.cql_log_alpha], lr=learning_rate) \n",
    "        \n",
    "        # Actor Network \n",
    "\n",
    "        self.actor_local = Actor(state_size, action_size, hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=learning_rate)     \n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "\n",
    "        self.critic1 = IQN(state_size, action_size, hidden_size, seed=1).to(device)\n",
    "        self.critic2 = IQN(state_size, action_size, hidden_size, seed=2).to(device)\n",
    "        \n",
    "        assert self.critic1.parameters() != self.critic2.parameters()\n",
    "        \n",
    "        self.critic1_target = IQN(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "\n",
    "        self.critic2_target = IQN(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=learning_rate) \n",
    "\n",
    "    def get_action(self, state, eval=False):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if eval:\n",
    "                action = self.actor_local.get_det_action(state)\n",
    "            else:\n",
    "                action = self.actor_local.get_action(state)\n",
    "        return action.numpy()\n",
    "\n",
    "    def calc_policy_loss(self, states, alpha):\n",
    "        actions_pred, log_pis = self.actor_local.evaluate(states)\n",
    "\n",
    "        q1 = self.critic1.get_qvalues(states, actions_pred.squeeze(0))   \n",
    "        q2 = self.critic2.get_qvalues(states, actions_pred.squeeze(0))\n",
    "        min_Q = torch.min(q1,q2).cpu()\n",
    "        actor_loss = ((alpha * log_pis.cpu() - min_Q )).mean()\n",
    "        return actor_loss, log_pis\n",
    "\n",
    "    def _compute_policy_values(self, state_pi, state_q):\n",
    "        with torch.no_grad():\n",
    "            actions_pred, log_pis = self.actor_local.evaluate(state_pi)\n",
    "        \n",
    "        qs1 = self.critic1.get_qvalues(state_q, actions_pred)\n",
    "        qs2 = self.critic2.get_qvalues(state_q, actions_pred)\n",
    "        \n",
    "        return qs1-log_pis, qs2-log_pis\n",
    "    \n",
    "    def _compute_random_values(self, state, actions, critic):\n",
    "        random_values = critic.get_qvalues(state, actions)\n",
    "        random_log_prstate = math.log(0.5 ** self.action_size)\n",
    "        return random_values - random_log_prstate\n",
    "    \n",
    "    def train(self, step, experiences, gamma, d=1):\n",
    "        \"\"\"Updates actor, critics and entropy_alpha parameters using given batch of experience tuples.\n",
    "        Q_targets = r +  * (min_critic_target(next_state, actor_target(next_state)) -  *log_pi(next_action|next_state))\n",
    "        Critic_loss = MSE(Q, Q_target)\n",
    "        Actor_loss =  * log_pi(a|s) - Q(s,a)\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        current_alpha = copy.deepcopy(self.alpha)\n",
    "        actor_loss, log_pis = self.calc_policy_loss(states, current_alpha)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Compute alpha loss\n",
    "        alpha_loss = - (self.log_alpha.exp() * (log_pis.cpu() + self.target_entropy).detach().cpu()).mean()\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        with torch.no_grad():\n",
    "            next_action, _ = self.actor_local.evaluate(next_states)\n",
    "            #next_action = next_action.unsqueeze(1).repeat(1, 10, 1).view(next_action.shape[0] * 10, next_action.shape[1])\n",
    "            #temp_next_states = next_states.unsqueeze(1).repeat(1, 10, 1).view(next_states.shape[0] * 10, next_states.shape[1])\n",
    "            \n",
    "            Q_target1_next, _ = self.critic1_target(next_states, next_action) #.view(states.shape[0], 10, 1).max(1)[0].view(-1, 1)\n",
    "            # batch_size, num_tau, 1    \n",
    "            Q_target2_next, _ = self.critic2_target(next_states, next_action) #.view(states.shape[0], 10, 1).max(1)[0].view(-1, 1)\n",
    "            Q_target_next = torch.min(Q_target1_next, Q_target2_next).transpose(1,2)\n",
    "\n",
    "            # Compute Q targets for current states (y_i)\n",
    "            Q_targets = rewards.cpu().unsqueeze(-1) + (gamma * (1 - dones.cpu().unsqueeze(-1)) * Q_target_next.cpu()) \n",
    "\n",
    "\n",
    "        # Compute critic loss\n",
    "        q1, taus1 = self.critic1(states, actions)\n",
    "        q2, taus2 = self.critic2(states, actions)\n",
    "        assert Q_targets.shape == (256, 1, 32), \"have shape: {}\".format(Q_targets.shape)\n",
    "        assert q1.shape == (256, 32, 1)\n",
    "        \n",
    "        # Quantile Huber loss\n",
    "        td_error1 = Q_targets - q1.cpu()\n",
    "        td_error2 = Q_targets - q2.cpu()\n",
    "        \n",
    "        assert td_error1.shape == (256, 32, 32), \"wrong td error shape\"\n",
    "        huber_l_1 = calculate_huber_loss(td_error1, 1.0)\n",
    "        huber_l_2 = calculate_huber_loss(td_error2, 1.0)\n",
    "        \n",
    "        quantil_l_1 = abs(taus1.cpu() - (td_error1.detach() < 0).float()) * huber_l_1 / 1.0\n",
    "        quantil_l_2 = abs(taus2.cpu() - (td_error2.detach() < 0).float()) * huber_l_2 / 1.0\n",
    "\n",
    "        critic1_loss = quantil_l_1.sum(dim=1).mean(dim=1).mean()\n",
    "        critic2_loss = quantil_l_2.sum(dim=1).mean(dim=1).mean()\n",
    "\n",
    "        \n",
    "        # CQL addon\n",
    "\n",
    "        random_actions = torch.FloatTensor(q1.shape[0] * 10, actions.shape[-1]).uniform_(-1, 1).to(self.device)\n",
    "        num_repeat = int (random_actions.shape[0] / states.shape[0])\n",
    "        temp_states = states.unsqueeze(1).repeat(1, num_repeat, 1).view(states.shape[0] * num_repeat, states.shape[1])\n",
    "        temp_next_states = next_states.unsqueeze(1).repeat(1, num_repeat, 1).view(next_states.shape[0] * num_repeat, next_states.shape[1])\n",
    "        \n",
    "        current_pi_values1, current_pi_values2  = self._compute_policy_values(temp_states, temp_states)\n",
    "        next_pi_values1, next_pi_values2 = self._compute_policy_values(temp_next_states, temp_states)\n",
    "        \n",
    "        random_values1 = self._compute_random_values(temp_states, random_actions, self.critic1).reshape(states.shape[0], num_repeat, 1)\n",
    "        random_values2 = self._compute_random_values(temp_states, random_actions, self.critic2).reshape(states.shape[0], num_repeat, 1)\n",
    "\n",
    "        current_pi_values1 = current_pi_values1.reshape(states.shape[0], num_repeat, 1)\n",
    "        current_pi_values2 = current_pi_values2.reshape(states.shape[0], num_repeat, 1)\n",
    "        next_pi_values1 = next_pi_values1.reshape(states.shape[0], num_repeat, 1)\n",
    "        next_pi_values2 = next_pi_values2.reshape(states.shape[0], num_repeat, 1)      \n",
    "        \n",
    "        cat_q1 = torch.cat([random_values1, current_pi_values1, next_pi_values1], 1)\n",
    "        cat_q2 = torch.cat([random_values2, current_pi_values2, next_pi_values2], 1)\n",
    "        \n",
    "        assert cat_q1.shape == (states.shape[0], 3 * num_repeat, 1), f\"cat_q1 instead has shape: {cat_q1.shape}\"\n",
    "        assert cat_q2.shape == (states.shape[0], 3 * num_repeat, 1), f\"cat_q2 instead has shape: {cat_q2.shape}\"\n",
    "        \n",
    "\n",
    "        cql1_scaled_loss = (torch.logsumexp(cat_q1 / self.temp, dim=1).mean() * self.cql_weight * self.temp - q1.mean()) * self.cql_weight\n",
    "        cql2_scaled_loss = (torch.logsumexp(cat_q2 / self.temp, dim=1).mean() * self.cql_weight * self.temp - q2.mean()) * self.cql_weight\n",
    "        \n",
    "        cql_alpha_loss = torch.FloatTensor([0.0])\n",
    "        cql_alpha = torch.FloatTensor([0.0])\n",
    "        if self.with_lagrange:\n",
    "            cql_alpha = torch.clamp(self.cql_log_alpha.exp(), min=0.0, max=1000000.0).to(self.device)\n",
    "            cql1_scaled_loss = cql_alpha * (cql1_scaled_loss - self.target_action_gap)\n",
    "            cql2_scaled_loss = cql_alpha * (cql2_scaled_loss - self.target_action_gap)\n",
    "\n",
    "            self.cql_alpha_optimizer.zero_grad()\n",
    "            cql_alpha_loss = (- cql1_scaled_loss - cql2_scaled_loss) * 0.5 \n",
    "            cql_alpha_loss.backward(retain_graph=True)\n",
    "            self.cql_alpha_optimizer.step()\n",
    "        \n",
    "        total_c1_loss = critic1_loss + cql1_scaled_loss\n",
    "        total_c2_loss = critic2_loss + cql2_scaled_loss\n",
    "        \n",
    "        \n",
    "        # Update critics\n",
    "        # critic 1\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        total_c1_loss.backward(retain_graph=True)\n",
    "        clip_grad_norm_(self.critic1.parameters(), self.clip_grad_param)\n",
    "        self.critic1_optimizer.step()\n",
    "        # critic 2\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        total_c2_loss.backward()\n",
    "        clip_grad_norm_(self.critic2.parameters(), self.clip_grad_param)\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic1, self.critic1_target)\n",
    "        self.soft_update(self.critic2, self.critic2_target)\n",
    "        \n",
    "        return actor_loss.item(), alpha_loss.item(), critic1_loss.item(), critic2_loss.item(), cql1_scaled_loss.item(), cql2_scaled_loss.item(), current_alpha, cql_alpha_loss.item(), cql_alpha.item()\n",
    "\n",
    "    def soft_update(self, local_model , target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        _target = *_local + (1 - )*_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    assert loss.shape == (td_errors.shape[0], 32, 32), \"huber loss has wrong shape\"\n",
    "    return loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "        # Experiment\n",
    "        \"policy\": \"TD3_BC\",\n",
    "        \"seed\": 0, \n",
    "        \"eval_freq\": 5e3,\n",
    "        \"max_timesteps\": 250,   #1e6,\n",
    "        \"save_model\": \"store_true\",\n",
    "        \"load_model\": \"\",                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "\t    # TD3\n",
    "\t    \"expl_noise\": 0.1,\n",
    "        \"batch_size\": 256,\n",
    "        \"discount\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"policy_noise\": 0.2,\n",
    "        \"noise_clip\": 0.5,\n",
    "        \"policy_freq\": 2,\n",
    "        # TD3 + BC\n",
    "\t    \"alpha\": 2.5,\n",
    "        \"normalize\": True,\n",
    "        \"state_dim\": 18,\n",
    "\t\t\"action_dim\": 3,\n",
    "\t\t\"max_action\": 1,\n",
    "\t\t\"discount\": 0.99,\n",
    "\t\t\"tau\": 0.005,\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "        \"state_dim\": 18,\n",
    "\t\t\"action_dim\": 3,\n",
    "\t\t\"max_action\": 1,\n",
    "\t\t\"discount\": 0.99,\n",
    "\t\t\"tau\": 0.005,\n",
    "\t\t# TD3\n",
    "\t\t\"policy_noise\": 0.2,    #args.policy_noise * max_action,\n",
    "\t\t\"noise_clip\": 0.5,  #args.noise_clip * max_action,\n",
    "\t\t\"policy_freq\": 2,\n",
    "\t\t# TD3 + BC\n",
    "\t\t\"alpha\": 2.5\n",
    "\t}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, **kwargs):   #config\n",
    "    #np.random.seed(config.seed)\n",
    "    #random.seed(config.seed)\n",
    "    #torch.manual_seed(config.seed)      \n",
    "\n",
    "    ############################# Define Path and Load Dataset ######################################\n",
    "\n",
    "\tpath = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\"\n",
    "\tpath2 = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 2/\"\n",
    "    ## WARNING check which type of feedback in which round, and which corner for each subject\n",
    "\t#corner =  \"left\"\n",
    "\t#data = resultsSingleSubject(initial, \"reward\", path, corner)\n",
    "\t'''dic for one subject composed of ~1000 timepoints for one shot, 25 shots in one block, and 10 blocks\n",
    "    First 3 blocks are baseline learning, then 6 blocks of adaptation to perturbation, and one final washout block\n",
    "    That is 250 shots per subjects, 300'564 points in the dictionnary'''\n",
    "\n",
    "\t# Environment State Properties\n",
    "\tcorner = \"all\"\n",
    "\tstate_dim=14\n",
    "\taction_dim=2\n",
    "\tmax_action = 1\n",
    "\tnormalize = True\n",
    "    # Agent parameters\n",
    "\tkwargs = {\n",
    "\t\t\"state_dim\": state_dim,\n",
    "\t\t\"action_dim\": action_dim,\n",
    "\t\t\"max_action\": max_action,\n",
    "\t\t\"discount\": args['discount'],\n",
    "\t\t\"tau\": args['tau'],\n",
    "\t\t# TD3\n",
    "\t\t\"policy_noise\": args['policy_noise'] * max_action,\n",
    "\t\t\"noise_clip\": args['noise_clip'] * max_action,\n",
    "\t\t\"policy_freq\": args['policy_freq'],\n",
    "\t\t# TD3 + BC\n",
    "\t\t\"alpha\": args['alpha']\n",
    "\t    }\n",
    "    \n",
    "\t# Initialize Agent\n",
    "\tif args['policy'] == \"TD3_BC\":\n",
    "\t\tpolicy = TD3_BC(**kwargs) \n",
    "\telif args['policy'] == \"CQL_SAC\":\n",
    "\t\tpolicy = CQLSAC(state_size=state_dim, action_size=action_dim, device=device)\n",
    "\telse:\n",
    "\t\traise ValueError(\"Chose Agent between [TD3_BC, CQL_SAC]\")\n",
    "\n",
    "\tif args['load_model'] != \"\":\n",
    "\t\tpolicy_file = file_name if args['load_model'] == \"default\" else args['load_model']\n",
    "\t\tpolicy.load(f\"./models/{policy_file}\")\n",
    "    \n",
    "\t'''\n",
    "    # Dataframe for all subjects\n",
    "\tinitial = \"AAB\"\n",
    "\tdata = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "\t#data = resultsMultipleSubjects([path, path2], 'reward', 'all')\n",
    "\n",
    "\t#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "\t#pd_dataset = pd.DataFrame.from_dict(dataset)\n",
    "\t#pd_dataset.to_csv(\"RL_dataset/AAB.csv\")\n",
    "\t#print(\"AAB saved\")\n",
    "\treturn data\n",
    "\t'''\n",
    "\t#dataset = pd.read_csv(\"RL_dataset/AAB.csv\")\n",
    "\tinitial = \"AAB\"\n",
    "\tdata = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "\t#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "\t#replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "\t#replay_buffer.convert_D4RL(dataset)\n",
    "\treturn data\n",
    "\t\"\"\"\n",
    "\tif normalize:\n",
    "\t\tmean,std = replay_buffer.normalize_states() \n",
    "\telse:\n",
    "\t\tmean,std = 0,1\n",
    "\t\n",
    "\t################## Training #######################\n",
    "\t#for t in range(int(args.max_timesteps)):\n",
    "\t\t#policy.train(replay_buffer, args.batch_size)\n",
    "\n",
    "\n",
    "\tsteps = 0\n",
    "\taverage10 = deque(maxlen=10)\n",
    "\ttotal_steps = 0\n",
    "\tbatch_size = 64 #256\n",
    "\n",
    "\tfor i in range(1, 100):\n",
    "\t\tepisode_steps = 0\n",
    "\t\trewards = 0\n",
    "\t\twhile True:\n",
    "\t\t\tsteps += 1\n",
    "\t\t\tprint(\"step: \", steps)\n",
    "\t\t\tpolicy.train(replay_buffer)\n",
    "\n",
    "\t\taverage10.append(rewards)\n",
    "\t\ttotal_steps += episode_steps\n",
    "\t\tprint(\"Episode: {} | Reward: {} | Polciy Loss: {} | Steps: {}\".format(i, reward, policy_loss, steps))\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_619/1458895838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTD3_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{args['policy']}_{args['seed']}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_619/4170048367.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_dim, action_dim, max_action, discount, tau, policy_noise, noise_clip, policy_freq, alpha)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize policy\n",
    "policy = TD3_BC(**kwargs)\n",
    "\n",
    "print(args[\"policy\"])\n",
    "file_name = f\"{args['policy']}_{args['seed']}\"\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Policy: {args['policy']}, Seed: {args['seed']}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "\n",
    "if args['save_model'] and not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "    \n",
    "#dataset = train(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5158/3957423419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph Analysis 1 subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = \"AAB\"\n",
    "df = pd.read_csv(\"RL_dataset/\"+initial+\".csv\", header = 0, \\\n",
    "        names = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]', r'\\r'], '', regex=True) \n",
    "states = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "actions= pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "new_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "trial = df['trial'].astype(int)\n",
    "terminals = df['terminals'].astype(bool) #=='True'    #Dataframe Object type converts to string (if convert using astype(bool), all values are True)\n",
    "#print(df['rewards'])\n",
    "rewards = df['rewards'].astype(float)\n",
    "#print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "\n",
    "successful_rewards_ind = []\n",
    "for i in range(len(rewards)):\n",
    "        if terminals.iloc[i] == True:\n",
    "                if rewards.iloc[i] != -10.0:\n",
    "                        successful_rewards_ind = np.append(successful_rewards_ind, i)\n",
    "successful_trial_list = trial.iloc[successful_rewards_ind]\n",
    "\n",
    "\n",
    "fig1, action_pattern_x = plt.subplots()\n",
    "fig2, action_pattern_z = plt.subplots()\n",
    "\n",
    "for i in successful_trial_list:\n",
    "        action_pattern_x.plot(np.arange(1, len(actions[trial == i])+1), actions[trial == i][0])\n",
    "        action_pattern_z.plot(np.arange(1, len(actions[trial == i])+1), actions[trial == i][1])\n",
    "\n",
    "action_pattern_x.set_xlabel(\"timesteps\")\n",
    "action_pattern_x.set_ylabel(\"Impulse force\")\n",
    "fig1.suptitle('Impulse force along x-axis at each timestep \\n (30 timesteps before hit, 20 timesteps after)', fontsize=16, y=1.04)\n",
    "action_pattern_z.set_xlabel(\"timesteps\")\n",
    "action_pattern_z.set_ylabel(\"Impulse force\")\n",
    "fig2.suptitle('Impulse force along z-axis at each timestep \\n (30 timesteps before hit, 20 timesteps after)', fontsize=16, y=1.04)\n",
    "\n",
    "\n",
    "\n",
    "#unsuccessful_trial_list = trial.drop(a.index, axis=0)\n",
    "successful_actions = actions.iloc[successful_trial_list.index]\n",
    "\n",
    "#unsuccessful_actions = actions.iloc[unsuccessful_trial_list.index]\n",
    "#successful_actions = pd.DataFrame(index=range(len(successful_trial_list)*states[trial==1].shape[0]),columns=range(actions.shape[1]))\n",
    "#unsuccessful_actions = pd.DataFrame(index=range(len(unsuccessful_trial_list)*states[trial==1].shape[0]),columns=range(actions.shape[1]))\n",
    "\"\"\"for i in successful_trial_list:\n",
    "        successful_actions.iloc[i:i+states[trial==1].shape[0]] = actions[trial==i]\n",
    "for i in unsuccessful_trial_list:\n",
    "        unsuccessful_actions.iloc[i:i+states[trial==1].shape[0]]  = actions[trial==i]\"\"\"\n",
    "\n",
    "s_a = np.zeros((len(successful_actions), states[trial==1].shape[0], actions.shape[1]))  #trial 1 is always complete (250 elements)\n",
    "u_a = np.zeros((len(trial.unique())-len(successful_actions), states[trial==1].shape[0], actions.shape[1]))\n",
    "j = 0\n",
    "k = 0\n",
    "print(\"a shape: \", actions.shape[1])\n",
    "\n",
    "episode_len = states[trial==1].shape[0]\n",
    "for i, trial_ind in enumerate(trial.unique()):\n",
    "        pad_len = episode_len - actions[trial==trial_ind].shape[0]\n",
    "        a = actions[trial==trial_ind]\n",
    "        if np.isin(trial_ind, successful_trial_list):\n",
    "                if pad_len > 0:\n",
    "                        print(\"pad successful trajectory \", trial_ind)\n",
    "                        pad_array = np.array([pad_len*[np.nan]])\n",
    "                        a = np.pad(a, pad_width = ((pad_len,0), (0,0)),  mode = 'constant', constant_values = np.nan)\n",
    "\n",
    "                s_a[j][:][:] = a\n",
    "                j+=1\n",
    "        else:\n",
    "                if pad_len > 0:\n",
    "                        print(\"pad unsuccessful trajectory \", trial_ind)\n",
    "                        pad_array = np.array([pad_len*[np.nan]])\n",
    "                        a = np.pad(a, pad_width = ((pad_len,0), (0,0)),  mode = 'constant', constant_values = np.nan) \n",
    "                u_a[k][:][:] = a\n",
    "                k+=1 \n",
    "print(\"s_a shape: \", s_a.shape)\n",
    "s_a = s_a[:,:,0:2]\n",
    "print(\"s_a cuevel shape: \", s_a.shape)\n",
    "print(\"u_a shape: \", u_a.shape)\n",
    "u_a = u_a[:,:,0:2]\n",
    "print(\"s_a cuevel shape: \", u_a.shape)\n",
    "successful_mean = np.nanmean(s_a, axis=0, keepdims=True)      #numpy.nanmean()\n",
    "successful_std = np.nanstd(s_a, axis=0)\n",
    "unsuccessful_mean = np.nanmean(u_a, axis=0)     #unsuccessful_trial_list\n",
    "unsuccessful_std = np.nanstd(u_a, axis=0)         #unsuccessful_trial_list\n",
    "\n",
    "#print(successful_mean.shape[0], successful_mean[:][1])\n",
    "successful_mean = successful_mean.reshape(-1)\n",
    "successful_mean = successful_mean.reshape(episode_len,2) #(50,2)\n",
    "#successful_mean = successful_mean[:,2:4]\n",
    "unsuccessful_mean = unsuccessful_mean.reshape(-1)\n",
    "unsuccessful_mean = unsuccessful_mean.reshape(episode_len,2)      #(50,2)\n",
    "\n",
    "\n",
    "fig3, mean_x = plt.subplots()\n",
    "#mean_x.plot(np.arange(1, len(actions[trial == 1])+1), successful_mean[:,0])   #linestyle='None', marker='^')\n",
    "mean_x.set_xlabel(\"timesteps\")\n",
    "mean_x.set_ylabel(\"Impulse force\")\n",
    "fig3.suptitle('Mean of Successful trial x-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_x.errorbar(np.arange(1, len(actions[trial == 1])+1), successful_mean[:,0], successful_std[:,0], color='blue', ecolor='orange', label='successful')#, linestyle='None', marker='^')\n",
    "mean_x.plot(np.arange(1, len(actions[trial == 1])+1), unsuccessful_mean[:,0],  color='green', label='unsuccessful') \n",
    "mean_x.legend()\n",
    "\n",
    "fig4, mean_x = plt.subplots()\n",
    "mean_x.set_xlabel(\"timesteps\")\n",
    "mean_x.set_ylabel(\"Impulse force\")\n",
    "fig4.suptitle('Mean of Unsuccessful trial x-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_x.errorbar(np.arange(1, len(actions[trial == 1])+1), unsuccessful_mean[:,0], unsuccessful_std[:,0], color='green', ecolor='red')  # linestyle='None', marker='^')\n",
    "\n",
    "fig5, mean_z = plt.subplots()\n",
    "mean_z.set_xlabel(\"timesteps\")\n",
    "mean_z.set_ylabel(\"Impulse force\")\n",
    "fig5.suptitle('Mean of Successful trial z-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_z.errorbar(np.arange(1, len(actions[trial == 1])+1), successful_mean[:,1], successful_std[:,1], color='blue', ecolor='orange', label='successful')     #, linestyle='None', marker='^')\n",
    "mean_z.plot(np.arange(1, len(actions[trial == 1])+1), unsuccessful_mean[:,1], color='green', label='unsuccessful')\n",
    "mean_z.legend()\n",
    "\n",
    "fig6, mean_z = plt.subplots()\n",
    "mean_z.set_xlabel(\"timesteps\")\n",
    "mean_z.set_ylabel(\"Impulse force\")\n",
    "fig6.suptitle('Mean of Unsuccessful trial z-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_z.errorbar(np.arange(1, len(actions[trial == 1])+1), unsuccessful_mean[:,1], unsuccessful_std[:,1], color='green', ecolor='red')\n",
    "\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#fig1.savefig('analysis/action_success_vs_unsuccess/'+initial+'_250_action_Succesful_pattern_x.png', bbox_inches='tight')\n",
    "#fig2.savefig('analysis/action_success_vs_unsuccess/'+initial+'_250_action_Successful_pattern_z.png', bbox_inches='tight')\n",
    "\n",
    "#fig3.savefig('analysis/action_success_vs_unsuccess/'+initial+'_250_action_Successful_pattern_mean_x.png', facecolor='w',bbox_inches='tight')\n",
    "#fig4.savefig('analysis/action_success_vs_unsuccess/'+initial+'_250_action_Unuccessful_pattern_mean_x.png', facecolor='w',bbox_inches='tight')\n",
    "\n",
    "#fig5.savefig('analysis/action_success_vs_unsuccess/'+initial+'_250_action_Successful_pattern_mean_z.png', facecolor='w',bbox_inches='tight')\n",
    "#fig6.savefig('analysis/action_success_vs_unsuccess/'+initial+'_250_action_Unsuccessful_pattern_mean_z.png', facecolor='w',bbox_inches='tight')\n",
    "\n",
    "\"\"\"\n",
    "actions[trial == 1][0].hist(bins=15, color='steelblue', edgecolor='black', linewidth=1.0,\n",
    "           xlabelsize=8, ylabelsize=8, grid=False)    \n",
    "plt.tight_layout(rect=(0, 0, 1.2, 1.2))  \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "g = sns.FacetGrid(df_monthly, col=\"Year\", row=\"Month\", height=4.2, aspect=1.9)\n",
    "g = g.map(sns.barplot, 'District', 'PM2.5', palette='viridis', ci=None, order = list_district)\n",
    "\n",
    "g.set_xticklabels(rotation = 90)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D KDE distributions visualisation (for each timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5IAAAEkCAYAAABdfQ5TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAADzM0lEQVR4nOy9d3xcWXn//z6jrlGZGTXLkm11997WdXdhaUuogUAogSQESAiBJBDal7AhQCDhB4QQAgFCXXqHZXfZ5m6v7XWRJRf1LlnSFPU2M+f3x7kzHo1H0owtr3eX5/166WXPnXPPPffeM+eez32e8zxKa40gCIIgCIIgCIIgxIvtdjdAEARBEARBEARBeHYhQlIQBEEQBEEQBEFICBGSgiAIgiAIgiAIQkKIkBQEQRAEQRAEQRASQoSkIAiCIAiCIAiCkBAiJAVBEARBEARBEISEWFBIKqXeqpTSSqmqqO3blVIepdRZpVT+rWvijaGUustq910LlLvPKhf6G1FKNSqlvq+UetHT09r5UUq1KaW+FfH5Lqvdi/YiIOI+x/rzJVBPmdW2ihjfzTqPpwurPc+Lsf1bSqm2p7s9z2VCv6fb3Y5EudG+GTF+JC9QLq7xSFgYpdTLlFIXlFKT1jV1JLDvs7J/CrFRSq1SSj2ulBq2+sIrE9j3pn6TMZ7LoWdoWQJ1vFUp9RcJHnfWc8t65mql1NsSqSdif5tS6gtKqV6lVFAp9csbqSfOY71SKfUPMbaH7sU9t+rY1nHeopT6mVKq3Tret+LYx2Fdm5jtU0qtVUr9Xik1qpRyK6W+qZRyxVFvwvO4G+ljEftqpdR9ie43R10x7+MzBes30nWD+26y7st193Axr+FiopR6r1Lq1bfr+DckRJRSu4FHgUbgeVrrwUVt1e1hL7ALeAXwacAJPKSU+u5iCrYb5FXAv0Z8vgv4GLfGovxazHWI/EtkcC/DtO06Icn15/F08THgOiGJacurnua2CM9MblffFBLAEuz3A93ACzHj08htbZRwO/kc5lnzJ5i+cPA2tuUBqw29CezzViAhIcniP7deA7wH+A9gD/BPi1h3NK8EbqcAeRNQCTwCDMe5z2fm+kIptRQ4AGRgruO7MPOl38Yxb7yLxOdxN9LHbgWv5Pbex1vJJsx9ifUyYBfw9ae1NfHxXuC2Ccl536LHQil1J/Bb4Dxwr9Y63h/jXPUpIEVrPX0z9SwCT2qt/RGfv6GU+nvMg+oc8P/dllYBWuuzT+Phzmmtm25FxU/zeSyI1rr5drdBuL0opdK01lPPtL4pzEkJkA38WGt96HY3RrjtrAYOaa0fut0N0VoPAAO3qv6IsWqxn1urrX+/oLUO3mxloXbebD23iBeFzlEp9eKFCiul9mDE57uBb8Qo8n4gBXiZ1tpn7dODeaHxSuDni9FopVQK4L/VfUyYH631idvdhmckWut5/zBvzDRQBbwAGAceB+wxyr4aOGGV8QE/AZZHlWkDvod5C3cZmMG8XQsd5w7MG+dhoAf4IpAeVUcm5i1RKzBt/fsRwBZR5i6rvrsWOL/7rHLJc3x/BuiI2lYAfAXzVnzKOo+3z3Hd5j0fjJj/V6AZmAQGgSPA3qhr9q2o9kb/pWEGmM/Pcw9XxXOfF7heS4BvW+cyhXkz9lugMOKaR//dFX0eUcfcDfwYY1m4CnzI+v7FwFlgDDgFbI1qywuB31ltGAfqgH8EkiLKxGrPfdZ33wLaouosBr5j3YcpoBZ4043c23muYQ7wpYhreAX4e0DF6L8vt8oOWn/fAxxxHOP1mN/pADBqXce3xCingU8Af4f5HY1gHoJro8olWeVC1/pxYFXk9Yzsn1H7JgMfwvxOpqzz/v8WulZAPfDzGNt3WMd9lfW5Cviu1f4JoAX4H8AZtd+3gC7MW8VjVtn/nKNvFgBfBRqs8+0Evg+UzDF+rAeesMr2Ah8njvGIOMbMOa7Ni6xzGLLu7xXgn6POtS3GfgeAAzHGsy9b5zhl/ftdIC2izEbgF4Dbum5XsH6niZwL8AZMXxzF/G4uAO+I+H47xloQOk4L8OV5xr4Dse5fVP+et3/G2OcB4EyM7cWAH/j7hcbCeeous9r01qjt1/UP614dwVg4znBtjHtV1L411r3pxzxDOqxrnxw1XpXF6rsxfqsfAC5adQ0ADxHx7Eigv/wa8Fr38SiwL+pYc97rha4vczxrEun7sa75PPftPVYfmwROA/ui+1ys68w8/d1qz1z9OVTXfute+jAvea87P671qb/BvPjut/rKb6PveYzzaovRhrdG9Pd4n4XXtTPGsb4V41htUfdiwecdN/g8idGeLmKMGRHfp2B+bx+NaN89UWWagftj7NsOfHueuu+LcS1C/Tfyfv67dX5BjJdcrD6WyLP+vrnaFPHbnve5N999nKfeeObs6cDnrWs+CvQBvyHGvBUox4w5fVYfaMF6lke0sQvYDBy2zqUReOcC7Qxd3+i/sljXMOI+rgIexsxVO4A/t75/s9VPRzHzg8oYx3w7xjAXmv9/A3DFGH8uYcZJL2YMCs1/2mK0N3JcimcsDl2v3Zi59qRV77vj+S0lYpF8KaYjHLBOYCLyS6XUOzGTt29iJlHZ1kU+qJTaoLWOdD+6G2M+/hfMoNeGGZjBdI4fYCYlu6w6vBhTc8i16WFgDUaAXcBM6D+KMUX/YwLnFA8PAh9WSi3XWncopXIwD/cMq22tmEnd/1hv4v4rav95zwfz0P57zI/qHEZkbCO2WR2MWb0U+EuMO24AQGs9pZT6JvCXSqkPaa0nI/Z5B3BQa305jvNNirHeK6ivvan8LrAC8yauEygCno8ZKM5gXDv+GyNMTln7XFzgmN/GPLD+F+Na+ylr3dO9wCcxP8J/B36plKrU16zXFcBjwH9hOv42zPUtAD5oldkFHMf8UL5qbYvpO6+UsmNElBP4sHV+bwK+q5TK1Fr/b9QuC93bWMewYSaqW4B/xvTfl2ImAAXWcSP5T8yE4A3ASus6BIC3zHUMiwrgpxg37SDmQf91pVSG1vorUWXfhBEG7wFSMS5Ov1JKrdLXrPT/YrXtPzBu7Vsxg1M8fA94GWb8OIZ5A/6vmAfmH8+z33eB+5RSTq21N2L7mwEP5joCLMXcq/dirn+F1dbfYe5LJLnAD4HPWmUmiI0L06c+hHlAL8WMLUet6zIZVf6XwP8B/4YZDz6Kue73zXVyCY6ZkftVYK79T639poFqYruTz4tSyom5Jy7Mi4JazET9FZi+MKWU2oEZ95swY1WXdbwNiZyLUmovpi98ETN+2DAPYIdVRxZmbD+JeaCPYPrIbuswX8dMMn5itfUB4ndPS4TvAj9QSq3RWkeOXW+w/v1+RLm5xsLFohIzBvwbZpLxj8BPrD4Y8hx5ANPv/9oqU4IZO29k6cMPMZaUL2B+5+mYsaMYuBxnf9mCmbydBf4KM4l7J/CoUmq31vqpOO41LPys2YX5HZziFrqlK6X+EnM9vgX8CPPi6geYPj7ffvP2d4xQ+B7mJd07rG3R/fl+61ivYWEPsg9h5hB/jrknnwJ+r5Raq7WemWOfV2Ge1W/l2ljZfAPPwnja+a+YZ9x2jGAEIwIiied5d6PPk0T5J0yf/neuf46glMrACJpYro71mDnqXMScx0XxEUzffjumj0Q/c0Ik8qxfiHiee/HcxzAJzNnTML+p0AtrF+Y3clwptVpr3WfVV44ZN8Yxc6hGYDnGsBBJDmas/gLmmfTnmHn6Fa31E3M09wHr+P8PMxcNzRUXciX+CfA1zLzib4D/U0pVY15AfBDzUuI/rfbsjLg2n7bOPzRGlFjHX2eNlQGl1BsxL0o+jhlXMzDP3pBGeBVmrnOea/ONAav+BcfiqOv1I8zvqgnzguKLSqkRrfW35j37ON7avJVrKreJiLeOEWWyMG/G/y/GW4Np4L0R29qsk1kyx3H+JWr7b4GGiM9vtsrtjyr3EetYhXr2G667Fji/+5jfIvkO6/ud1uePYn5o1VHlvoZ5iEe/BV7ofH5LDKtL1D5tzH7DELPNmAElALw5YtsGq+zrb/BNjAZ+G1FuFPi7eeoJXfd74jiP0DEjLSnJmJcLM0B5xPaXW2XvnOO4ytr3I5hJVeSbLg18IsY+32L2m92/jdVnMBOqfixLZ7z3do52/hGxLRJfxwzG+VHX8dtR5b5k9T8133Gi9rFZ1+ZrwPmo7zRmIE6J2PYaa/tu67PTuu9fjtr3H1jA4oN5QaSBP4va943W9k3ztHuZ1Z8jLVYpmEHyy/Psl4x5OGtgc9T91sArFuqbMb5PstqjibAIce23+MGo8l/DTI4dUffzLutz3GNmjLaE7k/OPGVm9e2I7QeYbZX5uHWNN89T1yHMRDJzju/jOhfgfYBnnuNss85rwzxlqoj9+4l5/xbqn3McI8M6n3+L2n4O+F3E53nHwjnqLpuj/bP6R8S9miHiWYMRCAHgw9bnfGu/l89zzLcSh0USs45cz3dOcfaXxzBvz1Ojfj+XgF8mcK8XvL7EsCol0Pevu+Yx9rFZff+hqO2v4/o3/7Ou80L9PaJNR+a5Z5+P8d2s84voUxeZ/dzbY23/ywXa8Ino3wSJPwuva+ccx/oW0BVje+hezPu84yaeJ/H0nYjvqjAvGe+Jat89EWWWWtuus3JhxG7zAse/j9jzuND9PEPUcz66j83RX+d71t8X7/Wx9pnruRfzPs5RR1xz9jmOnYl5jv59xPbvYMaGpQv0Mw3cHbEtDeP98L8LtDd0ja/zzou+hhH38M8itjkxnituIp7RmBc2GlgRcZ8DRMx/re2h3+0rI34D13nIRO3TBnwvxvYFx+Ko6/X6qP0fwVjX551vJrrItxLzpiKaXRg1e79SKjn0hxmAL2PekERyQltvF+Y4TiQXMG8bQrzYOrFjUcf6PWaSeUcC5xQPyvpXRxz/SaA16vgPA3lc/xZqofM5BdyrlPqkUmqvUir1RhuqtW6x2vGOiM3vwEy84/XVfxXmTVPk33uj2vt+pdR7lFLrrTWuN8uDof9oYwFrwgiy1ogyIWvqstAGpVSxUuqrSql2zIA0g3koOjATrkTZD3RrrQ9Ebf8e5g1covd2rmMEuWbViDxGKte/+Yx1jDTM2/k5UUpVK6V+oJTqxlyXGeBtmLe80TyiZ7+xvmD9GzqX9YAd89Ytkp/O1waLF2PuzU9j/F7h+rEhjNa6EzPRenNUffkYawUASqlUpdSHlVKXlVITmHM9bH0dfb4zGMG/IEqpv1ZKnVdKjWIeDB1z1AnGNTuSH2IE1ro5qk90zIzknHUeP1RKvUYpdSN9PcQLgVN6jjWiSqlMzIPtfq31+Bx1xHsupwCnUup7Sqk/UtdHW23EuMZ9VSn1JqXUMm4D2njb/BR4Y2h8U0qtx7gIfTei6K0YC6Np1Fo3RrStHzORD/023Ri3rk8rpf7Kegt+o7wQ85z72gJl5usvGcCdmLEiGNEXFEaEhPpCPPf66bi+C1Fq/UX/vn+GGRPmY6H+Hg+/SKDsT3XEGket9VGuufInSqLPwkTaOR8LPe9u+HmSIP8D/Epr/egi1Xcj/FJbs/n5SPBZvyAJPvfiIe45u1LqT5RSTyqTKcCPcRXNijr2CzHGjZ4FjjuuIyyP2qzbbWDhOdqNEDmH9WLG6BN6dgyZ6DnsCzDCP/q5+SRGPEc+Nzcppf5LKXWP9UxekATG4hABzLgWyQ8x16tkvmMlIiT/HuO7+zGl1AeivgtNZB7lWkcO/a3HCKxI5jMTe6I+T2EGkshjrYhxnJPW99HHullCNz3U5kLMDYg+fmiSHX38hc7nUxhXyJdjJr+h8NH5N9jeLwN7lFLrLPeUNwHf1PEHM6rTWp+O+osMvvM6jDvRP2HcmrqVUv+sbi6yrTfq8/Qc28C4WoVcRH+NsfB9AvM2fTvGFTZcLkFcxO6bfRHfR7LQvZ3rGJ4Y9yORY8A852e5jT2Cmfh+EPMWdzvG9TJW+xY6RrH1b39UuatztSGCQoxAHmP27yVU10K/1+9i+nO59fnNQJPW+nhEmX/DvBn8HsZNeAfXIphFX6cBrXUsN6JZKKXejfktPWrVtYNrD7xY1z76WoQ+zzUAJzpmhrF+jy/CjN/fBfqUUieUCYSWKHnM4ept4bSOM1+ZuM5Fa30Q4y60DDPxHFBKPaqU2mB9P4RZ9tCDufYdSqk6pdRiuqvFy3etdt5lfX4z5uH+y4gyt2IsjCb6twnm95kO4UVVL8Csmfk3oEEp1aKU+usbOFYeZmyay907VGa+vuDCvPH+KNf3hb/FCCtbnPf66bi+CxEa+2b9vq0Xnu75dlyov8dJItE5Y43HV1lgEjgHiT4LFyuK6ELPopt9niyIUupPMC7WH1cm9YcDI2YA7EqpXOv/PsyLF2eMalzE/u0mwoLX9Aae9QvVl+hzLx7imrMrpV6Gca28hHFt3ok5l4GoYy80BoWInkNCxNi5yCQ8h+Xac7OJ669NNtf68ncwyxZ2YgxFHqXUz9XCKWDiGosjz0Ff7wK/0DwGSCxqq8b4aqdj3n5Oaa2/YH0XGlDfivENjyZ6rc+Cb1nmwY1Zl/gnc3zfdhN1x+JeTLCdzojj92PWk8XiSiKVWzfuM8BnlFJLMMLocxiT/utuoL2/w1yDd2B8prMxaw8XBeuN+LuAdymlVmLWLvwL5sf+P4t1nDioxLhHvVlr/b3QRmswulE8xH7rtiTi+5vFA7iUUqlRYnIxj7ELM3Dv01ofCW1UC+Q6nIfIlyiRv+95raIWboxr0r45vl/oreLPMGtu36SU+iJmbcy/RZV5PfAdrfUnQhusB2ws4h17Xg88prUOr7mOELOxKMJYhiI/gwnIFYtEx8xZWG9an1BKpWEshh8HHlBKlWmTjmkSM+GKJo/ZE+DQmrq58GIs6POViftctNY/xVgTsjAi7TOYNEulWuug1voc8MdWX92G8YD5sVJqo9a6bp42XHe+SqmbmVQexLyJf5NS6iBmYvPTSJF1g2NhaJ1T9L254bZanih/ZlnsNmImCV9WSrVprR9M4JiDmLEpYx4xuVB/8WH6y39jJkCx2hu0/j3HPPf6Jp418fb9eAiNfbPGOqvNC96zhfp7HMdPZK4UazwuwngwJEqiz8KbmdMlws0+T+JhDWb+FWss+yXG7d2htR5XJp/n2jnquNlUNPFc08V+1if63IuHeOfsr8e8JH5rxLFTuP6lxUJj0LOF0Fj0QmKLXjeEXxZ+FeO94bTK/38Y0b0zxn4hfMQ5Fls4lVIpUWJyoXkMkOBifOugb8FY3z6vTIAFMAueRzA+xdHWrNNa64TE1QI8hHnDNzrHsRYtp6Uy6T82YYRd5PFXYcRlrOPPOwGcD611n9b665i3QXO5xMG1t3QZMeoIYjrdmzETikf1LUpzobW+orX+MOZHEGrvnG1bZELm/XCntwadN8YoOx1new4CpcqE/I7kDZiXBwsFDYqHg5jf3Wujtr8R087j1+2ROLGujRMTEONGuIB5Axzd5ujPsXgI8/Ipd47fy7wPfuv39EuMZf01mLes34sqlknEuVr8eRxtm49E64x+SL4es47jQoyysEhjpjbpAB7HBISwY9YlgnElKlJKFYTKKqUquX5y+Htgh1Jq4xz1j2OCi73JcpVZlHPRWo9qrX+LGauKiZqUa6392oRa/yjm97I6uo4o2rl+zHzpAvvMifXw/h6mz92Lmbh8d57yscbCWFzFjJGL1taINmhLnP2DtSl0jPaoz6GJZnRwit9j3J7eNs9hFuovYxjPmo2YdT3X9YcY+yx4rxO4vhB/34+HLoyLdvTv+49J4EX8PP19isV7Vr4m0sJgPcdKubFnyq16Ft7s+d7U8yROvoWxlkf+/b313fswL/tD/Bp4aYSVMhRkaQULB6NbjLnSYj/r433uJXIf452zZ3K9u/ibMVa1SH4P/JFSqphbw9M1h30EI/SWz3FdWqN30Fp7tdY/wrjaR46D192PGxiLk7g+WNXrMS9U5xWSCb+10CaK0Bswb/y+bFkmv6mUej/w39bg/SDmrU0Jxkf3gNY6ek3YjXI/pmM/ppT6/zBWt1SMherlmAWqc63lmY+dSqkAZpCqwEwgXoKJKPrFiHKfx1gKDyulPo+xQNox4nKf1jqhH7BS6lfWOZzBPCQ3Y3zKvzrPbqFB/B+VUg8CgahO8Q2Mq99GEo9itmkOt9rTmPN8FHMPQqlbXoFx7QitUWjADAZ/oZTyYKW3uBmBPQeXMBOGT1r3bYZrg300FzGD/UOYa9wzxwPnWxhL88+VUh/BTCLeiHEde4eOwyUyDh7ETMy/Yv1W6jET1bdhgnssxouQY5jof/+tlPoY5r79P8ybvNz5doyF1tqrlPoCJnrxCKYPbMFEnAMzGM617wGl1A8wb+U/h3FnCWIWmt8LfEBr3bBAE76LmcD8C3BUGwtMJA8Bb1FKXcC4ibya2dEfb4SHgA8opT5stfl5mDFhLv7KmsSdwridvg2zKH8oVmGt9fCNjpnWC7z9GO+DTsya0Q9h3saHrHY/wUTI+5513UNlovvX5zHX9lGl1Ccwwjcf87t+p/W7fR9mYnncGnO7MGPkJq31u+M9F6XUxzFvOJ+w2lqKCUBwTms9oJT6I4zXyy8xb7Dt1vcjLDwZ/iEmUt7nMWtgN2IspDfDdzERK7+CeZgeCH1hTRwXGguvQ2utlVI/wkTXbsA8P17KNRfahLDcJP8T83a6CTMZeCtmDH7cKnYKk6bgP6w+OoWJLDjL9U1r/YRS6mfA55RZs/g4Zg3TfuABbdbLxdNf/gEToOlhpdQ3MFa9fMyYkaS1/uBC9/pGr69FvH1/QbTWQaXUv2CiYH4T08+qMG6E80YMXqi/W8UuAn+jlHod5h6N3MSL92xMZPOvYtYx/htmLWpMa8QCfItb8yy8iLF6/zVmTjGptZ7rZdt13OzzRCm1hmvrOzOAFUqp0Lh+UGs9oLVuI8qzTV1bnnteR1j+MFHM3wT8Win1b5jn679j1rkttG50oXlcPCzqs574n3uJ3Md45+wPAa+MGMO3YfJ3+qLq+xjmXh9TSn0KM+6VAC/WWr/pBs451rmB8YT4NmbsqdWLnOtea92slPoM8CXL4+Igxtq+DPM7+7o1Jv8v156B/Zh0T29m9jh4Edhnjat9wKDVjxcciyPqGAH+3Zr/NwJ/ikk99Vbrxeq8J7NQxKW3EiOCEaYj/A6zQPMN1rZ7MYPmMNfytvwfsCZivzZiRxea6zj3cX1EsXRreyiPkAfzsLyPa1FT72KBiGyR9Uf8jWE65vcxyWtj7ePEPFBbMVakfozyf2+i54MJ/XuC2fnZ7mN2FM02ZkeHS8KYq/sxg6iO0caHMQ+vmNFo57n+c/3lYyYeX8WIn1HrPp8K3f+Iut6BcfPzR96DGOcx1zU6QFQkO65FMntbxLZNGFE2jnnQfRwzgdfMzrO0B3gK8yPVLJxH8rvElztrwb46x7XOwUTi6rX6TwNz55GMzlsVOnbZAsd4Hibk8wRmgvJ3sdpn1fWJqG2ha/3WqD73ScwgNWHdo91Wufcs8Hu1YSYl5617MGT9/98xb5YXul5J1rXSROVrtb7Px0zwvNbf/Zi1FdHn8C3miDTH9X0zA+M+N4AZYH+LsfaF+0/U+LEOM/ZNWNfoX4kvj+SCY2aMtu4CfsW1PH69mMnzyqhyr8QIywnrer+Q2HkkCzHu76H+2Il5gRaZF3AzJqeXz6rvMmbSFve5YATTw9ZxpqzjfAMr+h7GYvQjzLg6aV3732FFzLbKzBW11YYJBd9uHfthzEQl5v2KZ0y0yp+y6vhU1Pa4xsI56nRwbYzxYITqS6P7B3NH9GzjWl7hQuteNVjn7cFMSF4Utc9aq75RjCj+h1jXgmuRrxusvhC6BysjysTTX1ZjfpP91r3uwlho7o3nXsd7fZkj8iZx9H0SzyPZzrU8kntZII8kC/R3q8wS67xHrH0PRNUVK3Lkt5g/j+SA1RceICLy+Tzndl3UVmv7DT8L5zmWHZMmxGvt1xZ1LxZ83nETzxPmzsM9bz+Yq33Wd+sx1qUx67y+BeTFcS1izuOIMddZ4Hok8qy/b4E2xfvci3kf56k3njm7zeqLPVb/PYh57rRxfWTmSuv4oWUczcDnon4jsaIDHyDq+TdHez+GscIFmP2bnuv5Hx15t40onTNXH8KIwhNW/xnFGEm+BJRa37/FandoLG3F6I/IiLCruJYvUzN7XJp3LI68XszOI9lOnFHJQyGVhecQlmtDB/AFrfVHb3d7hOcm1pvcn2DCeh9eqLwgCIIgCILwzEEp9S2MwC29kf1vdEGu8AzEcitbiXlbZ8NE3xKEm0YptRPzhv1JzNuqrRj3rhMYq7AgCIIgCILwB4QIyecWLwW+ibFGvkVrvVghuQVhFLNW6l0Y19x+zILvD2lxaxAEQRAEQfiDQ1xbBUEQBEEQBEEQhIR4OhP7CoIgCIIgCIIgCM8BREgKgiAIgiAIgiAICSFCUhAEQRAEQRAEQUgIEZKCIAiCIAiCIAhCQoiQFARBEARBEARBEBJChKQgCIIgCIIgCIKQECIkBUEQBEEQBEEQhIQQISkIgiAIgiAIgiAkhAhJQRAEQRAEQRAEISFESAqCIAiCIAiCIAgJIUJSEARBEARBEARBSAgRkoIgCIIgCIIgCEJCiJAUBEEQBEEQBEEQEkKEpCAIgiAIgiAIgpAQIiQFQRAEQRAEQRCEhBAhKQiCIAiCIAiCICSECElBEARBEARBEAQhIURICoIgCIIgCIIgCAkhQlIQBEEQBEEQBEFICBGSgiAIgiAIgiAIQkKIkBQEQRAEQRAEQRASQoSkIAiCIAiCIAiCkBAiJAVBEARBEARBEISEECEpCIIgCIIgCIIgJIQISUEQBEEQBEEQBCEhREgKgiAIgiAIgiAICSFCUhAEQRAEQRAEQUgIEZKCIAiCIAiCIAhCQoiQFARBEARBEARBEBIi+XY3QBAEQUDf7gYIwnMcdbsbIAiC8FxDLJKCIAiCIAiCIAhCQoiQFARBEARBEARBEBJChKQgCIIgCIIgCIKQECIkBUEQBEEQBEEQhISQYDuCIAjCc4KZmRm6urqYnJy83U0RbhPp6emUlpaSkpJyu5siCILwnEdpLcECBUEQbjMyEC8Cra2tZGdnk5eXh1ISpPMPDa01brebkZERysvLo7+WDiEIgrDIiGurIAiC8JxgcnJSROQfMEop8vLyxCItCILwNCFCUhAEQXjOICLyDxu5/4IgCE8fIiQFQRAEQRAEQRCEhBAhKQiCIAiLQFtbG+vWrZu17b777uOzn/3sbWrR4vDFL36R1atX88Y3vnHecllZWddt8/l8fPnLX553v927dy/Yhlh1C4IgCLcXEZKCIAiCIMzJl7/8ZR555BHuv//+hPedT0j6/X4Ajh07dlPtEwRBEG4PIiQFQRAE4Wngrrvu4gMf+AA7duygpqaGw4cPA1BfX8+OHTvYtGkTGzZsoLGx8Trr5mc/+1nuu+8+AJqamrjnnnvYuHEjW7Zsobm5GYDPfOYzrF+/no0bN/LBD34QgObmZl784hezdetW9u3bx+XLlwH4yU9+wrp169i4cSP79++fsx3vfOc7aWlp4SUveQmf//znr7Owrlu3jra2tjnP+YMf/CDNzc1s2rSJ97///Rw4cIB9+/bx8pe/nDVr1gDXrI2jo6M8//nPZ8uWLaxfv55f/epX19XX29vL/v372bRpE+vWrQtfQ0EQBOHpR/JICoIgCM9JXvfV44ta34/eseum6/D7/Zw8eZLf/e53/Mu//AuPPvooX/nKV3jPe97DG9/4RqanpwkEAly9enXOOt74xjfywQ9+kFe96lVMTk4SDAZ58MEH+dWvfsWTTz5JZmYmHo8HgLe//e185Stfobq6mieffJK/+Zu/4fHHH+fjH/84Dz/8MCUlJfh8PoCY7fjKV77CQw89xBNPPEF+fn5YzMbLpz/9aerq6jh37hwABw4c4MyZM9TV1V2XoiM9PZ1f/OIX5OTkMDg4yB133MHLX/7yWQF0vv/97/OiF72Ij3zkIwQCAcbHxxNqjyAIgrB4iJAUBEEQhEVgroihkdtf/epXA7B169awJW/Xrl188pOfpKuri1e/+tVUV1fPeYyRkRG6u7t51ateBRjxBfDoo4/y53/+52RmZgLgcrkYHR3l2LFjvPa1rw3vPzU1BcCePXt461vfyp/8yZ+E25RIO26GHTt2xMrziNaaD3/4wxw6dAibzUZ3dzdXr15lyZIl4TLbt2/nL/7iL5iZmeGVr3wlmzZtuiVtFARBEBZGhKQgCILwnGQxLIiJkJeXh9frnbXN4/HMEk1paWkAJCUlhdcIvuENb2Dnzp088MAD3HvvvXz1q1+lpqaGYDAY3u9GciMGg0EcDkfYGhjJV77yFZ588kkeeOABtm7dylNPPRWzHc973vNm7ZecnHzT7bLb7TG333///QwMDPDUU0+RkpJCWVnZdfXv37+fQ4cO8cADD/DWt76Vf/iHf+DP/uzPEm6DIAiCcPPIGklBEAThOYnWOvz3dJCVlUVxcTGPP/44YETkQw89xN69e+fdr6WlhYqKCv7u7/6OV7ziFdTW1lJUVER/fz9ut5upqSl++9vfApCdnU1paSm//OUvAWNhHB8f5wUveAHf/OY3w66eHo+HnJwcysvL+clPfgKY63H+/HnArJ3cuXMnH//4xykoKKCzszNmO6IpKyvjzJkzAJw5c4bW1tZ5zy07O5uRkZG4rt/Q0BCFhYWkpKTwxBNP0N7efl2Z9vZ2ioqK+Ku/+ive9ra3hdsiCIIgPP2IkBQEQRCec4QEZCAQCP8Fg8FbLiy/853v8K//+q9s2rSJ5z3veXzsYx+jsrJy3n1+/OMfs27dOjZt2kRdXR1/9md/RkpKCv/8z//Mjh07eMELXsCqVavC5b/73e/yxS9+kQ0bNrB79276+vp48YtfzMtf/nK2bdvGpk2bwgFx7r//fr7xjW+wceNG1q5dGw5g8/73v5/169ezbt06du/ezcaNG2O2I5o//uM/xuPxsHbtWr70pS9RU1Mz77nl5eWxZ88e1q1bx/vf//55y77xjW/k9OnTrF+/nu985zuzzjnEgQMH2LhxI5s3b+ZHP/oR73nPe+atUxAEQbh1qKfrTa0gCIIwJzIQLwKXLl1i1apVs4Ri6P/Rz7rQukWbzTbrs/Ds59KlS6xevTp6s9xgQRCERUbWSAqCIAjPGeZ6ORotFLXWjI+Ph4PTKKXCf7HKC4IgCIIwGxGSgiAIwrMev9+fkMtqtGCMdnkVYSkIgiAI8yNCUhAEQXjWorXG7/eHI6BqrW9I9MWyWIqwfPYhy3UEQRCePiTYjiAIgvCsJBgMMj09jd/vRylFWloaHo9nUcREpHBUSqG1JhgMPu3Be4T40VrjdrvDuTUFQRCEW4sE2xEEQbj9yECcAKForDMzM8C1gDl+v5++vj6mpqZmlZ2L6elpUlNTF61dYq28/aSnp1NaWkpKSkr0V3JTBEEQFhkRkoIgCLcfGYjjRGvNzMwMgUBglqvpXGWnp6fnLHPq1Cm2b9++KG3SWnPlyhWWLVtGZmYmycnJ4T+bzSbi8vYjN0AQBGGRkTWSgiAIwrOCYDDIwMAAg4ODVFZWPmPEWUjQBgKBsGj0+/1hi6lSSoSlIAiC8JxDhKQgCILwjCYyoM5CVsZnAtGW0pAVNVpYpqSkkJSUJMJSEARBeFYiQlIQBEF4xhISjsFgEKUUNpuNYDAY1769vb20t7eTm5uL0+nE4XCQlJR0i1t8PUqpWceNJSxTUlJITk4WYSkIgiA8axAhKQiCIDwjibRCRkdQnY9AIMClS5fw+/2sXbuW0dFRPB4Pra2t2Gw2HA4HTqfztkVbjSUsp6enw0GCbDZbWFgmJycvuBZUEARBEG4HIiQFQRCEZxSRrqwhK2SIhYTk6OgoFy5coLS0lJKSEmZmZkhPTyc/Px+AmZkZvF4v/f39jI+Pc/bsWZxOJ06nk+zs7FnHupF23whzCcuJiQk6OjqoqKggJSUl7AorwlIQBEF4JiBCUhAEQXjGEAwGmZmZCbuyRgumuYSk1pru7m46OjpYv3492dnZMculpKRQWFhIYWEhw8PDrFmzBq/XS09PDyMjI6SlpeF0OnG5XNjt9oQF22IIvJCw1FozNDSEUorp6Wmmp6cBY7GMXmMpCIIgCE83IiQFQRCE285cuSGjiSUk/X4/Fy9eRCnFjh07SE6O/9GWlpbGkiVLWLJkCQATExN4vV7a29sZGxsjMzMzbLHMyMh42i2BITEdsliGzl2EpSAIgnC7ESEpCIIg3FYSyQ1ps9lmCcnh4WHq6upYsWIFJSUlN92WjIwMMjIyWLp0KVprxsfH8Xq9NDc3MzExQVZWVlhYpqen3/Tx5iO0NjSS0OdIYRlyhY0UltHBewRBEARhsREhKQiCINw2gsEgnZ2dpKenk5ubu6DFTylFMBhEa01nZyfd3d1s2LCBrKysRW+bUgq73Y7dbqe0tBStNaOjo3i9Xq5cucLU1BQ5OTlhYbnYxBKSsdoYnWpEa83U1FTM4D0iLAVBEITFQoSkIAiC8LQTGVBnbGwMAIfDseB+SikCgQDnz58nJSWFHTt2PG0pPZRSZGdnk52dzfLlywkGg4yMjMxaY9na2kpBQQEOh4OUlJSbOl48QjJWG+cTllprhoeHKSoqIikpKRwVVhAEQRASRYSkIAiC8LRyM7khR0ZGGBoaYt26dRQXFy9KW25USNlsNnJzc8nNzaWsrIza2lpcLhcjIyN0dHSgtQ6nGsnNzU1o7eZiES0sZ2ZmaGtrIzc3N3zuSUlJsyyWIiwFQRCEeBAhKQiCIDxthALqJJobUmtNe3s7vb29ZGVlLYqIXGyUUuTm5oYD9/j9foaGhvB6vbS2tqKUCrvB5uTkLGhJvRmRO1+dNptt1hrLYDDI5ORkuIwIS0EQBCEeREgKgiAIt5z5ckMuZJGcnp6mrq6OjIwMtm3bxlNPPbUobQoJ2FsllJKTk8nLyyMvLw8w1kCfz8fAwABNTU0kJyfPm8PyVgrJELFcYUVYCoIgCPEgQlIQBEG4pSyUG9JmsxEIBGLu6/V6uXjxIlVVVRQVFYUD7cTLM0n0pKSkUFBQQEFBAQBTU1Mxc1g6nU6ysrISOs94Cd2DuZhLWE5MTMyKGCvCUhAEQRAhKQiCINwSonNDzpXaI5Zrq9aa1tZWBgYG2LJlCxkZGXOWvVEWsy4g4brmymHZ0dHB2NgYqampTE9Ph/NZLoZgS9TKGbpnIStmLGGZnJwc/hNhKQiC8IeDCElBEARh0Yl2ZV0oN2Ska+vU1BQXLlwgOzub7du3X+eK+UzmZtoXncPS7XbT2tpKS0sLExMT2O32sMUyJKwTJRgM3lT6j1jCMhAIUFtbS2VlJWlpaSQnJ4ctljab7Rl/zwRBEIQbQ4SkIAiCsKgEAgF8Pl/YihZPLsSQNc/tdnP58mVqamrCLqC3isW2SC4mSinS09Ox2+2sWbNmVg7LhoYGpqamyM7ODgvLtLS0uOpd7HWXofsbCATCwjEQCOD3+8PfR7rCirAUBEF47iBCUhAEQVgUQtapiYkJ6uvr2blzZ1z7hcRHU1MTHo+HrVu3kp6evmhterYKl0iRO18Oy4sXL+L3+8nNzcXpdM6bwzI62M5iEbJ0xlpjGS0sI11hRVgKgiA8exEhKQiCINw0kbkhk5KS4s4LCSZNRk9PDyUlJWzbtu2WCJ1YPJMtkiHmElnROSwDgQDDw8PhNZZz5bBcKNjOjTKXy2wsYen3+2etmxVhKQiC8OxEhKQgCIJwUwSDQaanp2flhoxXoA0ODtLY2Ehubi7V1dWL2q7R0VGuXLlCVlZWWFA9XSJ1MUjEmpqUlBR2c4W5c1iGBOWtaGs81zaWsJyZmblOWKakpJCUlCTCUhAE4RmMCElBEAThhpgvN+RCBINBmpqaGB4eZvXq1QwODi5q23p6emhra6OqqoqZmRn6+/tpbGwkNTUVl8sVFlzPZIvkzbjlzpXDsre3l+HhYcbGxubNYfl0EVpDGSJaWA4MDFBSUhKOCCvCUhAE4ZmDCElBEAQhYSJdWeMJqBPJxMQEFy5cIC8vj61btzI8PLxogi4QCHD58mX8fj87duwI550MBe6ZnJwMu38ODw9z+fJl8vPzbyoS6q1ksURTKIelUoqsrCxKSkrCwvLKlSvX5bC8XWItWlh2dHRQUFDA1NRUuJ+FAvckJycn3PcEQRCExUOEpCAIgpAQ0WvcEpnIhyyDa9asCVsFlVIJramci/HxcWpra1m6dCnLli1DKcX09PSsMunp6RQXF1NcXMy5c+coLS1lbGwsHAk1JycnbLGcK2DN08WtsJaGrJxpaWkUFRVRVFQEGHHv8/no7OxkZGSEzMzMsLBcrByWN9rWaIvl9PR0WFi+613v4v7773/a2yYIgiCIkBQEQRDiJJHckNEEg0EaGhoYGxtj+/btpKamhr+z2Ww3LZpCAnXdunXk5ubGtY/NZiMzMxOXy8WyZcsIBoMMDw/j8Xjo7OwMB6xxuVzk5ubOEjRzsZiC61ZEnJ0rKE4oh2VxcTFaa8bHx/F6vbS0tDA+Ph5eZzqX5fZWiN5AIHDdNY8Ullpr6uvrF/24giAIQnyIkBQEQRAWJBgMMjMzc0OurCFLYVFREStXrrxu35uxSAaDQRobGxkdHb1OoCaKzWbD4XDgcDgAY3n1+Xy43W6am5tJTk7G6XTicrnIzs6+5Va6WyEk46lTKYXdbsdut1NaWrpoOSwTJZaQjOSZvL5VEAThDwERkoIgCMKchPIANjc3s3z58oSDsvT19dHc3MzatWvDAi0am812Q0JycnKS2tpa8vPz2bJlS8Kia6HossnJyeTn55Ofnw/A1NQUXq+Xrq6uWe6fLpeLjIyMRRc2t0IozWWRnI+FcljOzMwwNTVFf3//oroEBwKBeds6NTX1jFzXKgiC8IeCCElBEAQhJqEImoFAgO7ubsrKyhLat76+nunpaXbs2DGvuLiRfI6Dg4NcuXKFVatWhSOT3mrS0tJYsmQJS5YsmeX+2dzczMTEBDMzMwwMDFBYWHhTltFIbodFciGic1jOzMxw5swZRkdH6erqIhgMkpubi9PpxOFw3HDKkVBO0rkYHR0lMzPzRk9DEARBuElESAqCIAjXESs3ZLyMjY0xPj5OSUkJa9asWXDfRCySWmumpqZoaWlh69atpKenx92uaG5EwEbuG+n+GQwGOXPmDJOTk9TX1+P3+3E4HDclpm6Va+utSPWRmppKRUUFMDuHZVtbG0qp8LWId60pLOzaOjo6SlZW1qK0XxAEQUgcEZKCIAhCmJvJDQnX8jfa7XZKSkriEkLxCsnp6WkuXLgAwObNm297VNVIbDYbKSkpLFu2jLS0NAKBAD6fLyymbDZbeE1hTk5OXNf1VgXbuVEL4Xx1Rp7PXDksBwcHw2tNQ0GM5sthuZCQHB8fx263L+q5CIIgCPEjQlIQBEEAbi43ZCAQ4NKlSwQCAXbs2MG5c+fitjLGYxn0+XzU19dTXV1Na2vrogism7FILkRSUtIsMTU9PY3P56Ovr4+GhgbS0tLCaUbsdvvTll7j6YwEGyKUwzKUy3N6ehqv17tgDsuF1kiOjY2JkBQEQbiNiJAUBEEQwlbI+VxZ5xIhIyMj1NXVsWzZsrAVMhF31fnKaq3p6Oigt7eXzZs3k5mZSVtb27MuYmdqaiqFhYUUFhYCJm+jx+Ohra2NsbExsrKywsIy5K57O0Tf01FnamrqrByWk5OTeL1eOjs7GR0dJSMjA6fTuaCQFNdWQRCE24sISUEQhD9g4nVlTUpKIhAIzHKL1FrT3d1NR0cH69evJzs7O/xdIkJyLrHk9/upq6sjNTWVHTt2hNu2GHknQ8e9XYI0IyODkpISSkpKwuk1PB4Ply9fZnp6mtzcXGw2W9zrCePlmShO09PTKS4uDuewnJiYwOv10t/fz9TUFGNjYzFzWIYEuCAIgnB7ECEpCILwB0oiuSGjhaHf76e+vh6bzcaOHTuuW3d3oyk9QoyMjHDhwgXKyspYunTprO8SyTt5O8VivESm11ixYgWBQIDh4eGwhc7r9YbXFIYE5o1yq4TkYglepRSZmZlkZmaG683JycHr9dLY2Mjk5CTZ2dkEAgF6e3vjFpJKqRcD/wkkAV/XWn866vs04DvAVsANvE5r3Rbx/XLgInCf1vqzi3KygiAIz3JESAqCIPyBEcoNOTMzAxCXMAlZJAGGh4epq6tjxYoVlJSUxCx/M0Kyu7ub9vZ2NmzYEFMoLJY4vBUiczFEWlJSEk6nk8nJSRwOB8XFxfh8Pvr7+2lsbCQ1NTXsBhu5pjAeboVr60IuqDdTb1paGllZWWRlZbFs2TKCwSCjo6M8+uijfPvb38bn89HR0cHb3/52Nm7cOGc9wH8DLwC6gFNKqV9rrS9GFPtLwKu1rlJKvR74DPC6iO8/Bzy46CcpCILwLEaEpCAIwh8QWmtGRkYYGBiIO6oqGGEYCATo6Oigu7t7TpEXWT5RIRkdsGeu6KKLKSSf6SilrgtWE1pT2NHRwejoKHa7HafTicvlmuX6GYtnomtrIvXabDZycnJ49atfTVdXF0VFRdTU1JCWljZnPSdPngRo0lq3ACilfgi8AmNhDPEK4D7r/z8FvqSUUlprrZR6JdAKjC3OmQmCIDw3ECEpCILwB0IoN+T09DSDg4OUlpYmtP+lS5fIzMxkx44dC7oyJiokg8EgJ0+epKSkhGXLls0rdhbTkvhMdnudS/RFrykcGxvD6/XS0NDA1NQUOTk5uFwuHA4Hqamp19V5u4PtxMtC6T/GxsZwOBzs379/3nq6u7sBOiM2dQE7o4qVhMporf1KqSEgTyk1CXwAY818X6LnIAiC8FxGhKQgCMJznOiAOsnJyQmJvKGhIdxuN2VlZVRWVsa1TyJC8urVq4yPj7Nz505yc3PjqjseATg0NERdXR02mw2Xy3Vd3sLFdm1dbFEaj/VQKXWd6+fw8DBer5euri6CwSAOhwOn04nD4Qivh11MbqeQjAzwdIu4D/i81nr02WDBFgRBeDoRISkIgvAcJlZuyMj1jgvt297eTl9fHwUFBTidzriPG4+QDAaDNDQ0hBPLxyMiIb5gO11dXXR2drJu3TqSkpLwer309PQwMjJCRkYGLpcLv98f9/k8W7DZbDgcDhwOB+Xl5fj9fnw+Hx6Ph5aWFqampkhKSkJrTXZ29qKIytspJOPJI2mt410WsakU6I4q1m2V6VJKJQO5mKA7O4HXKKX+HXAAQaXUpNb6S/GfiSAIwnMTEZKCIAjPUUIBdaJzQ8YjJKenp6mrqyMjI4MdO3bQ1NQUl/gMsZCQnJyc5Pz58xQUFLBy5UqOHz8ed93zWRKDwSAXL14Mr7MMBoMEg8Fw3kKtNePj43g8HoaGhhgeHg5bK51O55zrMm8Hi+GGmpycTH5+Pvn5+QCcP3+e9PR0uru7w6I6cn3ljQjLp3ONZCTxpv/Yvn07QLVSqhwjGF8PvCGq2K+BtwDHgdcAj2vTyfaFCiil7gNGRUQKgiAYnjlPTEEQBGFRWCg35EIiz+v1cvHiRaqrqyksLIxrn2jmKz84OMiVK1dYvXo1LpdrVrvjETJzCcmJiQlqa2tZsmQJy5cvj2m5VEpht9ux2+1MTEyQn5+PUgqPx0NHRwdATDfY28GtCIxjs9koKChg+fLl4ZyNHo+H5uZmJiYmyMrKCovq+QLYRBIMBm+JAF/IIjk+Ph6Xa6vVtr8FHsak//g/rXW9UurjwGmt9a+BbwDfVUo1AR6M2BQEQRDmQYSkIAjCc4h4ckPOJU601rS2tjIwMMCWLVtmRQCN1x02RCwhqbWmubkZr9fLtm3bZgmVkDi8USHpdru5fPkya9asidsFN3R9QsnuAWZmZmK6wTqdTjIzM+Oqd7G4FYGAIq18kTkbS0tLwxF9PR4PFy9exO/3z1pfOZdYfKa7tgJorX8H/C5q2z9H/H8SeO0CddwX18EEQRD+QBAhKQiC8BwgOjfkXCJyLqamprhw4QLZ2dls3749YStmNKF0ISGmp6epra0lJyeHrVu3Xld/IoFvIi2NWmva2toYGBhg69atpKenx9znt3VXqesZobF/jN0VLnZXOLHFOF5KSgqFhYUUFhbOstg1NTUxOTlJbm5uOCJqSkpKvJfjhllsi+R8Yl0pRU5ODjk5OZSVlREIBBgaGsLj8dDW1obNZguL7pycnPA9vF2uraOjo09HsB1BEARhDkRICoIgPMuJdmVNVHyErHk1NTXhXIXR3IiQDIlan89HfX19XPUvlFYkVDZ0znV1daSlpbFt27aYomMmEOTff9/ED57qpcyVQUqS4gtPtPKFJ1rJTbOxY/kEd62aZle5g4Ls2a6c0Ra7UERUj8dDe3t72JqZl5d3SwTN7c75mJSUFHbzBfMywOfz0dfXR0NDA2lpabhcrrDAXmwWOv/Jyck5XxwIgiAItx4RkoIgCM9igsEgXV1d5OfnY7PZEhIeIVdTj8czrzUPjKiYmpqKu+6QRbKtrY2rV69e5yobTaIWyYmJCRoaGigrK2Pp0qUxy7lHp3nPjy9wqt0HQJtnApc9heevzMOZkULXoI/TXaM80ngFgOpCO7vLneyucLJlWQ7pKbNFbWRE1IqKirAbbG9vL1euXGFqaore3l4KCgoWxQ32VgjJm6kzNTU1bK0FsybV6/UyNDTE0NBQOH+l0+lcFIEXT+qT27mGVRAE4Q8dEZKCIAjPQiJdWdvb23G5XHFZ80IEg0FOnTqF0+lk+/btC07aE7VIaq3p7e0lLy8vpqvszdQ/NjZGb28vW7ZsmdMSWNs9xF9//zzusWlevDqf12xZytXhSY61+jje4sUzbqylFa5U9lblkZJko8s7wfdPd/PtJ7tIS7axdXkuuyuc7C53UlWQed01inaDfeqppwDCbrCRwupG3GCfaUIymoyMDDIyMhgaGqKkpASlFF6vl8uXLzM9PX3TbsDzvVjQWt+SNaSCIAhC/IiQFARBeJYRnRsyFAgn3sn6wMAAExMT1NTUhK1LC5FIsJ2RkREaGxux2+2sWbMmrn3izTvZ2NjIyMgIVVVVc4rIH5zq4lMPNpCdnkRJbjoPXRrkoUuDFGSlsqvCyfvuqaAgK5XD9e3UDQZ46OIAMwFNerKNrctyKXGkM+0PUt87wmcfbQGg0Np3d4WTO8ocuOyps44Zug9Lly5lxYoVs9xgOzs7AcJpNiLXF87HrRBKt8pdNikpCbvdTnZ2NsuXLycYDIbXV4ai4TocDlwuF7m5uYtmSVzscxEEQRDiR4SkIAjCswitNVNTU7NyQyYlJeH3+xfcN1KI5ebmJrSuLV6LYVdXF52dnVRWVjI8PBx3/Qu5tk5PT3P+/HmcTiclJSUxra/T/iCfeqiBH5zqwqZg5RIHd1W5qMrPoMtnrJEHG938uvYqCqhwpXDHilz+al85M4EgT7b5ONri5USbD4Ci7FRetDqfnPRk3GMzHGh08ytr39VLsoy1ssLJptIcUpKuD04UcoOFa9FgQ+sL09PTw+sP58vfeCuE0tOx7jIyMA+Y8/f5fAwMDNDU1ERKSkpYWGdlZV3XptBLkrm4FYJYEARBSAwRkoIgCM8iQmIrcuKenJy8oLUwlGOxoKCArVu3cu7cuZtO5xFJIBDg4sWLaK3Zvn07o6Oj+Hy+Ral/aGiIurq6cLCe1tbW68r2j0zxtu+e5crVUf5ofRFZackca/HwqYebAChxpLO73MnHXlJNbkYyZ7uGefxiLz84N8j9ZwfJTE1ixwoHb9xWQlVBJu2eCY62eDnW6mNk0o8C1izJ4u7qTGzKRqt7nG+d6OLrxzrJSLGxfYWD5anTZBVPUFWUvKAbbHQ02FhusM8WsRRPAJ+UlBQKCgrCwZYmJyfxer10dHQwOjqK3W4PC8uMjIwFAy9NTEzMu+ZWEARBuPWIkBQEQXgWESsq60Jup/39/TQ2Ns7KsZiUlJTQmsf5jjE2NkZtbS2lpaWUlpaGg6AkGuU1lkWys7OTrq4uNm/eHA5gE229PNPh4+9+XIt7dBqAI00edle6eOf+cpY70rjYM8yxVh8P1Pfzk7O9JCnYWJrD5uJ0/nLHEmzpdo63GmvkgUY3AKWOdPZUOPmXl1aTnWaE57EWL7+50E9Agz01iTvKHCzJTWNyJkht9zCHvFN87+I5nBnJPG9lPrssN9jcjNkux/NFg410g52YmLgl0VAXmxtJ/5Genk5xcTHFxcVorRkbG8Pr9dLY2Mjk5CR2ux2/38/09DSpqanX7R8Sn4IgCMLtQ4SkIAjCs4hYFqq5RF4wGOTKlSuMj4+zffv2WRPy6DyPCzGXMOzr66OlpYV169aRk5OzYPm5iMwNCcbCeenSJYLBIDt27JhlnQoJSa01PzzdzccfuExORgr/9so1JNkUR5s9HGly87u6qwBUF2Syp9LFn2xZQpLNxql2Ixrvrx3l/tphHBnJ7Cp38o49y1iRl8mVq2Mca/Hw6wtX+dGZXpJtig0l2eytdPK3+1cwNOnneKuX4y1ejrR4ASM8Nxcm0TQEQ5N+HrrYz8/O9WFTsK44O7y+cv3S7LjdYAcHB2loaKCnpycuN9jbxc3mkVRKkZWVRVZWFsuWLSMYDDIwMMDo6Ch1dXUEg0EcDgdOpxOHw0FSUhJjY2NkZWUt4lkIgiAIiSJCUhAE4VlGtEUuOTn5ujWS4+Pj1NbWUlRUxKpVqxK2YkYTXT4kUicnJ9m+fft1gX5uxiI5MTHB+fPnKS4uZvny5de1XSnF1EyAj/zqEj8720NOejK+8Rk+8IuLLM1NZ29VHh+9dyUF2WmcajOi8v5T3XzrRBfpyTa2rcjl5euLeOuGTHzTSdS5Axxr8fLgxQHApAHZU+HkT7YUY7MpTrcPcbTFy5cOtgOEhefb9y6nLC+Ty32j/K6+n7Pdk+ZcgJpCO+V5xoLaNDDO14528NUjHWSlGRfa0PrKZc7r3TNDbrBDQ0Pk5eWRnp6+oBvs7eRmhWQ0NpuNzMxMsrOzWbNmDX6/H5/Ph8fjoaWlBbfbze9+97tw7tTk5PmnMg899BAveclLrgBJwNe11p+O/F4plQZ8B9gKuIHXaa3blFIvAD4NpALTwPu11o8v2okKgiA8yxEhKQiC8CwnWuT19fXR3NzM2rVrw1auhfZZiEhhGFpvWVhYGFOkRpePh5BF0u12c/ny5VluuNEMjgf42CNdNAxOUVVg5y/3LKeqIIuLvSMcaXbzQF0fP36qmySbYmNJDrvLHfzNvhWMTAc43uLlaIuXzzzSDEBRVjL7qvP58IuqyLOncq5rmGMtHr538nrh+cEXVtLjm+RYq3eW8CzISmVwbBpXuuLdd5fTMzTDsVYvvzh/FQ3kZiRzV3UeBVkpjE4FzPrMBuNCu8xp1m7uqnCyY4WD7PRrj2WtdVhULeQGm0g02FvBrYoEC+ZFSX5+Pvn5+QC43W5Onz7Nk08+ydatW3nrW9/K3//938esJxAI8K53vQvgJUAXcEop9Wut9cWIYn8JeLXWVUqp1wOfAV4HDAIv01r3KKXWAQ8DJYt6ooIgCM9iREgKgiA8ywmJwkAgEM7ht2PHjnmtVTdqkRwYGKChoWFeoQc3JiR7enqYmJhg27ZtpKWlxSz3ZKuHv/15K9MBzfYyBy0D43zol5cAE0l1X1U+b9ixDLTmyVYvT1y5ypcOGUuiMzOFXeUO3r5nOeV5GRy52MH5q9M8WD/AT8/2kaRgQ0kOeyqd/PW+FYxawvNYhPAszkljd4WTD72wEmdmKv91sI1zXcMowDOp+fQjrWxb7uAlawv5pxdU0jd0LXdlv7WGszwvg5evLyQ9JYmeocmwC21o7eaucmOtTI4RuXSxosE+0wkEAnMG28nLy+OOO+7A7/fz2c9+lomJiTnrOXnyJFVVVTQ3N7cAKKV+CLwCiBSSrwDus/7/U+BLSimltT4bUaYeyFBKpWmtp274xARBEJ5DiJAUBEF4lhHLtXVkZISTJ09SUlLCsmXLFhQQiQpJpRTj4+O0tbXNK/RCJCIk/X4//f392O12tm3bFtOqprXmOyc6+fTDDQQ1FNmTWb0km7ftWUFuRgonW70caXbzf8fa+d8jbWSmJrGxKI1dBUH+Yr2Lpv4R6t2aY81ufldvLIkVzlR2LLPzlj3l2BQm/Ufz9S6sb9uzjIq8TC5dHeVYi5eHLw3ws3N94bZtLMnmXftXcKmhiT4cnGgbupZ/MjuV3eVO3ndPOUXZ6VzoHbHqGGTKHyQlSbGpNIcyVwaBoOZK/xhfPtTOfx9qJytFsX3FFHfWFLK7wklxbvp11yUyGiwYl+ZnshtsvMwnJIHwGslQ4KK56O7uZtmyZZGbuoCdUcVKgE4ArbVfKTUE5GEskiH+GDgjIlIQBOEaIiQFQRCe5fh8Pnp7e9m2bdusgDfzkYiQDOVw1Fqzbdu2uKxc8QrJ0dFRamtryc7OZsmSJTFF5MR0gPf/rI5HLg+wq9zJ+oIUznSP8KPT3XznRCepyTZ2rHDw/JUF/NMLq+n2jvObU82c75/ieGcA6qcoc2Wwc0UOL16j0JNjnO8dp97j5ycXvPyw1htO//HqTUv4aHEW7Z5JjrZ4Zrmw1lhrJ/dXufifwx30j0yxJCeN2u4R3v6DOuwpsLtihrfeUUplvp3GgTGOtXh5rMHNL638k2uLs9ld4eTP7yi1clcOcazVy0/OGmGaZ0/hBavycdlT6Lzqoa53nCeaGgEoy8tgT4WT3eVOtq1wkJl6vdCKdoMdGRnB7XbT2dkZzkHq8/luqxtsPMQrJJ8OlFJrMe6uL3xaDigIgvAsQYSkIAjCsxS/38+lS5eYnJykqKgobhEJRkhOT08vWM7r9XLx4kVqampobGyM21Uy2moai1DE1/Xr1zM4OBhTeHZ5J/jbH57nUt8oACfbvExM2dm8JJ1/uncdQxMzHG5yc6TJzb89bARXXrpiV1kuH3tZNUXZqZxq83Cs1ccvLwzwI3+Q1CTFlmW53LE8wBvWagbHpqkfDFLf4wun/1juTGdPpYuP3VuNMyOFpzrN2slvP9lF0DqtDSXZ3Lu2kHXFWXT7JvnN6WbOdY/wyBVTR2V+JrsrnHz65SvJSE0KB+35+rEO/vco4cA7r99SzMoldtrcJnfliVYf3vEZAKry0tlb5SI12UanZ5Kfne3j/lM9JNsUm0qz2bHCwZ3VeaxakoUthhtsbm5uOIXI1NQUZ86c4erVq894N9hAIDCv0B0bG4sr/UdJSUl4LalFKdAdVawbWAZ0KaWSgVxM0B2UUqXAL4A/01o3J3IOgiAIz3VESAqCIDwLGRkZ4cKFCyxfvpysrCy6uroS2n+h9B9aa9ra2ujv72fLli1kZGTQ2NgYd/3ziZJgMEhjYyNjY2PhiK9ut/s64Xms2c27fliLP6j5pxdWsbY4hyPNbp64dJVvnvHyzTOnKMhOZV9VHu++uxJn8jSPn2uhfdrOE60j/PbyhbDg2lPp4p17VzAy5eeYFXDnRNs4AEty0thV5uLt1UlkBsep7Rnj0lCAn5/r5Qene0ixhGdWmnlkFueksXV5LrXdw3z690ZblDjSqcmGD95TRmFuBue6hjna4uVHT/Xw3ZPdpCXb2Lo8lxeszuf995RzdWSa41bQnlDgneXOdHZXuLjv3mpy05N56EwTV4Zs/OZCP/6gJiPFxtZluZQ605n0BzjY6OF0xzBfPtyBMzOFO8od7KlwsqvcSWH29a7HycnJpKamsnLlSsC4wXq9Xpqbm5mYmCA7O5u8vLxnhBtsMBicNxrr2NgYBQUFC9azffv20AuQcoxgfD3whqhivwbeAhwHXgM8rrXWSikH8ADwQa310Rs6EUEQhOcwIiQFQRCeZfT09IQtednZ2YyNjV2X/mMh5nNtnZmZoa6ujvT0dLZv376oLpBTU1PU1tbicrnYvHlzWHBGusJqrfn60XY+92gT2WnJjE8H+PffN5FnT2VvlYs3bCmkIGWakVQXhxvdPHppgJ+f7UVhgtXsr3bxzrsrmZ7RHG4a5HCTm/98og0wrqN7Kpy8Y+9y7P4RGj3T1Ls1j1xx84vaQDjgzo4KO2/YGGTAO8T5qzMc6xlidNoI3elAkCSb4m/vLGO5M4MLPSMcbfFyvMXNE50NJNsUG0tz2FNhBe2Z8nO8xcexVm947WRRdiq7K5y89+5ySh3p1PaYtZO/qu3jh08Zi2ONK4n9NYW853kVDE/6OW4F7Tna6g1fz4q8DHaUOfCOz/Bkm48HrfWfVQXGGrqnwsmWZbmkpyShtZ4l8ENusCUlJWE32FA0WK11OBpsbm7unH0gkYBKiRAIBOZdhzs+Ph6XRTI5OZkvfelLvPSlL30Yk/7j/7TW9UqpjwOntda/Br4BfFcp1QR4MGIT4G+BKuCflVL/bG17oda6/4ZPTBAE4TmEWsj1SBAEQbjlJDQQu91u0tPTwxabyclJ6uvr2bp1a9x1eDwerl69yurVq2dtHx4epq6ujoqKCpYsWTLru2PHjrF79+64jxFd3ufzUV9fT01NzXXWpJB4yStayod/dZGH6vspzk3nb+4sY8syB/W9IxxqdHOk2Y1vfAYFbCjNYU+FiyX4UDZFTzCXI80e6nqG0dpEad1T6WJXWS6rlmRxuW+MI80ejrV6GZrwo4Ca/DSet7qIneVOdFCHrZUXLVfa3IxkkpXCPT7DncvTKLP7ueyzUe8OMDodRAHrlmazp8KJc7qfFRUVnOoY4Vizl0tXTR0uy1q4t9JFRV4ml6+OWhZRHyOTfmwK1hVnWylAcvEHTeCfxy/20DYUmFXH+qU5/OhMD+3uCVYW2en2TTIyFcCmYO2SLFYWZZFkU7R5JjjTOcRMQJOapNi6PJedK3IoDLj5o31bFnRjnZmZCeduHBoaIi0tLewGm5mZGd4/EAhw7ty5hPpePDQ3N+NwOMjLy4v5/Yc//GFe9rKX8cIXxr1s8ZnjtysIgvAcQYSkIAjC7Sehgdjv98+yJvr9fs6cOcOOHTvirmNoaIjOzk7WrVtnGqA13d3ddHZ2smHDhpjWnhsVklprurq66OrqYuPGjTGjbHZ3d9PmHuOTh9y0Do6xpjibbt8E3nE/SsG6pTnsr8pjT6WLkZFhDly+Sr1bc6FnBA04Moxo3F+dx9ql2VzqG+Vwo1k76bHWG65ZksXeShe7yx3YbIrf13ZwunucKwNTaCAn3URp3VPpZM2SLI62ePjqkU7Gp69d69VFWexYnsVqJ4yNjnC+b5LLPhtN3hmCGrLTkrij3MneSierl2TTNDBmWSu94XasKrKzu8LFHWUOUpIVJ9t8HG3xUtczEq5jZ5mTsvRx9qxZTu+YEbiHmj0MTxjL89LcNF60uoCdZQ7Skm2cbPdxrMXHhZ5hgtqsv9y2PJfinDQm/EEudI/QPGhceQuyUtllBe25o9xBnj11wXsZcoP1eDxhN1iXy0V2djYNDQ1s2rQp7n4RDw0NDRQUFMyZYuY973kPb3vb2xLpj4smJJVSo1rrLOv/9wJfAF4A/DnwV8AAYAcuAP8vlLNSKXUAKAZC+UqatNavWax2CYIgPN2IkBQEQbj9JDQQBwKBWa6sWmuOHz+ekMgbHR2lubmZjRs3EggEqK+vRynFmjVr5oyWefz4cXbu3Bm3q+uxY8fYuXMnFy+alH3z1f2LEw18/NFOxmc0ZXmZvHRdEfuq87ApxZEmN4eb3JzvGiKoISc9iXV5Sax2au7ZXE3fpI1DjabMoJWrcW1xNvuq8thV4cCmg5xoG+JYi5fz3cNhsbZxSTrbSzPZu3oZLe5xjjR7OdrsZXDM1GFTkJps4z13lbGhJIcn23wcafZwvmuYgAZ7ahI7ynLZUpxOSdo0de0DNI2lUOfWeCaM+KwqyGRPhYs7yh1kpyVzqt2IxnNdw+F1jztWONhT6WLD0my6hiY51uzlaIuHqyOmHWV5GVTmZ3K0xUt6so2XrC2geWCcM52mjvRkG9tW5LKnwsWGkmz6hqc4ZuW/7B022SqWO9NZt8ROYHKEYFoOp9p9+CxRurooi12WG+ym0hxSk+e/v5FusG63m/HxcUpKShZ0g02Ey5cvs3Tp0jkDSL3tbW/jIx/5CBs2bIi3ykUXkkqp5wNfBV6ktW5WSt0HjGqtP2uVex3wn8B6rfWAJSTfp7U+vVhtEQRBuJ2IkBQEQbj93JSQhMSthRMTE1y+fJmamhpqa2tZtmwZpaWl8+7z5JNPsnXr1nmDoERy5MgRkpOTWbp06Zy5LYNBzecfa+ZrR9pY7kjhrlVLqO0eDotGR2YKeyvz2F+dx4alOVy6OsJvTrdwumucYWPgM6KxOo+9VXmkJdk41uLhUKObc11DBII6bCXcV+li/dJsWtzjHG32cqhxgMFxI/iqC+3srXCyu9zBkRYv33mym/QUG9P+IAENmalJ7CxzsNcSWx1ekx7kaPM1sVZsV+yvLmB9QRJMjXG+d4zLPhuX3X5mIgTf3koXm0pzuGoJvqMtXrp8kwCUOtJNio8KJ8NXO3HbHDxw0U3jgLEmJttg63ITVGfzshx84zMca/VxrMVLu8cYuopz0thd4WRXuYOludfWX55s8zHpD5JsU2woyaaqwI4CmgfHZwnbbcsd7LbaUJ43fzTX8fFxmpqaKC4uXtANNhHq6+spKyubcx3kG97wBr74xS9SXl4eb5WLKiSBe4FvAfdqrS9b2+8jQkha274DPKW1/k8RkoIgPNcQISkIgnD7edqF5PT0NKdOnUIpxbp16+JKHXL69GnWr18/bxCUEIODg5w5c4bt27fP6Z44Ounnn35Rz2OXTYCYnFTF3auL2F+dz7ql2dT3mHWRh5oG8YyZdZEVzmQ2FCSzZWk6q2uqOdLk4VDTIOc6Q9bKZPZU5rGvOo9Npblc7hvmcOMgR5u99FvWyppCO3sqnazODZKepGibSOVIs4enOoYIWHciz57CX+5axh3lTrp8kxxt9nCk2UP3kBGNZXkZ7K1wsbvCQX5WKqc7hnjwbDtXvEGmAzocpXXr0gzK7X46BoaoGwxw0Qs9I9fcU/dUuNhT4WSpI41zXSMca/HwZJuPiZkgNqAoJ5Xe4WnWLsniL3cv41z3MMdavDQNzHZT3VPhpMyVQV3vCMdbfDzZ5g2vnQyt4VxXmEZ3bx992sHxlmtrOB0ZyWxfnktBThpjUwHOdw3TZonSJTlp7C53sssSprkZs6O5jo6O0tHRwZo1a8LbJiYm8Hg817nBOp1OUlMXdqMFqK2tpaamhvT09Jjfv+IVr+CHP/whhYWFcdXH4grJGWAEuEtrXRux/T6uF5LvBVZqrf86hmvrI1rr9y9WuwRBEJ5uREgKgiDcfhIaiIPBIDMzM7O2JSIkg8Egly5doq+vj/3798ed6uHMmTOsXr2ajIyMOctorWlpacHtduP3+9m5c2dMd9bmgTH+4jtn6B+Z4p37ynClBoz7av8MvvEZbAo2luayr8qIwqmpKX5+7DKXfDauDE5GWCtd7KvOZ2NJDpevjnKo0URpHbDcQlctyWKPFejGnpbMiVYvR5o9YbfQjBTFrnIXm5fl8NDFAep7R1lVZGd4Yoae4WuupXsrXewpN6Lxqc5hjjR7ONU+xJQ/SFqyjW3Lc1mROsbL7liFdyLAsVYfR5s9tLqvWQl3leeyoSAFux7jfPcol3yKi4N+Jvw6bCXcW+li+4pcRif9fPJ3l+gauRYV1WVPYXe5sRRWFWRyuc9af9nqZXjSBA9aU5zF7gond5Q5UQpOthlrZV2vWX9pT1HsrsxjV7mTtcVZtLjHOdbi5XiLL+zSW1WQyYaSbDKSk+gdnuJUu4+RKWO5Xbskiz2VLnZXONlQks3E2Cg9PT2sWrVqzr4WcoP1eDxxR4M9e/Ys69atm7NvPv/5z+fgwYPz9sUoFlNIjgOPA81a6/dEbL+P64Xk3wM1EUJSLJKCIDxnECEpCIJw+3nahOTExAS1tbUUFhbS09PDnj174j7uuXPnqKqqIisrK+b3fr+fCxcukJGRQU1NDadPn2bz5s3XiYFHLvXzgV/UM+M31rtkm2JDsZ31+Um8atcqJmeMqDzU5A5HYM1Jhb2VLl6wbillOUkcvNBK25Sdw01u3GPTKAVri3O4szqPfVV5pCbbONrs4WDjIGc7fASiAuFsLs3lbHMPJzpGONU9yeCYuZ6FWam8eG0Beyqc5NtTOdk+xJFmD6c7ZovGfVUuti3PZXB0mqMtRpxGisY9lU72VrhY7krnbJexIp5o9TE2bVKMbCzNYXupnZpczaB3iNr+GS75oMVrrJXJNoU/qLm72sm79pdxpX+coy0ejrf68FpBe1YvyWJPhZM7yhykJtt40hKNtd2RaziNG+z6pdk09Hh54nIf9e7grPWXu8uNS68jM5WzXUMcbfFypmOIaSvi6+bSbHyTAa5cHWNpbhpXh6fC9W8usbPaAa+8YyXLXQuLOr/fHw7aM58b7FNPPcWmTZvmXFO7f/9+Tp8+Pef3MVhs19ZC4DHgN1rrT1nb7yO2a+tprfUXRUgKgvBcQ4SkIAjC7eemhWQ8gXAGBgZoaGhgzZo1OJ3OhN1hL1y4wIoVK2K6wY6OjlJbW0t5eTnFxcXA9a6wgaDmi08085VDbRTnpvGZV64hKcnGwQY3j1++StOgJcRy09hflc/+mjxyg6McvNhFpz+HE21D+CaMtbLKmcy9m1awt8oFGg43uznU6I5aW+lib6WLNUUZNA9OcqTZw+FmD/2WiCp3plKcncSZninSU2y8dF0hLYPjnO4waTMyUmxsX+EIWwn7hqc42uzlSIuHNks0ljjS2VvhZG+li+GeZiaySzje6guLxlBOyb0VTnaWO5mcCXC8xcuRFi+X+q6lB9lV7mBTUSpJM+P810kvvilIS4JJK2Ds6qIs9lSaKKvpKUk82erjaEt04B8jGjeUZNPtm+Joi4djLV56LHfc0txU1uUnc+/mMgpz0jjTOcTRZi9PdQwx6Q+SkqTYsiyXXeVOti3PZXhyhqPNXn514SqjlkUy357C9jIHeZmpDE/OcLLNS9+I6YuljnSztrLcyY4yB9npC6+ljXSDHR8fJycnB5fLRVtbGzt27JhzfeX+/fs5c+ZMIusvb0WwHRdwGPic1vobMYLt/DHw30iwHUEQnqOIkBQEQbj9JDQQa62Znp6ete3UqVNs2rQppiug1pqmpiaGhobYsGFDeJ1aokLy4sWLLF26FIfDMWt7X18fLS0trF+/nuzs7PD2SFfYoYkZ3vezOg41uslJT2Z40ljeyvIy2V+dx9al6SRP+PCm5HOocZCjzZ6w9W57mZO7avLZU5nH6JSfxy/18djFXlp8lrjJSmVfVV54beWF7mEON0VHcTWpP/ZUOMlISeJoi4cfn+4Kr1fMSLGxq9wIwm0rcuj0TnHEWhfZ6TWBcFa4Mthjicbi3DSe6hjiSLOXJ9u8TMwESVKwdbkJpnNHuZPRSRMI50izh8tXxwCz9nJPhZM9lS5WFtq52GdySh5r8YYtjTYFL1jpYkPmEPaMdGqvTnHJp7jiNilGQqJxb4WTjaXZdHmnrDqureFc7kxnd4VJdbIkN40zncMcuNzP2e5Rpiwr8OZlOeyucLJ9uYOxaT/HW01E2cZ+01ZXZjLJNhv9o9P86bZiVhVmcbzNx3ErDydAVV4aFY5kCly5dHknOdUxxLh139aX5BhrZ6WTtcXZJNvm13KRbrAdHR3Y7faYbrBaa/bv38/Zs2dvq5C0/r8MOAS8B9jC7PQfdcBH5kn/Mai1vmex2iUIgvB0I0JSEATh9nPTQnKu9YtTU1PU1tbidDqprKycNfFOVEhevnyZgoKCcJL4YDBIQ0MD4+PjrF+//joRe+7cOaqrq+ka1fz198/R7ZtkX5WLd+4vJ8+eGnY9fbLVa9xGk2B3VT57yx3kTPQSTM+lYSSZQ03usLgpcaSzp8LBipRRXnTHRp7q8HKo0c2RZjdDE36SbIpNpbnst1xcg8EgT1zp52iLj1or9UdOehLZ6Sl0+ybZUpzO63aWcardx5FmL31WBNaqgsyw8CzISuVEm4+jzV5OtZvIpyEX1z2VTnaWOfCOzfCz45dpGk0JR1gtzE5lb4WLPZVOqgrsXOgZ5mizl2OWEFOYQDh7K51kpibxpYPtpCYpSp3pNFwdI2Dlg9xZ5mBLcTql6dM09g1R7w5S79H0jxkhXebKCEdZLcxO5UynOc7pDhO0J9mm2LIsh01L0qjKDuIsLOZYq5djzV6uhERjxPrLmkI7ZzqH+O9D7WHBCNfShOwqc5CWYnJXHrzSz8WrEwQ0VsTXXFa4MpgJaOp7R6jvHUUD2enJ3FHmCOfpXJobO4hOiJMnT7Jly5aYbrBDQ0O8853v5Pz58wv22Yceeoj3vOc9NDQ0NANf11p/OvJ7pVQa8B1gK+AGXqe1brO++xDwl0AA+Dut9cMLHlAQBOEPCBGSgiAIt5+bFpLnz5+nsrJy1vpFr9fLxYsXWblyJfn5+dfVk6iQbGhowOl0UlBQEBaoLpeLioqKmJah2tpaLk9k8alH2khPSaIoO43GgTECQY0jI4W9VS72V+ezfYWD820DPHi+kwvuYNgVs6rAzv7qfO6syaM4J53jLUZ4Hm/xMDETJDXZ5GDcX53P3qo8hiZmTJTXxkHqe0cAE9X0jrJc7qrJZ82SLE60efmvA+14xq+5Bq9ZkmW5wTqwpyVzvMXH4WYPZzqvubjuLDNrK7evcNA7NMkRa11k2MU1N42VOQFevqOKyoIsznYOc7jZw4lWEz01ScGGkhz2VBrXz4DWxgLYbNY0aowlcn+Vi+evzGdjSTa/f7KOHu3gWKsvLHAr8zO5Y0UOa/NsJM2McbZ7lEtWipGpgA67p4baOjwxE7Y0NvTPtorurnBRU5jJpb6x69ZfpibZCASDvH3vcvZUODnZbtZOno9IE7J9hYP1+UnUuJIh02lyV7Z6wxbcktw0tizPxZmZgmdshlPtvmtrM10Z4Wiz21c4yEydvdbx5MmT7NixY9a2iYkJ+vv7+bu/+zvOnj3Lq171Kl7xilfwile8ImZ/DQQC1NTU8Mgjj1BZWZkGnAL+NGQhBFBK/Q2wQWv9TqXU64FXaa1fp5RaA/wA2AEsBR7FBM0JzPMTEQRB+INChKQgCMLtJ+GBeGpqatbnuro6li1bRm5uLlpr2tra6O/vZ+PGjXOmUDh27Bi7du2K2z2wubkZu91Oeno69fX1cwpUAH8gyId+eIJfN4yTmqR4ydoiXryuiLXF2TzVMcShxkEONV4LlLOuOIuqzCk25NtYv3YVT3WNc6DRzel2LzMBjT0tiT0VRnhuKbVz+MwlelUehxoHaRk0FsDlrgz2V+WxvyafyvxMjjT088CZVuoGA4z7jW9jcpIiqOHdd65gZa7maKuPerfmfNhamczuCiMat5Tm0jw4HnZxDbmNlltRXPdWuijOSeOkZc083uJmKmAC5RgXVxM9dXzaz5EWL0ebvVyMWhc5OhXgYJOH1UV2KvMzebJ9iAHLHXdZto3nrylmb6ULR2YKT7YZQXi63cd0wOSl3Lo8l20lmVRkBegc8HFh0M8lL3QMGUtiYVYquy1X2rykKc53+WgYTuJ4qxefZRVdvcSsv9xV7mRwdJqPP9jI+LTRS0Erh+aOFbnsrnCxuTSH3uFr6y8jRaMRhi5KHOnUWmlKnmwza0VtCtYVZ7NmSRZJSYo290R4bWaSguetzOdzf3wthcipU6fYvn17zL41MjLCa1/7Wj73uc/R0dHBa1/72pjljh8/zn333cfDDz8MoCwLI1rrfwuVUUo9DNyntT6ulEoG+oAC4IORZSPLzfX7EARB+ENDhKQgCMLt56aFZMjtNCcnJxw5deXKlfMG3zlx4gTbt2+PO/JlS0sLY2NjjI2NsXHjxjlTL3jGpnnXD85zpnOIPWW5pKencLzFy/h0wFgRy5zh6KqjUwEONAzwcG0XTZ4ZNMaKuK86jzur89lUmktd7zCHGtwcbBwMW+ZW5Nh4ycbl7K/JJy8zhaMtHg41ujnR6mFyJkhakqLGqXjBuqXcWVPIo5cH+J8jnWhNOFekK8PGliXp3LtlBWuWZHOhZ4QjzR6ONnvDqTBWL8lib6WTvRVOsjPMeUSKuciAPFljXeSXVnKszUR6DeV6LMpODQvP6sJMartHONjo5vEGNzNWY9YuyWJflUmrkZ5ihN6DZ9tpGtL4g9oScw72VjrZujyHvuFp09YWb9gqujQ3jV3lDjYUJJPDOOe7R7jkU9QPBhidDmJTUJ2XyvNWL2FXuRObUpxo84atoqHrkpqkeMvOUl60toBu7yTHrHPu8hnRWOpItyyaTpKnhrg0OE29O8jJtmtRadeXmPWXO8scBIOaE6E0JD0jYVfXHctzybOn8MgVN/5AkCfeu4u0ZBvBYJAzZ86wbdu2mP2rt7eXd7/73SGBOCc//elPeeihh/j6178ORki+Gdiptf7bUBmlVB3wYq11l/W5GdgJ3Aec0Fp/z9r+DeBBrfVP5z2oIAjCHxAiJAVBEG4/CQ/E09PTRI7fjY2NpKSk0NPTQ0VFBUuWLFmwjlOnTrFx48a4ksQHAgFOnjyJUmpe8VnfM8zf/rCW3qFJUpNgX6WDF6wr4Y4yJ63ucQ40DHKwcZBWy4pY5spgVW6QXSuycSZNMZm7nIMNg+E1j8k2xZblueyvzmd/VR4aONgwwO/OttPoCxIIanLSk9lblced1XnsKHNx8nI7j17s5fJwcthiBmYd4D/dU8HGkhwON1zl8Yt9nL86zUQA43q61M6dNQXsrnCiNeG0HqHIqLOslctyaR0c50iLl8NNnrDIWuFKZ19lHnsqnZQ60jndYSKjnmjzMjploriuLc6if2Sa3uEp/nTbUhwZyRxr8XGhZ7ZVdKny8ap9G2l1T3A0yipa5spgT6WxAC51mMA/R5uvWQCTbYqNJdnsWJbFylyN2zfEmd4JLnkVrUNBNOY4u8qNe2lAa/7t4SZSkmzYU5PoH73mgrrbckEtyjFBe461eHiyzRcOMLSmKIO7VxWxo8zBjD/IsVaTl7K+dyR8nDvKTUTZdcVZtLgnOd7i5VCTm8GxGVKTFF94zVr2VbkAmJmZoa6ujs2bN8fsY01NTXziE5/gZz/72bx9VoSkIAjCrUWEpCAIwu3npoSk1pqzZ88yOjrK1q1bsdvtcdUxV4CeaMbHxzl//jx2u52cnBzKyspilvv52R7++TeXcGWm8qadpZxv6eNM3xSecT9KwcYSEwTnrpp8stKS+H1dDw/XdnLFq5kOaNKSYG91AXdV57O70sXVkSkOWcIzlCpjaW46+6pcFGsPr757G2c7hzhouckOjk6jgApHEi/aUMruyny+eaydx64MkmdPYXhihpkg1ppHk25jZa4mJSefxy72crxtiLYhKxJsZjL7rDWca4uzqO0Z4UiThyMxrJX7Kl3kZKTwk4O1tE1ncrpjmCl/kPRkG9tWmCiuu8oceCZm+F1dP7+svRq2RObbU03OyUoXa5fYqe8b40izcRsNubiuLLSzp9LFngoHLnsqJ6xIsKHclqlJim0rHOGckkMTJlrs0WYvl65arrT2FDYXpbEu30aVK5X6niHqPVDvDuKZMOecmqR45YYlPH9VHnn2VE61D3G0xcPp9tnpQcy6xlzGpwP87mwb569O0+Q2AteZmRIWjWuXZNE4MM7RFi/HW7xhcVqRn8mGkmyebPXhm5jhv1+3ju0rHOF+NDU1xeXLl9m4cWPMfnb+/Hm+9rWv8e1vf3vefiuurYIgCLcWEZKCIAi3nxsWkn6/n4sXLzI+Ps7SpUtZvnx53HXECtATTSj35Nq1axkfH2dqaory8vJZZWYCQT7zcCPffbITgLRkxa6KPNY4gtxZk09SpoODDYMcaBykrmcYrSEvM5k1Ts3Lt1Wyq6aIcx1efn78CpeHkugZMta9VUuyuLM6nztr8inOSQtHeT3a7GF8OkBKkmJnmZP91fnsLs/lfP1lLg8lUefRnOscCl/UmoJM/mL3MrYuy6VhYJxDTW6ONHvptqyIFfmZ7Kt0sbfSyTJHGgevXOVgwyBneyeY8IeslVncWZPP3koXQa050jzbWpmdnsxqh+alW8rZWe6i1R1aW+ml3WNcTwuzU8NW1vteWs3kTDCc+mN40o8tFJCnwrjSXr58kaGMEo40ezhnBbmxp5oornusYDo9vklr/aWHVsvFtTgnzYjTChdVhZmc7xrhSIuHo01uhqeCKGBNcRY7V+TgHR7jlxeHyMuAwsxkGr0BZoJ6lgjevjyXwbFpjrWY3JUhl92CLJOXcm+Vi20VRdT3jYYD7njGTNCemkI7eyqc7Cp3kJuRwqmOIR67PMjZrmGy0pL46p+uZ0PJ7Lyk4+PjtLS0sG7duph98ujRo/zqV7/if/7nf+bt336/n5qaGh577DEqKipCwXbeoLWuD5VRSr0Lk+cxFGzn1VrrP1FKrQW+z7VgO48B1RJsRxAE4RoiJAVBEG4/CQ/EMzMzDA8PU1tby/Lly1FKMTMzM6e1MBaRAXqua5DWtLS04PF42LBhA2lpaVy9epWRkRGqqqrC5QZHp/ir753jYu8Ir9u6lLtXFnCkyc2BRjddXiNsagrt3FmTz101+ZQ60vnF8UucaB+h3h1kZMoIwq3LHZSnjfHme7YQCGoONZo1kU91DF0X5fWOMge/OXQad2oRBxquBdspyU3leauKKM/P5CuH2vCOT7OhNIeW/jG8E9eEWkg0To4Nc7BhkEs+xekOE6E1MzWJO8rMmsdd5bl0eSd47GIvx1qH6Bg2GqLAnszeShd3VuezJsJaeeBKP74pcytXF1lrK6tcODNT+NbxLn5Z24fW5manJim2r3CEg9yMTl4LyBNyCc1OgTtXFrKn0snGpdk0DIxz1BKwvdZa0Yr8TCu3pUmpEbIinmiNcHEtzWFvhZMK+zRaKRqHkzna7OFct4lsm2yDfZUutpdmsCJjhsa+IeoGA1z0Es6zWZKbxm4rlUl5Xgbnu0c42uzlaPMgo9MaBawtzrbSkDhIT0niRKuPYy0eznRei/S6uiiLZvc4NqX42hvWs7Lo+pcYIyMjdHV1sXr16pj99ve//z0nTpzgP/7jPxbs47/73e9473vfS2NjYwvwf1rrTyqlPg6c1lr/WimVDnwX2Ax4gNdrrVsAlFIfAf4C8APv1Vo/uOABBUEQ/oAQISkIgnD7SXggbm9vp6WlhfXr15OdnU1fXx9jY2NUVlbGXcfFixcpLi7G6XTO2j4zM8OFCxew2+1UV1eHA/YMDAzg9XqpqakB4HzXEO/+US2Do9MEgib1xLYVTu6qyefO6jw6u7o53TPJ2asznOnwGYtaimLr0gz+aEsZuypctHkmONgwyMGGQRoHTHqKZc4MU0dNPquK7DzVMcTBhkGzpm7URHmtyLHx0i1lbC5Koa+zFU/aEk52jnG02YM/aC7nthUOXrquiB3Ls/FN+Dnc5OFws4f63lDk1GQ2F6Vy7+YVbCzJ4WLfaDhCaygFSXWhPey+WpyTysHLVznUOMiZngkmrbWVG0uyuLOmAOdEDzWrVnG8dYjDEdbKtGQbU/4gy5zp/McrVzE06Q9bNENWxBJHevg4VQV2znYN8auTTVwZsuEdnwkLtb2VJshNVloyx1u910VxDYnTO8oceMZnTKCcCBdXZ0YSeytd9I/O8GSbjx0rcilxpHOsxRtOzVGZn8kdZbmsz7eRMjPGme5RrvgU9YN+Jvw6LE73VDhZahvC4cyjdmCGo83e8DrP7LQkdpaZnJFbSnPo8E7y6wtXeezKIM7MFL755o2U52XG7Jc+n4/+/v5wP4vml7/8Ja2trfzzP/9z3H0dE7RXEARBWERESAqCINx+Eh6IOzo6yMvLIzk5GTAiz+PxsHLlyrjruHLlCnl5ebNSeIyMjHDhwoWYAXvcbjf9/f2sXr2aH5/u5r7fXiY3I5nPv2YdtiQbBxoGOdAwSLMlCEtzU9i5LIuXbS2nNAt+fqSOlslMTnWN4x6bxqZgY6nJ8XhXTT5X6s4xmlPGoUY3x1s8TPmDZKTY2FVhrH/7q/Nwj81wsHGQB860hYPG5NtT2Fedx3RA88CFq5TnZbJ5WS4n231hq2hVgXFf3VflYpkznZNtQzx2qY8n24cZmzHiaFNpDvuqXOytcGJTcLTFx6Emd9iilpWWxB1lTvZVGaHW7h6z1lYOz7JWhtZWriq0868PNXG0xUtqkmLaWhe5qshurKJVLvLsqZxoNaIyFMDGWGhzWZEyyuvv2sjEdMAK/OO9LiDP3konW5fl0mIF5Dnacs2VttSRzh5LnFbkZfLIuRbOXJ3iWNsYM5bYXldscmjurnCSmWqsiEdaPDxlWWhDLq7bSjKpyg7SNeCldsDPZZ+ixWvcV50Zyeyx6li/NJuG/jGOWYGKQuJ0SU4qvnE/+VmpfO2N6yl1zL0u1+124/V6Z1m+I7n//vsZHR3lfe97X9x9HRGSgiAIi44ISUEQhNtPwgOx3+8nELi2XMvr9dLb28uaNWvm2Ws2TU1NZGdnU1RUBJi0Cq2trWzYsCHmukmfz0dbRxc/b0viR091hy1tSTbjmhoShKlJioONbh6q7eJc77gRJEmwu8LJPWuL2Vflom94yqybbHBT1zMMgDNd8cK1S7mzJp/Ny3K50D0cjvIaWs+4siiLO6vzyBppZ2VJHp6UQp5ocPP4lQH8QeNiuW2Fg7tXFnBndR5aBzlwZYDDzV6e6hgKC8Jd5U62FqdRaZ8h1VXMoSYPR5o8XOk3IrgoO5V9VS72VbpYX5JNbfcIh5s8s8TRykK7EZ6VLoqyU/nBE2dpGEnmXO8kkwGjXDSwdXkuH35hBQENR5u9s6yV2enJ7Co3rrQ7lufS6ZsMWyubLZfdpblp7Kkw7ririuzU9oxwuNnLscjAP0VZ4aA9zsxky63Uy8n2a+J0pSsFz5SmZ3iGN21fSk5GyiwrYjiKa6WTLctyaPdMctQK/NNmidMSK8XIxsIUHGqCU61uGkeSqfcEGZ4KmrYsyWJvhZNdFQ5y0lP44VM9/PxcHyWOdL75po0U5aTN2y8HBgYYHR29bi1uiK9+9atkZWXxzne+M86eDoiQFARBWHRESAqCINx+blpIDg8P097ezvr16+Ouo7W1lbS0NJYsWcKVK1eYnJxk/fr1YStnNM09bv72Rxdo8fl53sp8/vVlq+nwToQtkVcs98nlrgzurM5nfR7k+oe44g3Q4c/mcLOXq9bavvUlOdxlBdIpzE7lSLOHnx+/wiUfjE1dC6Rzp+UmOxPQ1nEGONMxREBDbnoy28scXLk6Rqd3gtduWUpuRjKHmjw0WG0pcaSzuyyX/dUu1hRnc6F7lMPNHg43ecJRRNdYORz3VbkoyDIWwsNNXo63ehmbDoQthKG1lTMBzVEr7cfZzqGwIFzl0LxsawU1hXY+8MvLtHsncaSp8LrJwqxk9lXmcWdNPmuWZHG+ezgckCcUoXVVkT2cc7Kv9TJj2cus9CE+xq01j1uW5YStiIGg5lhLlDhNS+KOcmOt3L7CQZdvkkcvD/KLc73MGK3Hkpw0a22li9VFdi70jl4XLbam0B4O2lOQncrJNh9HW0yKkVBbqp029lXls64gCa93iHN9U1weUlxxzxC03Hr9QU11QSZf+dP15NkXTjXT19fH9PT0nIGjPve5z1FVVcUb3/jGOHp5GBGSgiAIi4wISUEQhNvPTQvJsbExGhsb2bRpU9x1dHR0EAgEGBgYID8/n/LycpSKPd8+3e7l735Uy/DEDKnJSSbxvM0IrLtqCrirJp/0ZBsHrSA5IddUk9Ijn7trCthfnYdnbCZsZTzXNYTWJvrn/up8luhB3vyiO7jcNxoWp21uY5Urz89kd1kuS/Gwb30FD51p5tJEDgcbB7G8NNm87Fp6EUdGCgcaBjjY4OZEm5fJmSBpyYpty3PZZ1nuBrzDPFrfy6UhG+e6jFXOYblp7qt0sWNFLi3ucY40G9EYshCWOtLD1srVS+yc7RrmcJOHJy5fC7ajFLxwVT5v2lFKXmYyT1zq41DTIOcta2WyDTaVZHFXTQF7K13MBIIcsdxXz1niNCMZ9lblm7aU5dLtm+JIs1nnGYqcWpSdGhae65ZmcaFnJGz17Lcsp2V5GbjHZpic9vPefcXY7VkcichtmaQwAXkqXeypcGKzKWtt5bVAOZmpSexY4WBvpZMdZbkMjs5wtMXLo3XdtA8bdZpvT2VXuYNNS1JZmjbFzy54eKwjQIUrlS+/dhXFeblz9q9Iuru7ASgpKYn5/Sc+8Ql2797NK17xini6eQgRkoIgCIuMCElBEITbz00LyampKS5cuMC2bdvirqOhoYHu7m7Wr18/a53krIZpzfdPdvGJB6+QkZLEq6pTefcf7aDVPcaBK4M8EWWJvKsmn13Lswm4W7k6k8lTfVOcvTpDrxW8Zu3SbO6qzueulfkszUnnSIuHgw2DHGlyMzzpJyVJsX2FMxzlVQEHGgZ5tL6HM12j+DXY05IoStd0jmqcmam8/4VVtLvHZ7nJFloC687qPLYud3DJEqeHGt10Wm6yyx2pbCxI4mVbK6gsyORM14hxcW324h2fwaZg/dIc9lvWyuy0JI62eDnU5OFkm49Jf5C0ZBs7VjjYV+VioLud+xsCoGGZK4PG/jGCGnIzzHrGkDhtvDrCYxd7OdE+QteIuYdFWcnsq8oLR4I93z3ML040cGUoKWw5XVloZ68lYIty0jjRagLphCynoXWee62ckzal+GXtVX74VE84b2VGis1YKyuc3FHuYGB0hiPNnlkBefLsKeypcLKn0sWmkhwuXx0Nr3nstu5jmSuD3RVOivGyf/NKLvSNh1OZDE2YSK8K2LIsm4/eWcDEiI+xsTFycnJwuVy4XC5SUlJi9rmOjg5SUlIoLi6O+f2HPvQhXvnKV3LPPffE3dcRISkIgrDoiJAUBEG4/SQ8EAcCAfx+f/iz3+/nqaeeYufOnQsfTGs6Oztpb2+noKCAVatWxSw3ORPgX357mZ+f66XMlYF7bJqRKSNYtpeZ6Kx31+STYgXaOdg4yLFmN9MBTWaKje3Ls9mYb+NP9q3FM+7nwJUBnmgY5HzXEEEN+Vmp7K/O4+6aAraXOfjF408ymLqEgw2DNFkBe1a4MtiyJJWV2X7+aM8GzvWM8j8H26jvHQm3c93SHO6qMS6jRVlpHG6y2tJiLG7JNsW2FQ7urM7jzuo8AB6u7eKR+l4afDoiP2Mu+ytd7K50hgXW4SYvddaxCrJSTWTVKhdbluVyue+am2yH14jTFJvij9YX8pK1hVQX2Dnd4eOwtebRM2air65bmh12k3VmJvPYxT4ONw5yrm+SKctauaU0m/L0Cd5w90ZmAtfyVobySYbcV/dVuthZlkv30JQJyBOxztORkczYVIDM1CT+49WraWnv5JLPxqmusXBU2vK8DMui6WSFK5OnOoaMi2urEYQKWL80mz2VTvZUOMlOS+Z4m48jzR5OtQ8x5Q+SmmSur8kX6eRn53q5/1QPO1c4+NLr1pKekhTud8PDw3g8HjweD0BYVObk5IStla2trdjtdgoLC2P2y3e/+928853v5I477liwr0cgQlIQBGGRESEpCIJw+7lpIam15vjx4+zevXvB/err67HZbOTn5zMyMkJ1dfV15bp9E7z7h7XU946wpjibv7mznK3Lcvjt4afoTy7kiSvXxF55fiZ3VeezKsdPrh5mJnc5R1qHeOJyP/2jJrLnhpIcIzxX5rMkO43DzR4ONAxyuMnNiGWJrHYoXrm9irtX5qOU4onL/Txwtp36gWlmgsaalpOewtWRKbYV2fjAy7dytNkzy002z57CvkoX+6vzuKPcSUP/GAcb3RxqctNsuYOW5KSy2hHglTuq2FJWwOl2Y2U81OQOu4OuKrKzt8KIz6WODE60+jjUZATWyKQ/Yq2ik77hab5/uofCTEVZfjbnukeYDuhr1j9LhPnG/UZ4Nnu40G1yRboyU8JBcrYty6Xh6nA4Emz3qHEZLc5OYV9VHvur81gdWlvZZIRltLVyb6WTpTlpfPtkNz883QNAUJs0JdWuFPZX5/P8NUtIS7ZxtCV2+pC9Vm7L4Sm/iQTb7OVCj2lvbkYyu62APNuWO/j98XMMJBdwtMUbzucJ8IJV+XzmlatISbLN2RdnZmbConJ4eBi73Y7L5WJkZIT8/Hzy8vJi7vcXf/EXfOxjH2PdunXz9vUoREgKgiAsMiIkBUEQbj83LSQBjh07Nq+QHB8f5/z585SWlrJs2TLcbjcDAwPXWSSPNrv5x5/WMe0PsnJJFleujjI2FSAt2cZKp+JVO6q5e2U+/kCQAw2DPHFlgCfbvPiDJvLnvqo87lqZz4b8ZOpau+n053KgYZDz3UbsFWanhSO8blvh4MpV43b64LkOesbMpShzZbA618/zVhVx58ZKHr00wH880oh3fCbczkhL5NKcNA43DnKw0c3RFi/Dltjbutysm7yzOo9kBb861cyx1iEue4NWepEkdlU4ubM6j32VLnwTfg42GhfY893X1k3uLjfuqzvLHHR4pzjc7OFQo5tGS5xmpNjYUaT44ztWsqEkh7rekXDeypD1r6ogM7y2coUrg1PtJt/k0WYPvgl/2JV2X5WxNLZcqWcwuYBDTW7O900yHYAUy1q5v6aAfZVOpmNYK9OSFdN+TXFuGp//4zVM+oMcafLwaH0vrUOmz+TZU6y1oE42l+ZwpX/ccnG9Zl1d7kxnj7X+srowk3OdVnCgFi+eMXMflmcrXri+lD0VTn5z4So/P3+Ve9cW8KmXryLJFr9201ozNjaGx+Ohs7OTpKQk8vPzcblcOByOcC5TgNe//vV8+ctfZsWKFXHXjwhJQRCERUeEpCAIwu0n4YE4GAwyMzMza9t8QnJgYICGhgbWrVtHbm4uYNJ5dHd3s3btWtMIrfnW8Q7+/feN2JTij9YX8cpNS9lYmsP5rmEevzLAg+e7GJiw8iEuyWJfhYMlwUG21SyjczqDJ66YIDmhPJE1rhReunkFz1tpAuAcanJzoGGQI81uxqYCpCbb2Fnm5O6afLJH29m8eQsP1XbxYG0XDV7NTNBY9oLaXKRPvGwVNUXZfOfRM7ROZc6yRO6vyuPOmjx2lDlp6h/jUKObg01uGkMpPexJbClO5ZXbq1hfmsP5zmEONXk42OimZ2h2epE7q/NY4crgWIsRjEcixN6GpdlsX5HLk21D1PaMsK/SRZINE2AoAKlJiu3Wusm9FU4CmrCojE5Bsq/Kxa4yB/2j01Z6kWuutDmpcPeqIvZWuti6LIeLPUM8fqmPE+3D9ERYK/dX57G/Op/VS+x8/rFWflPXT0qSCq+LrCk0kWCXJQ+zuWoZdYPTHGk26xmHJ61zKjHW1b2VLuypSdaaSC+n2s1a0Ej31d0VTqb9QY61eHnwfAdN3iBWMFhev3UpH3pRJbY4gurMxaVLlyguLsbv9+PxePD5fKSlpZGXl0dSUhJvf/vb+elPfzrnut5oPB4PeXl5jwJlQBvwJ1prb3Q5pdRbgP9nffyE1vrbSqlM4CdAJRAAfqO1/uANn5wgCMJzCBGSgiAIt59bJiS11jQ3N+P1etm4cSOpqdfSL4yMjITzRo5PB/jQL+t5qL6fLctzSU9JMi6P/iBZaUnsqzJuqemeZqrWbeaJKwP8vr6X2p4xNGa9453Vpswd5U5aBsf5fV0Pj128GraClTozuNuyRG5elktt9zBPWOsmO6w8heWuNFbnBnjNrpWsX5HPlw+08u0THeactYmGurEkl/L0cf7s+ZvJtycbkdbk5kizl5Ep/3VrIpUO8MNDdVzy2TjfN2lZIm3cUW6C8eyrcjI6FeRQ4yCHmjxWehFNbkYyeytNHbvKnXT5JjjYMMijVwZoGjDtzU5L4p6VeeyrcmHztJJZXM1hK8prKPdimSsjHCRnVZGds53DYRfXkCvt6qIs9lY52V/loiQ3nRNtPn55sonLPsXwpD8cWXWfZSHMTFE8dqmPQ41uaq8aa6UNCGLccj/18pVolJVexMNZK/qqPdUI2JD7av/odLhMfa8JtuOygu3srTRrQVsGx8JWz1a3OaeluWnsrXRREBikYSqXRy4P8mc7S3jf8yviisw6H3V1dZSXl2O328PbJiYmcLvdfOxjH+PAgQO8+tWv5t577+WlL30pSUlJ89b3T//0T/zHf/zHh7TWn1ZKfRBwaq0/EFlGKeUCTgPbML/Hp4CtwBSwU2v9hFIqFXgM+JTW+sGbOklBEITnACIkBUEQbj+LJiR37doVnsjPzMxQW1tLVlYW1dXVs9wDwbi6XrlyhfwVK3nXD2u5cnWUguxUXrt5Kc9bVUh5XiYn2jwcsKyMA6PTJhLn8lw2FyZTmTHB7i3rONk1xhNXZq933FnuYk9ZDiXKy8b16zjYOMgTVwY5ZqUFyUxNYk+li7usPJHDk37+78GTXPTZaPAGLBdNG1P+IBX5mXz5TzcwPh3kiYZBDjYMUtttorMWZadarqv5bFuey5WroyYFScSayKJMxb5KJy/euIx1S7M52zlsrJVzWCLL8jJ4ss3HoUY3h5vcuMdMBNeNJTmsXpLFQxcHmPQHee3mYnqGJjnWYqKmJinYtjyXvZVO9lflkZJs41CTCcZzylqHGFo3aQShg+HJQDhgTygPZCjKa6ny8frnbaHbN8lhK9/kpT4j9iID/6wvzub//eYyT7YPY08By+uUpTmhtZXGWvnro3W0TmZwvH04LGCrC+2mnkoXy53pnGwfCueT9EUE2wkF5HFkpnC8xcuRFi8nWr1MWIkp//bOFbx9z/KbFpEAtbW11NTUkJ6eHvP7vXv38oUvfIHDhw/z0Y9+dMFjrly5koaGhqVa616lVDFwQGu9MrKMUupPgbu01u+wPn/VKveDqHL/CdRprb9242coCILw3ECEpCAIwu0n4YFYa8309PSsbSdOnGD79u0kJSUxMjLChQsXqKioYMmSJTHrmJqa4juPnuF/a6dQwPNXFdDmGedcp4mqWpCdyt01Bdy9Mp+dZU6aB8f4zqNnqfNAq89YGZc5M7h7pYneurE0l7qeYZ6w0oKEckDWFNq5y6qnpiiL0+0+Hr8ywMGGQfqGrfWDzmTWOIK8+fmbKMzO4N0/vkBt9zCpSYrpgJ6ds7I6j8aLtbRP2zk/EOB42xDj0wFSk2zsKHOYdZPVefh8Q/ziyUZaJjM50zXKdCCIPTWJ3RUmGM++KidDEwEOWWsrz3YOE9AaR0YKe6uMtXJXhZMu7ySHGt08UHeVdsvK6MpM4fmr8k0AnLwUHj5ZT6c/h5OdY7RYVrtljnT2VDjYX+Vi3dJsLvSMGmEZsW6yutDOvkoX+6qclOdlcqp97iiv+6pcFGSnhlNxHG/xMjIVQFkdaFe5g/c9v4K0ZMVjF3s53OSmtm+K6aBxt13ptPGi9SXctbKQqYCOaa28o9zB3koXu8qduMdC1kovdVawHWdmCrsrnGxfnsvv6vs52T7EPz6/nLfesSzRLjwnZ8+eZd26dXOmB9m3bx9nz56NW7Q6HA58Pp8CUGYnr9baEVlGKfU+IF1r/Qnr80eBCa31ZyPKOIAzwD1a65aET0wQBOE5hghJQRCE28+iCMlTp06xceNGBgcHaWtrY8OGDWRlZc25/1cOtvCFJ1oB2LIsl5esK+LulQXYU5M41DjI41eurWVMS7axsyyXpXh4xbZKiouLOdDo5okrAxxv9YZdYPdW5XH3ygLurM5jYGicHxyqo3kig6faffiDGkdmCndWmzJ7Kly0XB3ix0cucnHIxuX+STSQbFMEgpo/3rKUD76wmqbB63NWljrS2FqcxqocP2VZQQZ1NnUeON4xGnaTLclS3LOmmOevLqKmyM5T7UMcbHJzsMHN1REj5NYtzTZrK6vzKHWmc6LVy8FGN4ebPOFckptKc3HZU3j8yiAV+Zm8elMx57qGOdbiMelFFGxZlsPzVxeyryqPZJuyxKmHk+0+pvxB0lNs7LCslfsqnUwFCFsiz3ReS+mxu8KIyl3lTg6ePIc7dQmHmzzhqKmuzJSwJXJVoZ1/+PklGgfGKcxODVsZl+Skhdc7birJ5kKXj8cu9XGsdYh+c2kozU0NR4JdVWTnfNeIZfX0cNWqJ1ZwoFBAHq9lrXzLmhT+8VW7Eu2+83L69Gm2bNlynQU91G/3799/nZC855576Ovru678Jz/5Sd7ylreEhSSAUsqrtXZGlltISCqlkoHfAA9rrb+wGOcpCILwbEeEpCAIwu1nUYTkU089RWpqKn6/n/Xr15OcnBxz39EpP3//kwscanSzsSCJTZVLOdzkDqdvqCm0c/fKAp63soBVRVk81enjwdouHr88iHvSNHXt0myeZ1kZy/IyOdHq5YkrA2EX2JAAq8qc4C33bKEgK5WjLR6euGIiovomZkiyQY3DxovXL+XeTcv5/fHz/M/5SSZngiTZFFP+IGnJNnZVXHOB9fv9VjoPD0+2+cJusltLMlnvgir7NBN+zQW3pnkig7NdI/iD2oom6+LO6nz2VDq5Ojwdjs56rmsYDeTZU9lfbSyRO8uctLnHOdAwyE/P9uK2/EWLclK5q9q0ZVn6NEfq2+nBydHWof+/vTsPjKo+9z/+PjPZ92RmEhISsieEAGEVEZCAilYRcK9aq7Vab1tvXeraxVq72fuzy63trba31VvbuoFWi0qtSECQRYQsJGQlIQshmZns28xk5vz+mJlDgtkIE2Lt8/qnJDlzzplkOuaT5/t9Hu37lxwTzOpMAxdmGJibEE5hYxc7PVVPrYOrMVhbUpoRG8rHDe6RHh/UtGH2jPRIiVC4bH4SqzJiSIgMYl+de+/lnmPuGY/gbkV62RwTty9PIjLYT2uSs7fWvdzW27l2VXoMBsdJZmdmUFBpdofTFhsOFwTqFRYnhbM6y8TK9Ghsg6eqlYdOq1YuTork70daqGjp5Yfrs4i3N7JkyZIzffmO6aOPPmLJkiUjVhy9QbKwsHDC5/PF0lZFUf4I9Kiq+o1JPzEhhPiMkSAphBDTb1JvxDabTfv3wMAAH374IfHx8cyePXvUZX/HLL3c/VIxNeZegvx05Bnh86tyWZVhoK3Xzo4KM9srLHxc34HTpWIIDWDJzCCyw+x8fnUe2w8U0xborkZ6O6bGRbjHeVyUbeK8lCiqzO4K4vsVZm1P39AlsAuSItlVUss/Sk5Q3u3HMUu/dn9hgXq+e3k263JiKWzsZEele29lQ7v7mNlxYdrS1YzYUD6q66CganiVMSPaj/MSQ8gOtxMX6ke9PcSzBLaLtiFVRvd5jBhC/dhd0651Z/WODlk0K5LugUGOnuxh/dw4Fs+K1PYP9juc+OtgWWq0FnJVFff+zCorB+o6sDvd40UuSIt2L6VNj6HX7tRCpXdJaVignuUp7iWwF6RF0dbnZGe1lXcKG6jtcuFS3UtKV6S5m/EkRAZy7+YyOvsHSYgM5LhnXEdsWAArPUtgF8+KoMoz0uODmjaqPftFEyIDtWWy8xLCKaxv4/2jLeyr76al173fMSkygFWeIJwzI4zDje6xHzur3CHXT6fws6tzuDA9iuLiYhYtWjSZl++oPvroI5YuXTri1wYHB7n44ov5+OOPJ3y+Bx98kKeeempos50YVVUfGnqMp9nOx4D3yRwCFquq2qYoyg+BHOA6VVVdCCGEACRICiHEp8FZBcn29nbKysoICgoiPT2dqKioEY/fWWnh3ldLCPTTccuyJBra+/lnaTM9Dvdy0vNSolmbbWRNtonwID92Vpp546NjHDppo8+hEuCnIydax4YlqVycE4e/XudpomNmd00bfXYnwf46Lkg3sMYTrg4cPEhPeAo7Ki3u8RiDLkL8Feab/NmwJI3V2Sae3nGMlw42EeqvMDCoag1nVmUYWZNlYHlqFOYeGzur3KM4vHsZDaHuZjL5WQYWJYSw8+ARagZCOHzSQXGTu8poCvNnSUIwc6KcpIUO0qMPp6xDx4HGPi3kJkQGkZ/lDk6LkyM52tzDu2WtbC5sZsDTTGZWdJAWGAN7TlJusVFnD2VXdZu2lDbdFMJqzzLZ7BlhHKrv1IKldy/o7Blh2lLaVEMI+2vb2FXtXkpr8eyJnDPD3QAndtDMxcsXsq+uwzMaxD2CBNw/r2sXzuCaBfEYQv3Zc8xdzfTum/TTKSxKinCPIEmPIcRfx192FNLgCGdfXQf9jlMjPVZ5Gumoqovtpc3sqm7jSOupauWSWREsTIrizZIWTnbbefq6XPcIELudsrIyFixYMJmX76jGCpKdnZ3cdNNN7Nq1a8Lns1qtGI3G94FZwHHc4z/aFEVZAvyHqqp3ACiKcjvwLc/DfqSq6nOKoiQCDUA57g6uAL9WVfV/J/PchBDis0SCpBBCTL9JB8njx4/T3NxMXl4ex48fx2QyYTAYRjz+tcMnePRvZcyKDmLdnDjWzjbRW3+E0Flz3ZXIcrO2PDPTFEJOxCAX5cSSn5dBYVMX75eb2VbcREufO1zNiQ9nTZaRtbNNZBhDOHC8w9Nox0yzZwlnWqSOKxensDbbREKEPy/tOMyRNoVDJ+3aEk5wzzG8ZbaepXPSKGqxU1Dp7hTb3udAr8CiWe4mOmuyjESF+LOnpo2CSndXVe94jIWJYVwyJ578LAMhAXo+qLZSUGllT00bvXYnAXqFhTNDmW/QkRFqIywkmKq+QApPOth/3B2ugv115M2MoMrcR9eAgwcvcc9E3FllZX9tu7vzqp/Cigx3NfPCzBh6bU4tMB707AUND/RjRbp7mezK9Gja+hzaMYWeIDx0vMgFadGc7LKxs9LCzmorJU2n9kSuSHPPpNQrCt/+eyV+eoW48EBqPD+roZXIpbMiqLL0sdszgqTSM0MzPiKQ7IhBrl6ezcLESI629LirldWnRnokRQe5z5Mew7yEMD6ua+P9oyfZU9eNdUAlyE/hp5fPYvWcRPR6PQMDA1RWVjJ//vzJvHxHNVaQbGpq4r777uOdd854+sbZt5MVQggxjARJIYSYfpPaI/nxxx+jKApz5sxBp9NRVVVFZGQksbGxIz7G2mPn7SMneb/CwoG6dveySn9YlxvP2tkmLkiLwdxj5+2iBv5R3ERVhwun6h41ke8JjMGd9YTNSGbv8R52VJo53OBe3hobHsiabCNrs00sS4nieFs/71eY+fvHdRzrdAdPQ5B7DMcVC2eRGBXMXX8tpKGtn7iIQK1ilxAZyEWzY8nPMpA3M5yjzd18UN1GQZVVC0XJMcHkZxnJzzQwM9DO9sJqThDDnrouLQinGUPIzzSQn2UkNyGMwoYudlZZ2VFp1ZbJphuCWRQfQHaYnaQwOOkKZ2eDnYLqDlyen8jsGWGszjCwIjWSvuZqzEo0xVaVXVUjN+xJNgSzv7bDs4/TisUzMmXezAhtvEhCZCB7R2jqkzczggszDaSHDBAw2Ic1II7dx9rZU9NO14C7Ehnkr+PmJfF8LjeWmJAArRL54bF2d+OfIXsiV2XEEOyvcx9T3caeGis2J/jrFZZ6KpGrMmLQ6+CDancn2AN1HQx49qYuTY5kdlwYbxS3MOBw8rMrUzDp+2lvb8ff35/w8HC6u7vJy8vzydgP7+v64MGDowbJqqoqfvKTn/Dqq6+e6aklSAohhI9JkBRCiOk3qTdis9lMRESE9nFtbS2BgYEkJCSM+9jugUE+qLbw8u6jlLW7h94H+OlYGB9MdriDm/PnERUexq5qi6dBjoUem3tf4PLUaC6ZM4P8bCN+OkWbEflBtfUTy1vDuupIT09ny94KjtlCOVDfpS0X9dMp3HJ+EneuSGHQ5eIvBcWUtit8VN+NbdA9qmNFegxrsoysyoih3+FiV5WVHZUW9te143CqhPjBqkz3ctwLM9zzKN2VPwsH6jpGbLTT0TdIgafRzsf1ne5ussF+zIr0p6y1n8gA+MqiKNqcgXx0op/Cxi5cKkQG6cnPMrE6y8AFqdGc6LRp+x2LRmjYc35qNA1tA9ox3hEapjDv7EsD5yVHUWvt0+ZalnmW286ICGSVJ5web+vjqX/WEBseQHigH9WesDwjIoAVqVGszjSwKCmCSnO/1gnWuydyZlSQFhidrVUEx2dpMym9oTspOmjYvskjJ7rZXdPO9goLzV02DKH+/O6m+WTFhmqvn4GBAU6cOEFzczP+/v5ERUURExNDdHQ0er1+Mi9nwD0f9dChQ6M28Dl8+DB//OMfef7558/01BIkhRDCxyRICiHE9JvUG7HD4cDlOtX7o76+HkVRSEqa+Ey/vXv3smjJUg7Vd7JlXyUHmvq1piu5CeGszTZxUbaJNGMIH9d38OqHFRS2DnKiy70sdf7MCK0SmWrwLm81s6PCwonOARQgLUrH5/KSuGTODAobOnji7UqC/XUE+umw9jpQFMibGcncaBeXzp9J9kwDB4679xcWVFoxe6p6CxIjyM8ycmFGDD2txyk6aePYQAg7q9qw9tqHNdHJzzIyIzyAD2vbKai0sqvKSlufA72isDApgtWeiqYxzJ8Pj3Xw3N56jpzoBkAH5M0MJc/kR1pQHwEuO2a9gdIOhQ9rO7VmPItnRXqqjEYig/UjNuxZkhylVSLDg/z4oNodGPfUtGkVxKXJ7kY7M1QrMRGh1DvC3Hsiq630eUJ3dIg/X75gFhfNNhLop2NXpbviube2/dR+R894kdUZBvz9dFqo3O/ZE+ltDuQNjYqC1inWW4kM8tOxNDmKNGMIWwqbCfHX88db8kiOCf7Ea6erq4vm5mYyMzPp6OjAarXS0dGBv78/BoMBg8FASEjIGb+mjxw5wsKFC0f8+gcffMBbb73Fb37zmzM6LxIkhRDC5yRICiHE9PNJkGxqasLhcJCSkjLhcxw4cIDc3FzKysowGAykpKRwzNLH9nIzOyotWmfWhMgg1mYbSQ/u46J5s+hwBbGjwsz7FWaKGrsAd/XLu2dyUVIE2z8qpaDCQp0jnBJPSAN3te1bl2WRn2WkxtLL+xXuhj3eIBcfEcDa2SbWZBpZlBxJTWsfOyotFFRZKGt2V+ziwvy4OCeONVnuY6paet37KqusQzrFBrE60x0YFyVHUnGyh4JKKwVV1mHzKMOC/Ck/2cOaLANfXJbkWXZqoaLFvZR2ZmQAS2cGMzt8kJQwJ+1KOKXtCvvre6kyu4+Z5Rn5sTrTwIKkSMpOdGt7IqvNp5bkrs50z27MS4yg1HtMpYVjnn2K3mNWpsfwz3Izrx5qJt0YgktVtb2MKYZgVmcYWJVpYP7MCIqbuthVZWFXVRt1nsY/KTFBWpfXuQnhHGnu4dU9R6nqCeB426nzrEqP4cKMGHLjwyhqci8j/me5GXOPg1nRQfz+5vkkRAaN+Nrp6OjAbDaTmZk57PP9/f20tbVhtVoZGBggKioKg8FAVFTUuNXK8fZdbtu2jYMHD/LTn/50zPOMQIKkEEL4mARJIYSYfj4Jki0tLXR3d5Oeng4woX1r+/fvx+FwkJ2djclk+sTXLT02CiotvF9hYU+NlQGHixB/HRdmuauQqzONOJwuzzFmPjzWxoDDRZAfLE0MZXaYnc9fvJTvvFnO3mNtzIwKwtJjwzaoEhqoZ6VnCeyFGTF09w3wTlE9u6rbKTE7cLggxF/HyowY1mSZWJoYQvGRUhpd0Rw6aWdvbfuwJbDezqt2p0urZu4b5RjboIv3ys38fnc9ll53ddV9LXcYTA/up9lsxeIfxwc17drMytAAPYsTQ5kbAxkhNgKCQ6jqCeDQSTv76zqxO91zLVekxXBhpoELM2OwD7q0ULm/1j0WJDRAzwVpMVyQGkH0wAliZ8yktP1UUx+HZ5NmckwwX74gidWZRgY859k1ZLxISIBnvEiGO6D2O5zs8hzz0fEO7E6VEH8dy1IimeXXzRcuWojNidYFVjsmQM/5KVHMiAhkS+FJEiID+cPN8zGFB4762mlra6O9vV17vY3E5XINq1YGBARgMBiIiYkZsVrZ29tLXV0dubm5I57vtddeo76+nu9+97tjvq5HIEFSCCF8TIKkEEJMv0m9EQ8ODuJ0OrWPLRYLVquVrKysCYXI5uZmSktLWbBgAUajcdzjBxxOtuw+wuFWJ/vqezD32NF7lniuzTaxNttEqG6QV3YWccwWyr6GHszdpzqzrssxcf/FGcyICGJvbZu7w2uFGXPPqWWp7lmTBkyhfmw/0sj75WY+auqn0+5OAnPjQ9z7M7OMJEQGsr+2gwLPHsTWbvcS2DzPEtg1WQZmRgV5jnHvm/QeMyc+DEuPg9ZuG9+8OI00Y6hnHqWFFs8x82eGe5bAGpkVE8SBEWZWzokLYWGcP1lhNuJD9TQNhlFsdfFhbSctnuc+NyGc1ZkG8jONJBuC+aiug53Vp64FkBsfru2Z/J9ddRw43sH8mRG0dtu0RkRz4t2Nfy7MNJBuCtXOs2vIeJGcIeNF0k2hHDze4d7DWt6Kuc/9WsmODWFFWrR7FqcxhEON3XxQ08a7R8109A8yZ0YYz9w4j+gQ/zFfD2azmZ6eHlJTU8d97Xj19/djtVqxWq3YbLZPVCu7u7tpbGwkJydnxMe/8MILDAwMcP/990/4mh4SJIUQwsckSAohxPTzSZBsb2+nubmZOXPmjH0xVaWyspLe3l70ej2pqanDmvaMpaamhtDQUGJj4zhyoovtFWbeLzdrHVUTQhUuyY3nsnkJDDicfO2vh3GhIzY8gIb2AQBSjSGsyTKxNtvAnBmhVLT0sKuqjR1VVso9y1JnxQSzxhMGZ/j1sedIHRU9Aexv7ON4l7sKOzMykLWzTeRnGlg8K5IqzxLYnVVWSpvdy2RnRgWxOtNd9VwyK5IaSx+vFzbzyqETOJzub3t8ZCD5mUYuzIgmuLuRTlcQVX1B7KyyaktyZ0QEesKggfNSoqhr66eg0l1lLPHMrIwNC2BpYjA5kU5SQ+wM+EVQ3qXnQGMfxZ5xHsawAHdgnBlCaE8DkTPTOXhigF1VVgo9DXsAFiZFcOv5SSzXmvpYPcd04lLdY0FWeULlBWlRtHTZtapnUaN7vEh0iD8r02PIjhgkO8KJMSmDXdVW7VpOFSKD/LggNYqIYD9ePXySufHhPHPjPMKD/MZ9LbS0tDAwMEBycvKEXjunczqddHZ2DqtWhoSE4HA4Rn0N//a3vyU6Opo777zzTC8nQVIIIXxMgqQQQkw/nwTJ7u5uamtrx5zr53A4KCoqIioqivT0dI4ePUp8fDzR0dETumZdXR0BAQHDOsOqqsq+kiq2l7dS1RfEwePuTqh6HQTo4P6LM7h2cRLtfXYKKtxLYPfVtmvdUi/0BL2V6TFa19X3K4Z0ZvVX3ONHsk2szIihvbuPd4oa2FXdzhHLIIMuCA3QsSrD4GnG41666u3euvdYOwOD7mWgc+LDKD3RTZC/nh9vmI2l186OSuuQJbkKK9IN7iWwme55nB9Ut7Gz0sqeY2302Z0E+ek4PzWa1VnuKqOfXhlxZuWimWHMN+rICB0gIiSIY/3BFLY63KM67C789QrnJUdxYaaRnBlhfPvNozR32shLjKCqtfcTTX3ys4xEBfuzu6aNnVVWdtdY6ewfRK8oLJoVqXWCNYb68+Ex93iRggoz3XaXVvH17tGcERHAnpo2dlVZ+We5hX6Hi2XJkfz8qmxCAv3Q6XTodLoxXwvNzc0MDg6eUXOnsfT391NfX09bWxt6vZ7o6GhiYmKG7a186qmnmD17NjfeeOOZnl6CpBBC+JgESSGEmH6TeiN2Op0MDg5qH/f19VFRUTFqx8vu7m5KSkrIyMjQZk1WVFRgNBoxGAwTuubpnWGdTielpaX4+fkxe/ZsdDqdNlpka/FJPqyx0D8IAX46Lkhzj/JYnRlDkJ/Ch8faPUtO3YHIT6ewLCXKEwajOXm8hrI2FxU9gVrXVW+wWpNlJD/LSHSwnn8W17OjwsxHJwbosuMJTRGsyTa5K5oRgRyo6+D/9jWyv65dey7eLrDnJ4XS0VBJV3A8h1sc7Ki0aEtF53mXpWYZSTeG8NFx93LSgkorTR3uCmvOjDDyMw2szjKSHRfKofrOT8yszDAGs3BGAGlBfczwt9MTZKSsU8/+4z0cs7rHcOgUuGxOLNcvTmDezCHNeIY07Dm9qU95c7c2s1JrDhQVxIUZMaQG9jJvRghqxAx2VbvD59Eh40UuzDSgqiqvHmpmZXoM/31dLn6KisvlYujvBjqdDkVRPhEsm5qa3NebOXNCr52JaG1tpa+vj6SkpGF7KwMDA+nq6uKtt97isssu48orr5zQ+dra2rjhhht47733qoE64HpVVdtPP05RlFuB73g+/KGqqv932tffBNJUVZ17Vk9QCCE+QyRICiHE9PNJkLTZbJSUlIw4g+/kyZMcO3aM+fPnExYWpn2+qqqKyMhILViOp7GxEafTSXJyMjabjcLCQuLj45k1a9aIxx84+DEDYQnsru1ie7mZRk/4mpsQ7lm6aiTdFExRYzc7PE19vF1F02ICuDQ3nvwsI7NnuCuJ71daKKg8FazSTSGsyTSSn2VgbkI4B2taePdIM3vru2nodn9bEyMDiQ0P5FBjF4uSIvjP/DQO1ndosx0BZoQHaMtklya7l67uqLSws9JKsWfpalx4oKcKaWBZShRNnTZ2ejrFHm5wLzk1hPpzoWc/5PLUKMw9DnZWue/54/oO93LSYD+WzgxhboyK4hjg2ZJBUBQyYkMpa+5h0KUSHuinNQdalRFDn935iYY93kY7qzONXJgZw6BTZVe1ew/nh8es2J0Q5KdjWWq0Von003mrpxZ2VrUx6FK5PDeWn2zKwV9/Kih6w6TT6RzW0MkbKHU6HQ0NDfj5+REfHz+h185ENDc343A4PvF66uvr48033+SXv/wlDoeD9evX89BDDzFjxowxz/fQQw8RExPDI488oiiK8ggQrarqw0OPURQlBjgILMH9/8WPgcXewKkoytXAtcB8CZJCCHGKBEkhhJh+PgmSg4ODfPzxxyxbtuzUiVWV6upqurq6mD9/Pv7+wxuoHDt2jODg4AmHgebmZvr7+zEajZSUlDB79uwxq5mFhYVkZmYSEhLC4OAgVa29WrXOG9DiIwPdY0OyjOQY/Sg4eIRGVzQHmvq1gGYKC9Aa6JyfGo25x05BpYUdlVYOHu9g0OXeE+iuHhpYkRZDS0cPbxc28FKRlbYB97c4LEDHqkwja7ONzI50UXWsDmtgPB/WdQ1bArsiLUZb3qqqaAHt9OWt+VlGVmcaCPTTsbumjYJKizZH0rt0dXWmgZTAXvSDA7QHuauDu6ra6Oh3ABCoV7gyO4xFMQ6Mof402EMoMrvYU9uJtfdUAyHvzMqkaHfjn9Mb7eTGh7MqPZp41UpeqokWV4S2t9Ib4LNiQ1mVHkNLt52tR1q4cl4cP96Yg1439spPl8uF0+lEVVUtWDY0NBAcHExcXNy4y2Anarwq5913382Xv/xl+vr6WL58OZGRkWOeLzs7m4KCAuLj4xVFUeKBAlVVs4ceoyjKjUC+qqp3eT5+1nPci4qihAHbgK8Ar0iQFEKIUyRICiHE9PNJkFRVlb1793LBBRcA7mBZXFxMWFgYmZmZI3ZyPX78OHq9nsTExAlds6WlhebmZvr6+sjLyyM0NHTM44uLi0lNTSU4OBhVVVEURbsPc4+NXVVW3q+wsvdYGwODLoL0sDI9mktyZ7AqwwCeEPd+hTugeUPc8rQY1ma7g5U7xLnD6a4qqxbiFs+Kwtxjo8bcx63LZpIeqfB+hYWPTwzQ7XAvJV2YGMFFs03kZxmZERHI/tp2dgzpzDo0xK3JMpIcE8zB+k4KPFXGYctbs9wzK7PiQilu7KKgyl35885/TDeGuI/JMlBn7eN7WyuICQkgMsSPGrN7eWtSVBCLEwLJiRgkOXSQbl0EZR06DjT2aQ2EvMtSvY1/GtoH3BXGSgtFnqY+hlB3M57VmQYuSIumtduuzaw8WN+JCnzhvEQevTRjQh1+h3K5XPT29lJSUkJOTo72s/VWKs8mVNbX1xMQEDBqpfFLX/oSTzzxxLgNpbyioqLo6OgAUBT3E21XVTVq6DGKojwABKmq+kPPx98F+lVVfUpRlF8Au4DDwFYJkkIIcYoESSGEmH6TeiN2uVw4HI5hn/vwww+54IIL6Onpobi4mLS0tDGX/w1dqjruTaoqxcXFdHR0cMEFF3yiujmSkpISDAYDJpNpzIBRVXucHaUnOO6MZFd1O9ZeO3rFsx8y270EdkZEIB8d72BHpYUdlRaaO91Bb97MCNZkGVibbSTFEExhQxdbj7Twt6KTWmfW7Lgw1mS5q5W6zhMcOdFFWYeevfU9NPV4ZzYGsTbbxJosI3mJ4cO6wHqXwCZEBpHvWd66NDmK+vYBbemqt6OqtzPrqvRoIvpOMOgfSnVfELuq27TqKYAhNID71qZy0WwTvXb3/MeCSgv7hsyaXJIYylwDpAfbCA4Jobo3kMMtdvbWdtLvcBLop2NZShQr06Iw2JpJT5lFeaeOXVVWrTLq3Ve6Mj2Gww2dvF9p5bblSTx4cfoZh0hwz3osLi5m7ty5hIeHa9XKie6tHEttbS1hYWEjzjQFuP7663n22WeHLX29+OKLOXny5CeO/dGPfsStt96qBUkARVHaVVUd1llqtCAJvAc8oarqBkVRUpAgKYQQw0iQFEKI6efTIJmRkUFVVRXz588nPDx8zHN4l6qmpaWNedzg4CAlJSUoikJAQMCERoy4XC7a29upr6+nr6+PmJgYTCYTUVFRWrjwjiKx2+3MmTMHvV6PS1UpaepiR6WVHZUWqlqH7If0LIGdmxBOtdkd9AoqLdqYjplRQSxIjODDY+3YBl08vC6drgEnBZUWbZlsdJCOtTlxrMkysjwtmhPWLt4pauSDYx2UWQZxqhARpOfCDANrso2sTDfQ73AHvR2Vp6qnwf56VqSfWt6qU9wdXgsq3R1Ve2xO/PUKy1KiWZNlZFVGDH8+0Mif9jeSEBlIn91Jx5Cuq/meLrCx4QHsr+vQlqV6Z1bmzgh1z6wMtREfptDiDKfYqrKntpOmTvcxWbGhWjOe3IRwSprczXgKKi3UWNxVz/svSuOOFZMb2dHT00NJSQnz5s0bttfWa7S9lRMNldXV1URHR4+6XPqKK67gjTfemHCX4bNZ2gpEAd8F7IAfEAt8qKpq/oQuLoQQn3ESJIUQYvr5JEiqqkpBQQFhYWHk5eUREBAw7jlaW1vp7OwkMzNz1GP6+/spKioiKSmJsLAwGhsbyc3NHfV4b4h0uVzaUlan00l7e7t2vfDwcGJiYmhubiYqKoq0tLRRq2ON7f1aqPRW9Ayh/qzONLIm28Dy1Bh6bO6xIVsOnaDYEypD/PWsznIvSV2eHEHxkSMct4dSZHGxu9o9piPQT8fy1GjWZLuXpQboVP5R3MCOCjMfN9vocYBeB0tmRWmdYmPDA9hf2+5ZunpqCez8mRGszjJwQXI4XY2V9ATHUdTqZEeVlXpPAyFwL4P99qWZzJ0ZQVlzt6dzrUXrujorJtjdBTbTwKJZkRyz9LGz0krBkJmVprAAzksMJjvcQYKuB0KjOTYQzEdN/Rxu6PKMVvFnZUYMy1Oj+XvxSfbVdfDwugxuPX9y4zq6u7s5cuQI8+fPH3dJM6AFyZGqlXq9fsRQWVlZSWxsLFFRUSOeMz8/n717907otQ3w4IMPYjAYhjbbiVFV9aGhx3ia7XwMLPJ86hDuZjttQ45JQSqSQggxjARJIYSYfpN6I1ZVFbvdDpyqGLa3t7N69Wpt7t54rFYrFouF7OzsEb/e3t5OWVkZc+bMITo6etxZld5q1On7IU8/xmKxcPToUXQ6HSEhIZhMJkwmE0FBQWPeb9eAw1P1s7Crqo1u2yCBnsY30SH+/L2khVRDCLeen0hRo7uqae21o1Ngfnwol86NJz/LQHxkEAfrOthx2l5HbzfZ/CwDGcYQ9lU18+6Rk+yr76Gp1/1jSjUEszbbRH6WgbyZEVS29lLg6d7qXQIbF+7PRbNjWZ3pPubezaXsq20nITKQk502XEBMiKfDq6c5UOfAoKfqadE6s4YF6t3dWzPdFU0VtJmVu2us9Nld7pmViZ6ZlSEDRIUGcnwghCKzk93H2unoH0SnwBNXzubqBZPrsNrZ2UlZWRl5eXmEhIRM6hzePy54Xx9eQ6uVR48eJTExcdRK+oUXXsihQ4cmvCTXarVy/fXX8/7771cDx3GP/2hTFGUJ8B+qqt4BoCjK7cC3PA/7kaqqzw09jwRJIYT4JAmSQggx/c4qSPb19VFUVERycjL19fUsXbp0wkGyvb2d5ubmEZeqNjU1UV9fz4IFCwgODgbc++OqqqpYsGDBiPczXoiEU6EkJyeHqKgo+vv7MZvNmM1mnE4nBoOB2NhYwsLCxgwMDqeLj+s73XsmKyxaZ9I5M8K4eLaJNdlGTAEO3tlfRhMx7Knr1pbJphlDPONHDMyfGcExi3eZ7KlusjMiArVQuSwlmrrWDrYVN/JBTQflbU73KI8gPaszjeRnG5lv9KOorBxrwAz21vdqS2B1CrhUWD8vjocuycBfr3g6vFr5oNqq7WM8LyXKXWXNMhATGsC+2nZt/6W5Z3j31sXxgfSdPIYzJpV9Db0UnDazcnF8IImBNjZXDtDQrfL4pSlcszRlUnsiOzo6KC8vJy8vT3sdnC1vhdIbLL3Ky8tJTU0dcdmsqqpceOGFHD58eDLP48yfuBBCiDFJkBRCiOk36SB54sQJKioqmDt3LpGRkRw8eJD58+dPeOlfV1cXx48fZ968ecPOW1FRQX9/P/PmzcPPz0/72sDAAKWlpSxevHjYeYYGgrH2wbW0tFBXV8f8+fNHDCUOhwOLxYLZbKa3t5fo6GhMJhPR0dFjnldVVarNvbxf4Q5eRU1dABiCFC6eE8clc+JYkhxFa5eNHZXuPYMfnTY2ZE2WkQvSo+m3u7R9hR8ea6PfMXwkyOpMA7ic7iWwlRYONdvoGwS9AouTIrgoJ441WQZUVG5/oYjmzgHCg/zo7B/UmgN590Omm9zNgQo8MyKPefYxeudjrh5S9Ty98U9ceIBnSa6R81Iiae60U1BlYVeVeySKU4UAvcLjlySSGWqjq6uL8PBwTCYTBoNh2M91NO3t7VRUVLBgwYJxq8Vnw+Vy0dbWpu3t1ev1qKqKXq/XqpXeIFlYWDiZS0iQFEIIH5MgKYQQ029Sb8QOh4OPP/6YOXPmEBgYCMDhw4fJzs6e8PLD0yuMg4ODFBUVERkZSXr6J7t62u12ioqKWLp0qfvGR9gPOeITVFXq6upob29n3rx5E+r46m3WYzabaW9vJzQ0lNjYWAwGw7iPL6qo5b2yk9QMhLC3tgPboHuZ6KoMg9b4Rqco7K5xjx/xVgaHNsfJzzIQE+rPvtoO99LVIfshFyRGuOdaZhsJGGjn/ZLjlHf5caCxn5N97h+nn2c244MXp3Pj0plUmd2Vw6HNgeIjA8nPdF/rvJQoTnbZ2OnZD/pxfae213FVRgz5WUayI5xU1x6nLSiB3bWd7PUE3WB/HeenuoPu7LgwHnitlJYuO7+9cR7L02K0n0FXVxcWiwWr1Yper8doNGI0Gkfc89jW1kZlZeWUh0hwV6mPHj3KggULCAgIGLFhj6IorFmzRoKkEEJ8SkiQFEKI6TfpN2KbzTbsY+/cxvG6tXoNrTD29vZSVFQ05sgQp9PJwYMHWbZs2YRDpMvl0vZDZmdnT2rOoKqqdHd3YzabtRDk3Vc5tLKpqirV1dUMDAyQm5uLTqej3+Fk77F2rcOrtdfxidEi8ZGBHG7oZEeFhfeHLBPNmRHmWQJrJGdGKOUtvdoSWO9cx9gQHety41k728TiWZG8W9LEY2/XYHequFT3Dzc62I/VnvNckBZN32ldYN1hUM8Faae6wLrnY7axs8q9H7Sj34FegUWzIlmbbWJ1poH4yEAOeDq8FlRaOdE5gF4HIf5+PHPTfBYmRY75s7dYLFgsFgYGBrTqb1RUFG1tbdTU1LBgwQLtjxRTxRsiR1s66612v/POOzzwwAM0NjZO5jISJIUQwsckSAohxPSb9Bux3W4f1rikrKyM+Pj4CY9HcDgcHD58mPT0dMrLy7UlsqPeqKqyd+9eli9fPqEQabfbKSkpwWQykZSUNKk9eiMZGBjQ9lU6HA6MRiMGg4Hjx48TEhJCRkbGiNfyjhZ5v8LCjkor1eaRR4scb+vXAuPhhk5UIC480L0kNcvIspRICsuq2Xu8m4qeAG32Y7C/DrtTJTxQz29vmk9ieADbiurZUWWh8KSdvkHw1yksTTnVBdYY5s/+2g4KPPshT3admo+Zn+m+XuCAlf3VLTQRw86qNu2+Uw0hnmW5BkID9dz5l2IcThfPf3Ehc+In9scEQOuq611WPDg4SHp6OjNmzJjwMunJ6OrqorS0dNg+3JFs376dH/zgB7z99tsYjcbJXEqCpBBC+JgESSGEmH4+C5IVFRUYDIYJ/7LtcrnYtWsXwcHBE64+ffjhh5x33nnjNtXp7e2lpKSE9PT0UQfM+4LD4aC1tZXq6moURdEqlTExMeNWP+vb+imotHhGi3TiVIePFrkgLYZ+u5Nd1VZ2VFjZXdNGv8NJoB4WxgexfmEyq7OMBPvr+e0HtTz3YQMKCk5VHTYfck2WkfiIAD44eoL3yk6yv7GPFs8S2ExTiNYFdm5COFWtfdo9eZfAGoJ1XDxnBmuyjCxLjcLcbXfvq6y0sL/OvdfTX68QFezPH25ZQIZp/PEcI2ltbaW2tpbs7Gza29uxWq2oqorBYMBkMo3bAOlMeEPkeJ1gd+3axXe+8x3eeust4uLiJns5CZJCCOFjEiSFEGL6+SxIVldXEx4ePqFfuL1LTk+ePEl+fv6EOr2qqsru3bsxGAzExcURGRk5YrDw7q/Lzc2d8DLbyerv76e4uJi0tDQMBgMdHR2YzWba2toIDQ3FZDJhNBrH3VfZ2T9ktEi1lR6be87kBWnuPZOrswyE+ut4peAwpe06DjbbtT2TiVFBNHYMkB0Xxv9+YT4N7QMUePY6Vraeqh56Q2VeYjiVJ9rZVtzE7tpOKtucqEBMiB/5nkrl8tRoSiuq2VffTWVvIHuPtZ+2BNbAhZkGqlp6uPvlI4QG6vnLlxYzK2ZynVVPnjxJQ0MDCxYsGPa98jZAslgs9PT0EBkZqQX1iXYHPp13JuV4IXLPnj08/PDDbN26lYSEhEldy0OCpBBC+JgESSGEmH6TfiN2OBzDGpLU1tYSGBg47i/d3qY5RqOREydOsGLFirFvcMh+SG8TnNbWVrq6uj4RLJqamjhx4gTz58+f8v113qrWnDlzPrEkV1VVenp6MJvNWCwWdDqdVq0crxmR3eni4+Md7PCEQe+cybRIHaszYti4JJUMUwhHT/bw1Hs17K/r0B6bFB2k7atcNCuS1u5TnWIPeKqHkcF+XJhhYE22kZXpMfQP2NhW3EBBpZXCFjv9g+Cvg7mxAaxfmMyabCPRIe4lsDs9syZPdrn3x/rrFWZGBfGHLywgPnJyTXGam5tpampiwYIFY3ZzdblcdHZ2YrFYaGtrIyAgQGvYM9HRIN4QOX/+/BGb/HgdOHCAe++9l7///e8kJSWd8XM6jQRJIYTwMQmSQggx/XwWJBsaGlBVlVmzZo36mJ6eHoqLi8nIyCA2NpYPP/yQCy64YPSbG6OpjqqqdHZ20traitVqxeVy4efnR15e3pR3+jSbzdTU1Ex4vqG3uYzZbMZut2vLNSMiIsZcrqmqKiX1Vl7ec5TyLn+Otrob8cyMCiImxJ+SE92snxfHPfmp7D7Wxo4Ki7ZnMiLIj1UZMazJMrJS6xTrrnrurLLS2X9qhmS+Z65lTLCeV3cc4lCLg8JWJ6397pdHdmwoa7PdnWJzZoTx149O8NN3q0iKDuaF2xZhDJvcXsampiZOnjzJggULzrjC6J0BarFYcDgcxMTEYDKZRq1U9/T0UFJSMm6IPHToEF//+td54403SElJOdOnNBIJkkII4WMSJIUQYvr5LEieOHECm81GamrqiMebzWZtVp936PtYQXKinVmdTiclJSX4+/sTHByMxWJBr9cTGxuLyWTyeahsbGzk5MmTZzQzc6jBwUGsVitms5nu7u4xl2t6w09OTg5RUVGYu20UVFnZUWFhz7E2HE6VsAA9F2a6K4yrMmLQ6xT2Hmvn/Qr3XMe2Pgd+Ok+nWM/YkPjIQAoburSmPrVW9wzJpHA9F6SEsWlpOrnxYRxtbGNbSRN7ajupanehAuGBevrsTubEh/PszXlEBY8/TmW072Nrayt5eXmTXqY69Hva1taGxWKhs7OT8PBwrQmSv7//hENkcXExd911F1u2bCEjI+Os7mkICZJCCOFjEiSFEGL6TfqNeHBwEKfTqX3c0tJCd3f3J34B985xtFgs5OXlDQtfowVJ7yy/8ZrqDAwMUFxcTGJi4rAltd7Oqq2trTidToxGI7GxsYSGhk66YYuqqtTU1NDX10dubu5Zhx84tVzTu68yODhY21fZ19fH0aNHmTdvnha8h+qzO9lb26bNh7T2ngqMa7Pdex0TIoMobnIHxh2VFmrM7sCYYQolP8vA2iwj82ZGUGvu5sVdRyht11HaMuBp/BPA6kwD+Vnuxj/dfQP817Zy3qnsIiNS4fG1sSQnxBEdHX3GY1Xq6+uxWq3Mnz/fJ9/HoU4f16KqKgMDA+Tk5GAymUb9+ZeVlXH77bfzyiuvMHv2bF/ekgRJIYTwMQmSQggx/XwWJL1VtqG/hDudTkpLS/Hz82P27NmfCBx79+5l2bJlwz6vqiqDg4MAYwYU73637OxsYmJiRj3O27CltbWV/v5+bQlkVFTUhEOly+WitLSUwMBAMjMzfdY9dChVVent7cVsNtPc3MzAwACzZs0iPj5+zCoajD5aJDM2lPxMA2uz3YGxsX1Am2np7RQbE+JPbrTKJbkzuHxRGnaniw+q3bMhP/A0/gnQ60iKDqLG0sfK9Bh+ce0cBnq6MJvNtLe3a42FDAbDuFXauro6Ojs7mTdv3qTmep6J3t5eCgsLiY+Pp7u7m76+Pm1m5dAAXF5ezm233caLL75Ibm6ur29DgqQQQviYBEkhhJh+PguSHR0dNDU1ab+I22w27Zf40fZN7t+/n8WLF+Pn5zfhpaxwao/ivHnzxg1ZQzmdTtra2jCbzXR2dhIREUFsbOyYXUAdDgfFxcWYTKYx93/6SlNTE83NzeTk5NDR0UFrays2m03bVznaHsChRh4tEqB1bl2eFo190MWO8hbe+OgYpW0qPXYXAXod56dGsSbbSH6mkehQfz4+3sHTBbUUNnZx8WwjT12dS4Df8ODvbSxktVpRFAWj0ag1Fhp6r8eOHaOnp4e5c+eekxBZXFzM3Llzte693mZN3oY9ZWVlnDhxgr/97W+89NJL5OXlTcWtSJAUQggfkyAphBDTb9JvxE6nU6scgrtCWFtby/z58+nq6qKkpITZs2djMBhGPcfHH3/M3LlzCQgImFCIVFWV+vp6LBYL8+fPH3esxliGNuvxLiuNjY0dNq7DO94jNTWV2NjYSV9rovcztFo3NNg6nU6t4tvV1UVERIRWARxvaah3tMj7FRZ215waLbIsOZK0wB6uWpZJSoKJg8c7tP2XjZ5OsXNmhBEZ7M/e2nbWz4vjxxtn4zdOALTZbFpjoYGBAaKjozEajbS3tzMwMMCcOXOmPET29fVRVFQ0LESeTlVVdu7cyWOPPYaiKPj5+fG///u/UpEUQoh/ARIkhRBi+vksSPb19VFRUUF8fDzHjh0jLy9v3Grh4cOHycrKIigoaNwQ6XK5KC8vR1VVcnJyfBpGhi4rNZvN6PV6wsPDMZvNzJ079xPjPXxNVVUqKysZHBwc97l5A7C3AhgYGKgF4PFGntidLg4e7+DdI81sL2/F6s6LzJ8ZwZos9xLYdGMINZZ+3q8w85cDTVh67Vy/OIHHLs9Cd4ZLer0V4JqaGvr7+7Wq6kRma06WN0Tm5uYSEREx6nGNjY1cf/31PPvssyxbtoyOjg4CAwMnPErkDEiQFEIIH5MgKYQQ089nQdJms7Fv3z7CwsImXC0sLi5m1qxZWhOc0UKkw+GgpKSEmJgYkpOTp2SP4lAnTpygurqa4OBgVFXVlmqGhYX5/Nre/ZfBwcGkp6ef8fm9AdhisQy719EaC3nnX86dO5cTfQo7KtxLYEtOdAOQGBXEhZkGmjsH2FFp5dbzE3nokoxJPW9vQHa5XGRnZw+7V71er82BPJPlyWPp7++nqKiIOXPmjBkim5ubufbaa3n66adZuXKlT649BgmSQgjhYxIkhRBi+k36jdjlcuFwOAD3fsni4mI6OzvJz8+fUOhQVZXjx4/T2NhITEwMsbGxIzbA6evro6Sk5JwsLwX3HsUTJ05oHWa9zXrMZjO9vb1j3uuZ8n7fjEajT/Zf2u127V6HNhaKjIxEp9PR3t5ORUXFiPMvW7ttWgfYD4+1MW9mBOenRvO1C1MmHSIrKioAyM7O/sQ5bDabVgG22WyfuNcz5Q2ROTk5Y1aQT548yXXXXcdTTz3FmjVrzvg6kyBBUgghfEyCpBBCTL+zDpLeX+ATExOpr68fdS7kUE6nU1vKqqoqbW1ttLa20tnZSWRkpNYAp6uri6NHj467TNEXvOM9ent7mTt37oh7DyfTrGc0drudwsJCZs2axYwZM3z1NEa914CAAPr7+1m4cOG4FcA+uxOX6iIscHLLT1VV5ejRo/j5+U2oy+3p9xoWFqbtAZ1IZXtgYIDCwsJxQ6TZbOaaa67hxz/+MevWrTvj5zVJEiSFEMLHJEgKIcT0O6sg2draSllZGXPmzCE6OnrUuZDaxcbpzKqqqtap1DsDMj09nfj4ePz8/CZ7qxN6LmVlZfj7+5OVlTXhiurpzXpMJhMmk2nc8OMN35mZmWM2I/KVkydPcuzYMQwGAx0dHQQEBGh7FYOCgnx6LVVVKS0tJSgoaFJLdU+fA6nX67V7DQkJ+cTx3hA5e/ZsoqKiRj2v1Wrlmmuu4Xvf+x5XXHHFmT6tsyFBUgghfEyCpBBCTL9JvxF3dnZy+PBhFixYoC2THCtIekOk0+lEp9ON2Zn12LFjdHV1kZKSgtVqxWKxEBQUpDWVGW9W4ZnwxXiPkZr1eEPl6UtIvfMv58yZM+VNfMC9VPfkyZPk5eVpYbyvr0+7V5fL5bM9oN79nqGhoaSlpfnk/gcGBrTluqePQfGOmBkvRHZ0dHDNNdfw8MMPs2nTJp/c1xmQICmEED4mQVIIIabfWVUkbTbbsP1sowVJVVVxOp2oqjpmUx2n06lVs05fEtnb20tra6sW1GJjYzGZTGdVURsYGKCoqIiUlBTi4uImfZ6RzusNag6HA6PRSGxsLHa7naqqqjOefzlZx48fp62tjfnz5485J/P0PaAmk4moqKgz2qvocrkoKSkhIiKC1NRUXz2FYYaOQens7MRmszFr1iySk5NHrVh3dXVx7bXXcs8993DddddNyX2NQ4KkEEL4mARJIYSYfpN+I1ZVFbvdPuxzH374IcuXLx8WACcaIm02G8XFxcTHx5OYmDjmtQcGBrRQ6XQ6MZlMxMbGnlE481YGc3JyxqxmnS1vUGtoaKC7u5v4+Hji4+N90qxnNN6qrne/50QDocvl0vYqdnR0aHsVjUbjmEuLXS4XxcXFREdHk5yc7KunMSqbzcahQ4dITEzEZrNhtVrx9/fXKqveKnBPTw/XXXcdd911FzfddNOU39coJEgKIYSPSZAUQojp59MguX//fhYvXqyFjqFNdcYKTT09PRw5cmRSewa9nUpbWlqw2Wxa9S88PHzUa1qt1nNaGWxsbKSlpYW5c+fS3d2tNRaKiIjQmsqcabOe0XhHbjidTnJyciYdVofuVbRYLPj7+2vLdYdWgZ1Op9Z5NikpySfPYSx2u53Dhw+TmZlJTEyM9vn+/n6tsnrixAm2bdtGRUUFd9xxB1/60pem/L7GIEFSCCF8TIKkEEJMv7N6I7bZbMM+PnjwIPPmzSMgIGDMpjpDWSwWqqurfRLqBgcHsVqttLa20tPTM+KojhMnTtDU1KSN95hK3spgT0/PJzrBepv1eJvKeJv1nM0eUJfLxdGjR/H3959Qt9Qz0d/fry3XdTqdGAwGDAYDNTU1xMXFjVtF9gVviMzIyBjzDw5ms5k777yTvr4+Ojs7ufvuu7nrrrum/P5GIUFSCCF8TIKkEEJMv7N6I7bb7Qx9Lz98+DBZWVkEBQVNKEQ2NDTQ0tLC/PnzfR7qvMs0h44VcTqdOJ1O5s2b57MK4GhUVaW8vByA2bNnjxvqvHtALRYLOp1u1GY9oxm6RzElZXKzHyfK4XDQ2tpKdXU1iqJo+1Wjo6MnNQNyIiYaIm02GzfffDNXXHEFX/va13C5XHR1dREdHT0l9zUBEiSFEMLHJEgKIcT082mQLC4uJikpSev+OVZn1oqKCgYHB5kzZ86UhQ8vp9NJUVGRVkENCwsjNjYWg8EwJWNFnE4nR44cISwsjLS0tDMOdaM16xmtq6r3+ZlMpnOyvHRwcJDCwkJmzpxJXFwc7e3tmM1m2tvbCQ0N1SqrE5kBORHemZtpaWkYjcYxj7v11lvJz8/n3nvvndIwfQY+FTchhBCfJRIkhRBi+vksSKqqyvHjx2lsbMRkMhEXFzdi8BkcHKSkpITIyEhSU1On/Jf9wcFBiouLMRgM2ngP7z7FqRgr4h0nEhsb65NQ53A4tOW6I3VVdTgcFBUVkZCQQEJCwllfbyL3U1hYyKxZsz7R6VZVVXp6erR9lWONQTmT6x0+fJjU1FRMJtOYx91+++2cd955PPTQQ5+WEAkSJIUQwuckSAohxPTzSZAc2lTH6XRisVi04GMwGIiLiyMiIoKBgQGKi4tJTk5mxowZvnoOoxp6vdHGe/hyrIjNZqOoqGjM652N05frhoaG0t3dTXp6+jn5fnorgykpKcTGxo57/OmVVe8MyIiIiAkFvYmGyMHBQe666y5ycnL47ne/e9Yh8vbbb2fr1q3ExsZy5MgRAB588EH+/ve/ExAQQHp6Os899xxRUVHU1dWRk5NDdnY2AOeffz7PPPPM0NNJkBRCCB+TICmEENPvrIPk4ODgqPshvXP/Wltb6ejoYHBwkPT0dBITE6e8YuQd7zF79uwJ7487m7EifX19FBcXk5WVNayb6FTp7+/n0KFDhIeH09fX5/PK6um8exTT09PHXF46Gm8jJLPZTHd3N1FRUZhMJmJiYkZc2uytfCYnJ48ZWp1OJ1//+teZNWsWP/jBD3zyutq1axdhYWF88Ytf1ILku+++y9q1a/Hz8+Phhx8G4Kc//Sl1dXWsX79eO24EEiSFEMLHfL8pRQghxDmjqio7duxg6dKlBAYGjvgLvLfC513ymJaWRkdHB/v27SMyMpK4uLgpadDiHe8xf/78M+oEGxQUxKxZs5g1a5Y2VqSysnLcsSJdXV2UlpYyd+5cwsPDffpcRtLb20tJSQm5ubnaDExvZbWoqGhYA5zJLikdymazUVhYOG6jm7H4+fkRFxdHXFwcLpeLjo4OzGYzVVVVhISEDOtY692DOV6IdLlc3HfffcTFxfHEE0/47I8TF154IXV1dcM+t27dOu3f559/Pps3b/bJtYQQQpw5qUgKIcT0m/QbcX9/P/fddx+7d+9m0aJFbNy4kbVr1xIYGHjq5KpKXV0dHR0dzJs3T2ts4w0Sra2ttLe3ExERoTW/OdtQ6R3vMX/+/GH3cjbGGivS1tZGVVUVeXl5Pglt4/FWWscKrTabDbPZTGtr64Sa9YxlYGCAwsLCKau0qqpKb2+vtq/Se83k5GRtT+tIXC4XDz74IAEBAfziF7/w+R8jxqo0Xnnlldxwww184QtfoK6ujtzcXLKysoiIiOCHP/whq1atGnq4VCSFEMLHJEgKIcT0O+s3YqfTye7du9m8eTM7duxg3rx5bNy4kRUrVvDKK6+wZs0asrKyRv1F3ztPsaWlhba2Nq2jqtFoPKMRHaqqUltbS1dX15SO9xi6T9FqteJ0OsnOziYuLm7Ku892dHRQXl5+RjM3BwcHh+1ZPb1Zz1j6+/spKipi9uzZWuVzKg0ODnLo0CHCwsKw2WzYbDZtX2VkZKQWgl0uF9/+9rex2+385je/mZLv+2hB8kc/+hEHDx7ktddeQ1EUbDYbPT09GAwGPv74YzZt2kRpaSkRERHeh0iQFEIIH5OlrUII8Rmg1+tZvXo1q1evxuVysX//fv70pz9x9913s2jRIm1YfVhY2IiPVxSFqKgooqKiUFWV7u5uWlpaqK2tJTg4WFuiOdaYDpfLRXl5OTqdjry8vCndf6nT6TAajfT19dHf309ycjJWq5W6uropHSviXa57ppVPPz8/ZsyYwYwZM3C5XLS3t9PS0kJFRQXh4eHa/Z4evL17PnNycoiMjPTpcxmJdznrrFmztMZB3j22TU1NHD16FD8/P6qqqigtLaWnp4ff/e53Ux7eh3r++efZunUr27dv115jgYGBWuV78eLFpKenU1lZyZIlS87ZfQkhxL8bqUgKIcT08/kbsaqqXHLJJdretc2bN7Nt2zaSk5PZuHEjn/vc5yYUTLxLHltaWrBYLAQEBBAXF4fJZBo2n9A73iMmJobk5OQpb+Kjqio1NTX09/eTm5urBRlvCPZWKgMDA33W/Ka1tZW6ujoWLFjgs0Y6qqrS1dWl3e/QZj3eESa5ublDK2tTxul0UlhYSEJCAvHx8aPeb01NDQ888ACHDx9mxYoVfOUrX+Hyyy+fkns6vSK5bds27r//fnbu3Dmsg6zZbCYmJga9Xs+xY8dYtWoVJSUlQ5cBS0VSCCF8TIKkEEJMvyl5I7bb7cMCj8vl4siRI7z66qu8/fbbxMXFsXHjRq644ooJ77sbOqbDz8+P2NhYIiIiKC8vH1bFmkoul4ujR4+i1+vJzs4eM7SePlbE2wH2TMeKNDc309TURF5e3rAA7WvefYonT56kr6+PxMREkpKSpnzf50RCJLiD5M9+9jOOHj3KCy+8QE1NDR0dHSxbtszn93TjjTdSUFCAxWIhLi6O73//+/zkJz/RltrCqTEfW7Zs4bHHHsPf3x+dTsf3v/99rrzyyqGnkyAphBA+JkFSCCGm3zl/I1ZVlaNHj7J582a2bt1KVFQUGzZs4MorrxxzVuBQ/f39NDQ00NjYSEhICAkJCZMKaWfC6XRSUlJCZGQkKSkpZ1T5nOxYkYaGBsxmM3l5eVO253MobyOf7Oxs+vr6hjXrMZlMI3asPRtOp5OioiJmzJhBQkLCqMepqsrTTz/NRx99xEsvvTSlgXoKSJAUQggfkyAphBDTb1rfiFVVpbq6ms2bN/P3v/+doKAgrrzySjZu3EhcXNyooaWtrY3Kykrmzp2Lv78/ra2ttLa24nK5tJAWEhLis/t0OBwUFRURHx/PzJkzz+pc3rEiLS0tY44VGdo46FzsA/SOMDl9ZIq3WY/ZbKanp4fo6GitY+3Z3Jc3RMbFxY35PVVVlWeffZaCggI2b948JTMyp5gESSGE8DEJkkIIMf0+NW/E3lEhW7Zs4W9/+xs6nY7169ezadMmZs6cqYWs5uZmGhoayMvL+8R4D7vdjtlspqWlBYfDoYXK0Rr9TMTAwABFRUWkpaVNuGI6USONFTGZTFgsFhwOBzk5OeckRHZ2dlJWVkZeXt6YAdzbrKe1tZWOjo4xm/WMxel0UlxcjMlkIjExcdTjVFXlj3/8I2+//Tavv/76WVecb7/9drZu3UpsbKy297GtrY0bbriBuro6UlJSeOWVV4iOjkZVVe655x7efvttQkJCeP7551m0aNFkLitBUgghfEyCpBBCTL9P5RuxqqqcOHGCLVu28Nprr2Gz2Vi/fj319fUsWrSIG2+8cdyuqA6HQ6v8DQwMYDQaiYuLO6NZir29vZSUlJyT8Rculwur1UplZSUOh4PY2FhiY2OJiYmZ0jDpHSlypt1gvc16vPMfg4KCMJlMmEymMauGLpeLoqKicUMkwJ/+9Ce2bNnCm2++6ZO9mrt27SIsLIwvfvGLWpB86KGHiImJ4ZFHHuHJJ5+kvb2dn/70p7z99ts8/fTTvP322+zfv5977rmH/fv3T+ayEiSFEMLHJEgKIcT0+9S/EauqSlNTEzfeeCNms5mIiAiuuOIKNm7cSGZm5oRCobfy19LSQl9fHzExMcTFxRERETHq471Vunnz5p1VRXOiXC4XpaWlhISEkJqaSmdnJ62trcNma/p6rEh7ezsVFRUsWLDgrKt93mY9ZrMZRVG0UDm0wulyuSguLsZgMJCUlDTm+V566SVeeOEFtm7dOuGZmRNxejfW7OxsCgoKiI+Pp7m5mfz8fCoqKrjrrrvIz8/nxhtv/MRxZ0iCpBBC+JjMkRRCCDEuRVGoq6vjiiuu4OGHH8ZqtfK3v/2NRx99FLPZzOc+9zk2btxITk7OqKHQz8+PuLg44uLitNmEDQ0NdHd3Ex0dTVxcHFFRUdrjLRYL1dXVLFiwYMq7lsKppZ7eESYA0dHR2hJL71iR2traYWM6zma/oNVqpbq6moULF35iifBkhIaGEhoaSkpKCjabDbPZTEVFBXa7HYPBgNFopLa2dkIhcsuWLfzf//2fz0PkSFpaWrRwOGPGDFpaWgBoamoadp+JiYk0NTVNJkgKIYTwMQmSQgghJmTlypWsXLkSAKPRyB133MEdd9xBe3s7b775Jj/4wQ+or6/nkksu4aqrrhqzQY1er9eWjbpcLtra2mhubqa8vJzIyEj8/Pzo6Ohg0aJF56Sxy+DgoNa5dKSmM4qiEBERQUREBBkZGdpYkcLCwkmPFbFYLNTU1LBw4cIpeY6BgYEkJiaSmJjI4OAgZrOZ4uJiAIKDg2lraxu1Wc+bb77Js88+y9atWwkPD/f5vY1FUZQpn0MqhBDi7EmQFEIIcVaio6O59dZbufXWW+nq6mLr1q387Gc/o6qqiosuuohNmzaxaNGiUUOlTqfDaDRiNBpxuVyUl5fT0tKCXq+nqqpKW046VXsU7XY7RUVFJCUlTXgOZmhoKKmpqaSmpmpjRUpLSyc8VsRsNlNbWztlIfJ0Op2O1tZWkpOTSUpK0pr1VFRUEB4ejslkwmg0otfr2bZtG//93//NW2+9NeV7Ur3i4uJobm7WlrbGxsYCMHPmTBoaGrTjGhsbz7pjrxBCCN+QPZJCCDH9PpNvxL29vbz99tts2bKF0tJS8vPz2bRpE+edd96I3UVVVaWqqgq73c6cOXNQFIXOzk5aWlqG7VH0Bh5fsNlsFBYWkp6ejtFoPOvzTWSsSEtLC/X19SxYsOCczGJ0uVwcOXKEiIgIUlJShn1taLOel19+mXfffRer1cpbb71FTk7OlN3T6XskH3zwQQwGg9Zsp62tjf/6r//irbfe4te//rXWbOcb3/gGBw4cmMwlpcQphBA+JkFSCCGm32f+jXhgYIB3332XV199lcOHD7Ny5Uo2bdrEBRdcgJ+fH06nk6NHjxIYGEhGRsYnljZ69yi2tLRgtVoJDg4mNjYWk8k06cY3/f39FBUVkZ2dTXR0tC+e5jAjjRXx8/PDarWycOHCcxYiS0tLCQsLIzU1dcxjd+3axeOPP866desoKCjggQceYMOGDT6/pxtvvJGCggIsFgtxcXF8//vfZ9OmTVx//fXU19eTnJzMK6+8QkxMDKqqcvfdd7Nt2zZCQkJ47rnnWLJkyWQuK0FSCCF8TIKkEEJMv3+rN2K73c727dvZvHkz+/btY8mSJZSVlfHYY49x0UUXjft4VVXp7e2lpaUFi8VCQEAAcXFxmEymCYeznp4eSkpKyM3NJSIi4myf0rhcLhdVVVW0tLTg5+dHVFTUlI8VUVWVI0eOEBoaSlpa2pjH7tmzh4cffpitW7eSkJAwJfcz1VwuFzqdjra2Nvz9/U/f2ylBUgghfEyCpBBCTL9/2zfi5uZm1q1bx8yZM2lsbGTx4sVs3LiRNWvWTLiLqbfxjdlsxs/PT6tUjvb4rq4uSktLz9lIEXB3Hz158iQLFixAp9PR0dExpWNFVFWltLSU4OBg0tPTxzx2//793H///bz55pvjdnL9tPKGyOLiYh599FF+8IMfsGjRoqGHSJAUQggfkyAphBDT79/2jfiDDz6gt7eXyy67DKfTyQcffMDmzZspKChg3rx5bNq0iYsvvnjC4z/6+/tpbW2ltbUVRVG0zrDebqremY15eXnnZKQIQENDAxaLhfnz539ib+fQsSIWi8UnY0VUVaWsrIygoKBxQ+ShQ4f4+te/zhtvvPGJ/ZNno6KightuuEH7+NixYzzxxBN0dHTw+9//HpPJBMCPf/xjLr/8cp9cs76+nmuuuYbvfve7bNiwAafTSXt7u3fvqwRJIYTwMQmSQggx/eSN+DROp5N9+/axZcsW3nvvPbKysrjqqqtYt27dhGca2mw2LVS6XC5CQkLo6upi0aJFPpnZOBH19fW0tbUxf/78CS1hHVpdncxYEW+IDAwMJD09fcwxGkVFRdx111289tprZGRkTPg5nSmn08nMmTPZv38/zz33HGFhYTzwwAM+v87f//53XnnlFR577DH279/Pm2++id1u55FHHuH888+XICmEED42NRszhBBC+Ny2bdvIzs4mIyODJ5988hNft9ls3HDDDWRkZLBs2TLq6urO/U36iF6vZ8WKFfz85z+nsLCQhx9+mOLiYi655BJuuukmXn75Zbq6usY8R2BgIElJSSxevJj4+Hja29sJCAigsLCQY8eO0dPTM6XPoa6ujvb29gmHSDg1VuS8884jNzcXgNLSUg4cOEBtbS29vb2jPlZVVY4ePUpAQMC4IbK0tJS77rqLV199dUpDJMD27dtJT08nOTnZp+d1Op2Au6kRwKJFi4iLi2PNmjWcPHmSG264gZUrV2K1Wn16XSGEEG5SkRRCiOk37hux0+kkKyuLf/7znyQmJrJ06VJefPFF5syZox3zP//zPxQXF/PMM8/w0ksv8frrr/Pyyy9P6Y2fay6Xi5KSEl599VXefvtt4uPj2bhxI1dcccWonVe9+xPz8vLw8/PD4XBoIzoGBgYwGo3ExcURFhY2Zvg6E96gOnfuXJ800/GOFWltbdXueehYEVVVKS8vR6/Xk5mZOebzKC8v57bbbuPFF1/UwupUuv3221m0aBF33303jz/+OM8//zwREREsWbKEn/3sZ5PqmOvdE9nc3MwTTzyBXq/n6quvJi0tjfDwcAwGAxaLhcsuu4zvfOc7bNq0SSqSQgjhYxIkhRBi+o37Rrx3714ef/xx/vGPfwDwk5/8BIBHH31UO+bSSy/l8ccfZ/ny5QwODjJjxgzMZrPPwtGnjXcZ5+bNm3nrrbeIjo5mw4YNrF+/XtuDd+zYMbq6upg3b96Isye9IzpaWlro6+sjJiaGuLg4IiIiJvV9U1WVmpoaBgYGyM3NnZLv/eljRaKjo7HZbAQGBpKdnT3mNaurq/nCF77ACy+8QF5ens/v7XR2u52EhARKS0uJi4ujpaUFo9GIoih897vfpbm5mT/+8Y9ndE5VVVEUha6uLq655hrWrVuH1Wqlv7+fxMREbrvtNjo6Orjxxhu5+eabue+++0D2SAohhM/5pj2cEEKIKdXU1DSso2ZiYiL79+8f9Rg/Pz8iIyOxWq3eZiOfOYqikJubS25uLo899hhVVVVs3ryZz3/+8wQFBREeHk5ycjI//vGPRwyR4P4+xcXFERcXh9PpxGq10tDQQHd3N9HR0cTFxREVFTWhQKiqKtXV1djt9ikLkSPdc3FxMf39/fT29nL06NFRx4rU1dVxyy238Nxzz52TEAnwzjvvaEtOAe1/Ae68807Wr19/xudUFAWHw8GTTz6JXq/nwQcfBODdd9/lD3/4A5/73OeIjIzkJz/5CZdccon3MYoqfzkXQgifkj2SQggh/uUpikJWVhbf+ta32L17NykpKVitVg4dOsQVV1zBb37zG5qamhgrS+j1emJjY5k7dy7Lli3DaDTS3NzMvn37KCsrw2q14nK5RnysqqpUVlYyODjInDlzzkkV2Btcg4ODWb58OcuXLyc+Ph6r1cr+/fspKSmhpaWF3t5eGhoauOmmm/jd737H4sWLp/zevF588UVuvPFG7ePm5mbt36+//jpz586d8LmGfu87OzuJiIigqqqKP//5zwCsW7eOyMhIduzYQVJSkhYiXS4XEiKFEML3pCIphBD/AmbOnElDQ4P2cWNjIzNnzhzxmMTERAYHB+ns7MRgMJzrW512fX19LFiwQFsy2dTUxJYtW7jjjjtwOBxceeWVbNy4keTk5FEDn06nw2g0YjQacblc2tzHyspKIiIitLmPOp1O25+o0+mYPXv2OQuRVVVVuFyuYdeMjo4mOjpaGyvS1NTE5ZdfTnd3N1/84henvLHOUL29vfzzn//k2Wef1T730EMPUVhYiKIopKSkDPvaWJxOJ3q9HrvdTnFxMRERETz00ENERUXxwQcf0NnZyVe/+lXKyspYsWLFsMf6Yo+qEEKIT5I9kkIIMf3GfSMeHBwkKyuL7du3M3PmTJYuXcpf//rXYc1SfvOb31BSUqI123nttdd45ZVXpvTG/5WoqkpLSwuvvfYar732Gt3d3VxxxRVs3LiRjIyMCS9f7ezspLW1FavVSlhYGDabjfDwcLKyss5pJdLhcJCTkzPmNU+ePMl1113HPffcQ3NzM0ePHuX555+f8nucCg6Hg/z8fFavXs2zzz7LT37yEy677DK2bdvGj370IzIyMvjc5z432mgR2SMphBA+JkFSCCGm34TeiN9++23uvfdenE4nt99+O9/+9rd57LHHWLJkCRs2bGBgYIBbbrmFw4cPExMTw0svvURaWtqo59u2bRv33HMPTqeTO+64g0ceeWTY13/+85/zv//7v/j5+WEymfjjH//o8xEO08lsNvO3v/2NLVu2YLFYuPzyy9mwYcO44czL5XJRVFSEw+HA5XIRHBxMbGwsJpMJP7+pWfDjbeZjt9vHvU+z2czVV1/NT37yE9atWzcl93Mu3XvvvcyaNYv777+f7OxsfvjDH3LdddfR39/PX/7yFwoLC8nPz+faa68d6eESJIUQwsckSAohxPQ752/EExknsmPHDpYtW0ZISAi//e1vKSgo+MyNE/Fqa2vjzTff5LXXXqOhoYF169Zx1VVXjTq+w+VyceTIEcLCwkhLS0NVVXp7e2lpacFisRAQEEBcXBwmkwl/f3+f3KM3RNpstnH3YVqtVq655hoef/xxLr/8cp9c/1zr6uoiIiJC+/gvf/kLiqLwzDPPcPXVV3Pvvfdy6NAhVFUlJyeHF198kR07dvC9732PzMzM008nQVIIIXxMgqQQQky/c/5GPJFxIkMdPnyYu+++mz179pyze5wunZ2dbN26lddee43q6mouvvhiNm3axMKFC9HpdDidTo4cOUJkZCQpKSkjnqO3t5fW1lbMZjN+fn5apTIwMHDS91VTU0N/f/+4HWE7Ojq4+uqrefTRR9m4ceOkrzealJQUwsPD0ev1+Pn5cfDgQdra2rjhhhuoq6sjJSWFV155ZVLzIb0aGhrYsWMHubm5/PnPf+baa6/lwIED/OpXv+Lhhx/mP/7jPwC45JJLWLVqFY899hhms5n29naysrJGOqUESSGE8DFptiOEEP+GJjJOZCjvWIV/B5GRkdx8883cfPPN9PT08Pbbb/PrX/+asrIyLrzwQgoLC/nOd74z5giN0NBQUlNTSU1Npb+/n9bWVoqLi1EUhdjYWGJjYwkKCprwPR07dmxCIbKrq4vrrruOBx54YEpCpNeOHTuGjZV58sknueiii3jkkUd48sknefLJJ/npT3866fMHBgZitVq56qqrmD9/PitWrGDFihUcOHCAPXv20NDQQGFhISkpKTz22GMAmEwmbX6oEEKIqSdBUgghxJj+/Oc/c/DgQXbu3Dndt3LOhYWFcf3113P99dfT1tbGunXrCA0N5cEHH2TVqlVs2rSJ5cuXj7knMjg4mOTkZJKTk7HZbLS2tlJaWorL5cJkMhEbG0tISMioj6+traW3t5e5c+eOGSJ7enq44YYbuPvuu0fbJzhl3njjDQoKCgC49dZbyc/Pn1SQ9HZnjY2NZdasWYSFhZGSksLevXtZvnw5L774Iv/3f/9HQEAAKSkp3HnnncMeJ4QQ4tyRICmEEP+GJjJOBOC9997jRz/6ETt37jyrZZmfBeXl5Xzta1/j9ttvx2azsX37dl566SXuv/9+LrjgAjZu3MiqVavG3BMZGBhIUlISSUlJ2O12zGYz5eXlDA4OYjQaiY2NJSwsTDu+traW7u7ucUNkX18fn//857n99tuHzW2cCoqisG7dOhRF4a677uIrX/kKLS0txMfHAzBjxgxaWlrO+LxDw+Cjjz7Kww8/zJIlS9i6dSt//OMfcTqdrFy5kvj4+GHNg1wul4RIIYSYBrJHUgghpt85fyOeyDiRw4cPc+2117Jt27aRmpcID4fDQUFBAZs3b2b37t0sXbqUjRs3kp+fP+Hw7XA4sFgstLS0MDAwgNFoxOl0MjAwwLx588achdjf38+NN97Iddddp1XoplJTUxMzZ86ktbWVSy65hKeffpoNGzbQ0dGhHRMdHU17e/ukzn/zzTdjs9l48cUX8ff3p6mpiZdffpmioiKOHDnCypUr+e///u8zPa3skRRCCB+TKb1CCPFvyM/Pj1//+tdceuml5OTkcP3115Obm8tjjz3Gm2++CcCDDz5IT08P1113HQsWLGDDhg3jnnfbtm1kZ2eTkZHBk08+OepxW7ZsQVEUDh486LPnNF38/f255JJLePbZZykqKuK2225j+/btrFq1ijvuuIOtW7fS398/7jni4+NZsGABS5YsYWBggJaWFvr6+qiurqazs5OR/vBrs9m45ZZb2LRpE3fcccdUPcVhvJXr2NhYrrrqKg4cOEBcXBzNzc0ANDc3ExsbO6lzv//++6iqyubNm2lpaeEXv/gFRUVFXHXVVdx+++1cffXVkwmRQgghpoBUJIUQYvp9Jt6IJzJSBKC7u5srrrgCu93Or3/9a5YsWTJNdzy1nE4ne/fuZcuWLbz33nvMnj2bTZs2afssR1NfX097ezvz5s1DVVWsViutra10d3cTHR1NXFwcUVFROBwObr31VtasWcM999wzodmXZ6u3txeXy0V4eDi9vb1ccsklPPbYY2zfvh2DwaA122lra+O//uu/xj2fy+UaVm398MMP+eY3v6kt5e3p6eHIkSP85je/YdWqVdpxk9gTKRVJIYTwMdkjKYQQwicOHDhARkYGaWlpAHz+85/njTfe+ESQ/O53v8vDDz/M//t//286bvOc0ev1rFy5kpUrV+JyuTh48CCbN2/mv/7rv0hLS2Pjxo1cdtllw2Yl1tfX09bWxvz587WA5e3y6nK5aGtro7m5mauvvhqbzcaSJUv42te+dk5CJEBLSwtXXXUV4F4efdNNN3HZZZexdOlSrr/+ev7whz+QnJzMK6+8Mu65hobIN954g5iYGBYtWsR3vvMdTpw4wYYNG4iLi+PLX/4ylZWVw4Kk7IkUQojpJ0FSCCGET0xkpMihQ4doaGjgiiuu+MwHyaF0Oh3nnXce5513Hk8++STFxcW8+uqr/OpXvyIhIYGNGzdSX19PSkoKN9xww4h7InU6HUajkaioKG2WY2BgICtWrGDPnj0EBARM+fNIS0ujqKjoE583GAxs3779jM7lfY4PPvgg1dXVzJ07ly996UscPnxYq3jed999HD9+nD/84Q8+uX8hhBC+I0FSCCHEOeFyubj//vt5/vnnp/tWppVOp2PBggUsWLCAH/7wh5SWlvLoo49SWFjInDlzsNlsrF+/fticRi+n08ndd99NZmYmP/jBD1AUBVVVz1lF0tfeffddmpqaeP3117nrrrtYu3Yt4eHhDA4OcvToUcxmM//4xz8AGfEhhBCfNtJsRwghhE+MN1Kku7ubI0eOkJ+fT0pKCvv27WPDhg2fiYY7k6UoCoGBgaiqSmVlJb/61a9ob2/n+uuvZ/369fz+97/n5MmTqKqKy+XivvvuY8aMGTzxxBNaePxXCpGn92UwGAwsWLCAb3zjGwwMDPC73/0OgOeee46FCxfy5z//Gb1eLyFSCCE+haTZjhBCTL/PxBvxREaKDJWfn89TTz31mW22cyZOryqqqkptbS1btmzhb3/7mxaiFi5cyC9+8Ysxx4FMRkNDA1/84hdpaWlBURS+8pWvcM899/D444/z+9//HpPJBMCPf/xjLr/88kldY+hzbGtrIyYmhra2NlauXAlAWVkZAN/73vcoKSnhlVdewc/PZwun/nXSthBC/IuQiqQQQgifmMhIkcmayFiRV155hTlz5pCbm8tNN910Vtc7106vKiqKQlpaGg8++CC7d+/mz3/+M0uWLJmSEAnun93PfvYzysrK2LdvH7/5zW+0YHffffdRWFhIYWHhpEMknHqOP/zhD7n00kt59NFHKS8v55133gHgK1/5CnfccQcffPABzz//PH5+fiOOPBFCCPHpIBVJIYSYfvJGPIaJjBWpqqri+uuv5/333yc6OprW1tZJzzIUsHHjRu6++2727NlDWFgYDzzwwKTPNbQSWVJSwk9/+lPuvvtuDhw4QElJCddeey3Lli3jrbfeAmD9+vVERkb6ejmrVCSFEMLHpNmOEEIITWNjI2+88Qa33XbbmLMOz6WJjBX5/e9/z9e//nWio6MBJESehbq6Og4fPsyyZcvYs2cPv/71r/nTn/7EkiVL+NnPfqZ9jydiaIjctm0bxcXFZGVlcf755zNr1iwiIiLYvHkzVquVm2++WXuc7IkUQohPP1naKoQQAoCOjg4ee+wx/vM//5Pq6urpvh3NSGNFmpqahh1TWVlJZWUlK1as4Pzzz2fbtm3n+jY/E3p6erjmmmv45S9/SUREBF/96lepqamhsLCQ+Ph4vvnNb07oPKevdvrTn/7EPffcQ1VVFb/61a+oqqoiISGB9evXs3TpUoqLi+nv79eOlxAphBCfflKRFEIIAcAvf/lL2tra+M///E9KS0vJy8vTvuZyuQCmZH+eLwwODlJVVUVBQQGNjY1ceOGFlJSUEBUVNd239i/D4XBwzTXXcPPNN3P11VcDEBcXp339zjvvZP369RM6l9VqxWg0oigK+/fv5/nnn+e9994jKSmJmTNnctVVV/HSSy8xd+5crrvuOoKCgggODv6XHmUihBD/bj6dvxEIIYQ4p/76179SWlrKU089hcViwWazAdDX18fg4CA6nW7aQuR4Y0XAXaXcsGED/v7+pKamkpWVRVVV1bm+1X9Zqqry5S9/mZycHO6//37t883Nzdq/X3/9debOnTvuuTo7O/n617+O2WxGVVWOHj1Kc3Mzzz33HACPP/44N998MxdffDFHjhwhOjqa4OBg4F9rlIkQQvy7k4qkEEL8mzt27Bi//OUveeaZZ8jIyECv15OVlQW4m9h8//vfJzMzk9zcXK655ppheye98w2ncini0qVLqaqqora2lpkzZ/LSSy/x17/+ddgxmzZt4sUXX+RLX/oSFouFyspKbU+lGN+ePXt44YUXmDdvHgsWLADcoz5efPFFCgsLURSFlJQUnn322THPMzg4SGRkJC+++CIfffQR//znP7X9tu+++y7PPPMM//Ef/8Gjjz5KaGgoYWFh5+DZCSGEmAoSJIUQ4t9Yd3c3zzzzDEVFRdx5550sX74cvV7P0aNHWbFiBaWlpezatYsvfOELvPLKK7z22mts2bJFC46Komj/9jZIsdvtBAQE+Oweh44VcTqd3H777dpYkSVLlrBhwwYuvfRS3n33XebMmYNer+f//b//h8Fg8Nk9fNatXLlyxFEbZzLuw2638/LLL3PJJZdw/PhxnnjiCY4ePUpvby933nknANu3b+epp57igQce4Bvf+AbgXjb9aV0yLYQQYnQSJIUQ4t/YI488QmBgIA0NDZjNZl5//XWefPJJ1q1bB8ChQ4f46le/ytVXX01ubi533XUXg4OD9Pb28vrrr/PCCy9w3XXX8YUvfEGrVL7zzjt88MEHPPTQQz7rnnr55Zd/ItQ88cQT2r8VReHnP/85P//5z8/ovNu2beOee+7B6XRyxx138Mgjjwz7en19PbfeeisdHR04nU6efPLJs5ql+FkWEBBAVFQUCxcuJDg4mAMHDnDy5EluueUWBgcH+epXv4rT6WT37t1YLBaMRiPw6d13K4QQYmwSJIUQ4t/Y9ddfT0pKCrGxscTGxpKbm8vBgwdJSEigtraW6upqHn30UQB2797NeeedR0dHB08//TRFRUV861vfYvPmzRQUFLB48WLuv/9+mpqasNvtWoj0Ln/V6XSfqj1wTqeTr3/968PmU27YsGHYWJEf/vCHXH/99Xz1q1+lrKyMyy+/nLq6uum76U+55cuXExsbS19fHy0tLcydO5f/+Z//4Zvf/Cb9/f3cf//9fO5znyMyMnK6b1UIIcRZkj8DCiHEv7HVq1eTnJwMuANfV1cXS5cuZfv27Rw+fBin08myZcsAOHr0KLNmzcJsNlNeXs63vvUt1q5dy/r16ykrKyMrK4vu7m4qKyu1fXZ2u11b/vppCpEwfD5lQECANp9yKEVR6OrqAtxNZBISEqbjVn1i27ZtZGdnk5GRwZNPPjkl1zAajezatYuf/vSn3HjjjfzjH/9g+fLlfOUrX+G3v/0tra2tEiKFEOIzQiqSQgghAHdoioiI4Nvf/jYNDQ2UlZVx/vnnA3DkyBHMZjNXXHEFg4ODNDY2snz5chwOB0ajkYSEBFatWkVRUREdHR2sW7eO48eP8+qrr/LPf/6T8847j2984xuYTKZPXHe6Rj6MNJ9y//79w455/PHHWbduHU8//TS9vb2899575/o2fWIi1VdfiYyM5Oqrr6a3t5evfe1rfPGLX6SgoIBXXnnFZ0udhRBCTD+pSAohhPiEpKQkLr30Ur797W8D7qY8iYmJJCcn09nZqVWV7HY7f/vb30hKSiI6OpqioiIMBgOJiYncfPPNZGVl8bvf/Y6goCBefvnlEa/1aatUDvXiiy9y22230djYyNtvv80tt9yizdT8VzKR6quv3XLLLfzP//wPNTU1fO9732PhwoUjNvQRQgjxr0kqkkIIIUY0tJvm8uXLWb58OaqqMmvWLGJiYli4cCGrV69m586dfP/736e7u5uamhry8/PZvXs3H3/8MT/+8Y/Jy8tjzpw5/OIXv+Duu+8GTlUhn332WW666SbCw8PPeWVyIvMp//CHP7Bt2zbtezAwMIDFYvmXq6xNpPo6FS699FLWrl2Lv7//tFWehRBCTA1F/joohBBiPIqi6FVVdSqKEgAsAD4C4oF7gWTgP4CFwB3AN4H/BPpUVf2hoii3ABsAs6qqX1MUxU9V1UFFUQzAy8C9qqoe8VwnXVXVmtOvDbhUH/8HS1EUP6ASuAho8jynm1RVLR1yzDvAy6qqPq8oSg6wHZjp63uZaoqiXAtcpqrqHZ6PbwGWqap69/TemRBCiH9VsrRVCCHEuFRVdXr+GQRcClQAvwbicAetdqAPKAdOAgW4Ayaqqr6gqup1qqp+zfPxoOd/rcB+IA9AUZQrgL8oipKsKMp8RVEu8F7bG9w8X1ur+KC05bmPu4F/AEeBV1RVLVUU5QlFUTZ4DvsmcKeiKEXAi8Bt44VIRVH+qChKq6IoR0b5uqIoyq8URalWFKVYUZRFZ/tcJqAJSBrycaLnc0IIIcSkSEVSCCHEGVMUxR9YAzQNreAN+XoE8FsgE3dYLFBVdcuQryuqqqqKonwAPAIcB34C7FFV9RlFUdbhDnFxwK9VVf1fT2XyJtxh7iJvmPy0VQcVRbkQ6AH+pKrq3BG+fjnuiu3lwDLgv1VVXTbF9zRu9VUIIYQ4E7JHUgghxBlTVdUBvDv0c4qi6FRVdXlCYhdws6Ios4H1Q45RVA9FUYJxh5pu4Je4l43+2XOoFdgIpAM3KooShvu/WfOArZ57GBYgvdf3/bM9M6qq7lIUJWWMQzbiDpkqsE9RlChFUeJVVW2ewnsaVBTFW33VA3+UECmEEOJsSJAUQgjhE94Q5wmJOlVVXaqqluNe7uo9Rh3y735FUSqB/wYGVFX9raIoJkVRvgPMBlJwh8xI4GdADJADPKMoyiW492i+rqpq99Drw6nAOrXPeNJmAg1DPm70fG7KgiSAqqpvA29P5TWEEEL8+5A9kkIIIXzOG+o8+wHH+m9NMmAEfuD5+HNAKu5llwuAbUCDZw/mEqBfVdVjwAVAFuC9zmOKoiQMub53T6X8d04IIYSYAvIfWCGEEFPGs4p11OWmqqreinuf4F7Pp/YBocAViqKswb0M9B3P15YBexRFiQYMwBFVVXsVRZkF3Al4w+NCRVE2KIoS+GlY6joCaXwjhBDiX54ESSGEENPCW61UVbVvyDLUauBN4Au4G8MsBrZ4wmIq7j1+C4BBoMrzmGuBfwItiqJ8HrgFd2Vzt6Iot52jp3Mm3gS+6Hn+5wOdU7k/UgghhJgKskdSCCHEtPCER/W0z7mA3wG/UxQlCNitqmqzZw5ijKqq5Z55jlGqqn7sedh64GlgJe5uqFZVVTcoirIeWHSuu7sqivIikA8YFUVpBL4H+Hvu4Rnc+xQvxx2a+4AvnYv7EkIIIXxJxn8IIYT4VPGM+XANDX6ecSOzVVUtURRlKfB/wMO4l8E+i7sJz2W4u7p24g5yycBB4F5VVWXpqBBCCOFDUpEUQgjxqaKqqnOEzzmAEk/znI9xV/nWAHnAXlVVTyiKogKpqqpuAlAUJRFIkBAphBBC+J5UJIUQQvzL8lQv4zxBMgP4f7iXjv5DVdX66b07IYQQ4rNLmu0IIYT4l6Mois4zK9KpquoJAFVVq4FfAyuAbYqi3KYoSsi03qgQQgjxGSUVSSGEEJ9JiqJEqKraNd33IYQQQnwWSZAUQgjxmeHZQ6mMtM9SCCGEEL4jQVIIIYQQQgghxBmRPZJCCCGEEEIIIc6IBEkhhBBCCCGEEGdEgqQQQgghhBBCiDMiQVIIIYQQQgghxBmRICmEEEIIIYQQ4oxIkBRCCCGEEEIIcUb+P6d5Ig1zblSlAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as pl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def create_kde_plot(successful_actions_list, unsuccessful_actions_list):\n",
    "    # Create a figure and axis\n",
    "    #fig, ax = plt.subplots()\n",
    "    timesteps = 140\n",
    "    precision = 20\n",
    "    fig = pl.figure()\n",
    "    ax = fig.add_subplot(1,1,1,projection='3d')\n",
    "    fig.suptitle('Kernel Density Estimation on angle variable successful vs unsuccessful distrib for the 140 trial at each timestep', fontsize=16, y=1.04)\n",
    "    \n",
    "    Z = np.zeros((timesteps,precision))\n",
    "    Z_u = np.zeros((timesteps,precision))\n",
    "    X = np.linspace(0,1,precision)\n",
    "    Y = np.ones((Z.shape[0], X.shape[0]))\n",
    "    for timestep in range(timesteps):\n",
    "        successful_actions = successful_actions_list[:,timestep]\n",
    "        unsuccessful_actions = unsuccessful_actions_list[:,timestep]\n",
    "        \n",
    "        # Create a kernel density estimate plot for successful actions\n",
    "        #sns.kdeplot(successful_actions, color='blue', ax=ax)\n",
    "        kde = gaussian_kde(successful_actions)\n",
    "        Z[timestep] = kde.evaluate(X)\n",
    "        # Create a kernel density estimate plot for unsuccessful actions\n",
    "        #sns.kdeplot(unsuccessful_actions, color='red', ax=ax)\n",
    "        kde_u = gaussian_kde(successful_actions)\n",
    "        Z_u[timestep] = kde_u.evaluate(X)\n",
    "\n",
    "        Y[timestep] = (timestep+1)*Y[timestep]\n",
    "        # Add a title to the plot\n",
    "        #ax.set_title(f'Timestep {timestep} Action Space Distribution')\n",
    "        #if timestep >= 84 and timestep <=89:\n",
    "            #ax.plot(X, Y[timestep], Z[timestep], color='b')\n",
    "        #else:\n",
    "            #ax.plot(X, Y[timestep], Z[timestep], color='r')\n",
    "        #ax.plot(X, Y[timestep], Z[timestep], color='g')\n",
    "        # Show the plot\n",
    "    ax.plot_wireframe(X, Y, Z, rstride=5, cstride=5, color = 'r', label='Successful trials')\n",
    "    ax.plot_wireframe(X, Y, Z_u, rstride=5, cstride=5, label='Unsuccessful trials')\n",
    "    ax.set_xlabel(\"Angle value\")\n",
    "    ax.set_ylabel(\"timesteps\")\n",
    "    ax.set_zlabel(\"KDE\")\n",
    "    ax.legend() \n",
    "    plt.show()\n",
    "    fig.savefig('analysis/action_kde/'+initial+'_kde_angle.png', facecolor='w',bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "successful_actions_list = s_a[:,:,0]\n",
    "unsuccessful_actions_list = u_a[:,:,0]\n",
    "create_kde_plot(successful_actions_list, unsuccessful_actions_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation of the distribution of successful actions vs unsuccessful action for each timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_kde_plot(timestep, successful_actions, unsuccessful_actions):\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Create a kernel density estimate plot for successful actions\n",
    "    sns.kdeplot(successful_actions, color='blue', ax=ax)\n",
    "    \n",
    "    # Create a kernel density estimate plot for unsuccessful actions\n",
    "    sns.kdeplot(unsuccessful_actions, color='red', ax=ax)\n",
    "    \n",
    "    # Add a title to the plot\n",
    "    ax.set_title(f'Timestep {timestep} Action Space Distribution')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "for timestep in range(140):\n",
    "    successful_actions_list = s_a[:,:,0]\n",
    "    unsuccessful_actions_list = u_a[:,:,0]\n",
    "    successful_actions = successful_actions_list[:,timestep]\n",
    "    unsuccessful_actions = unsuccessful_actions_list[:,timestep]\n",
    "    create_kde_plot(timestep, successful_actions, unsuccessful_actions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-test for each timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:  (14, 140, 2) (235, 140, 2)\n",
      "shapes:  (14, 140) (14, 140) (235, 140) (235, 140)\n",
      "140\n",
      "t-test x:  -1.9193730025214988 -1.1495346547466563 p-values x:  0.07187112048403925 0.266442697366411\n",
      "t-test z:  -1.818440995312596 3.307213290659873 p-values z:  0.0031988508449180427 0.9989895622538534\n",
      "index max t-test min p value:  27 27\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "print(\"shapes: \", s_a.shape, u_a.shape)\n",
    "# generate sample data\n",
    "np.random.seed(0)\n",
    "successful_actions_x_axis = s_a[:,:,0]    #np.random.normal(0, 1, (250, 100))\n",
    "successful_actions_z_axis = s_a[:,:,1]  \n",
    "unsuccessful_actions_x_axis = u_a[:,:,0]# np.random.normal(1, 1, (250, 100))\n",
    "unsuccessful_actions_z_axis = u_a[:,:,1]\n",
    "print(\"shapes: \", successful_actions_x_axis.shape, successful_actions_z_axis.shape, unsuccessful_actions_x_axis.shape, unsuccessful_actions_z_axis.shape)\n",
    "# calculate t-statistic and p-value for each timestep\n",
    "t_statistics_x = []\n",
    "p_values_x = []\n",
    "t_statistics_z = []\n",
    "p_values_z = []\n",
    "print(successful_actions_x_axis.shape[1])\n",
    "for t in range(successful_actions_x_axis.shape[1]):\n",
    "    t_statistic_x, p_value_x = ttest_ind(successful_actions_x_axis[:,t], unsuccessful_actions_x_axis[:,t], equal_var = False, nan_policy='omit')\n",
    "    t_statistics_x.append(t_statistic_x)\n",
    "    p_values_x.append(p_value_x)\n",
    "\n",
    "    t_statistic_z, p_value_z = ttest_ind(successful_actions_z_axis[:,t], unsuccessful_actions_z_axis[:,t], equal_var = False, nan_policy='omit')\n",
    "    t_statistics_z.append(t_statistic_z)\n",
    "    p_values_z.append(p_value_z)\n",
    "\n",
    "print(\"t-test x: \", min(t_statistics_x), max(t_statistics_x), \"p-values x: \", min(p_values_x),  max(p_values_x))\n",
    "print(\"t-test z: \", min(t_statistics_z), max(t_statistics_z), \"p-values z: \", min(p_values_z),  max(p_values_z))\n",
    "print(\"index max t-test min p value: \", np.asarray(t_statistics_z[60:]).argmax(), np.asarray(p_values_z[60:]).argmin())\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"timesteps\")\n",
    "ax.set_ylabel(\"t-test\")\n",
    "fig.suptitle('t-test on angle variable successful vs unsuccessful distrib for the 140 trial at each timestep', fontsize=16, y=1.04)\n",
    "ax.plot(np.arange(1, successful_actions_x_axis.shape[1]+1), t_statistics_x, color='blue', label='t_test value')\n",
    "ax.plot(np.arange(1, successful_actions_x_axis.shape[1]+1), p_values_x, color='red', label='p value')\n",
    "ax.legend() \n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_xlabel(\"timesteps\")\n",
    "ax2.set_ylabel(\"t-test\")\n",
    "fig2.suptitle('t-test on cuevel x-axis variable successful vs unsuccessful distrib for the 140 trial at each timestep', fontsize=16, y=1.04)\n",
    "ax2.plot(np.arange(1, successful_actions_z_axis.shape[1]+1), t_statistics_z, color='blue', label='t_test value')\n",
    "ax2.plot(np.arange(1, successful_actions_z_axis.shape[1]+1), p_values_z, color='red', label='p value')\n",
    "ax2.legend() \n",
    "\n",
    "\n",
    "#fig.savefig('analysis/action_t-test/'+initial+'_angle.png', facecolor='w',bbox_inches='tight')\n",
    "#fig2.savefig('analysis/action_t-test/'+initial+'_cuevel_x.png', facecolor='w',bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Analysis all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/TD3_BC/RL_dataset/Offline_reduced/\"\n",
    "filename = []\n",
    "for file in sorted(os.listdir(path)):\n",
    "\tfilename.append(path+file)\n",
    "\n",
    "df = pd.read_csv(filename[0], header = 0, \\\n",
    "\t\t\tnames = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]', r'\\r'], '', regex=True) \n",
    "\n",
    "states = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "actions = pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "new_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "trial = df['trial'].astype(int)\n",
    "terminals = df['terminals'].astype(bool)\n",
    "\n",
    "#Train/Test split\n",
    "trial_ind = np.arange(1,len(trial.unique()))\t#+1 if we want 250 index as well\n",
    "train_trial = np.random.choice(trial_ind, size=int(0.8*len(trial_ind)), replace=False)  #distrib proba for each value, could be useful to weight more \"important\" trajectories\n",
    "test_trial = np.delete(trial_ind, train_trial-1)\n",
    "\n",
    "train_ind = trial.isin(train_trial)\n",
    "test_ind = trial.isin(test_trial)\n",
    "\n",
    "train_set = {'trial': trial[train_ind],\n",
    "\t\t\t\t'states': states[train_ind],\n",
    "\t\t\t\t'actions': actions[train_ind],\n",
    "\t\t\t\t'new_states': new_states[train_ind],\n",
    "\t\t\t\t'rewards': df['rewards'][train_ind],\n",
    "\t\t\t\t'terminals': terminals[train_ind]}\n",
    "\n",
    "test_set = {'trial': trial[test_ind],\n",
    "\t\t\t\t'states': states[test_ind],\n",
    "\t\t\t\t'actions': actions[test_ind],\n",
    "\t\t\t\t'new_states': new_states[test_ind],\n",
    "\t\t\t\t'rewards': df['rewards'][test_ind],\n",
    "\t\t\t\t'terminals': terminals[test_ind]}\n",
    "\n",
    "#print(filename[0], len(train_set['trial'].unique()), train_set['terminals'].value_counts())\n",
    "\n",
    "\n",
    "### If multiple files are passed ###\n",
    "if len(filename) > 1:\n",
    "\tfor i, file in enumerate(filename):\n",
    "\t\tif i > 0:\n",
    "\t\t\tdf = pd.read_csv(file, header = 0, \\\n",
    "\t\t\t\t\tnames = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "\t\t\tdf = df.replace([r'\\n', r'\\[', r'\\]', r'\\r'], '', regex=True) \n",
    "\t\t\n",
    "\t\t\tstates = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "\t\t\tactions = pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "\t\t\tnew_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "\t\t\ttrial = df['trial'].astype(int)\n",
    "\t\t\tterminals = df['terminals'].astype(bool)\n",
    "\t\t\t#Train/Test split\n",
    "\t\t\ttrial_ind = np.arange(1,len(trial.unique()))\n",
    "\t\t\ttrain_trial = np.random.choice(trial_ind, size=int(0.8*len(trial_ind)), replace=False)  #distrib proba for each value, could be useful to weight more \"important\" trajectories\n",
    "\t\t\ttest_trial = np.delete(trial_ind, train_trial-1)\n",
    "\n",
    "\t\t\ttrain_ind = trial.isin(train_trial)\n",
    "\t\t\ttest_ind = trial.isin(test_trial)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\ttrain_set['trial'] = pd.concat([train_set['trial'],trial[train_ind]+ (i)*250 ], axis=0)\n",
    "\t\t\ttrain_set['states'] = pd.concat([train_set['states'], states[train_ind]], axis=0)\n",
    "\t\t\ttrain_set['actions'] = pd.concat([train_set['actions'], actions[train_ind]], axis=0)\n",
    "\t\t\ttrain_set['new_states'] = pd.concat([train_set['new_states'], new_states[train_ind]], axis=0)\n",
    "\t\t\ttrain_set['rewards'] = pd.concat([train_set['rewards'], df['rewards'][train_ind]], axis=0)\n",
    "\t\t\ttrain_set['terminals'] = pd.concat([train_set['terminals'], terminals[train_ind]], axis=0)\t\n",
    "\n",
    "\t\t\ttest_set['trial'] = pd.concat([test_set['trial'], trial[test_ind]+ (i)*250 ], axis=0)\n",
    "\t\t\ttest_set['states'] = pd.concat([test_set['states'], states[test_ind]], axis=0)\n",
    "\t\t\ttest_set['actions'] = pd.concat([test_set['actions'], actions[test_ind]], axis=0)\n",
    "\t\t\ttest_set['new_states'] = pd.concat([test_set['new_states'], new_states[test_ind]], axis=0)\n",
    "\t\t\ttest_set['rewards'] = pd.concat([test_set['rewards'], df['rewards'][test_ind]], axis=0)\n",
    "\t\t\ttest_set['terminals'] = pd.concat([test_set['terminals'], terminals[test_ind]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = test_set['rewards']\n",
    "terminals = test_set['terminals']\n",
    "trial = test_set['trial']\n",
    "states = test_set['states']\n",
    "actions = test_set['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "#for i in np.array(trial.unique()):\n",
    "'''\n",
    "ind = actions[terminals == True]\n",
    "for i in ind.index:\n",
    "        print(\"rewards terminal state: \", rewards[i])\n",
    "print(\"reward ind.index[12]: \", rewards[ind.index[12]])\n",
    "a = ind[rewards != -10.0]\n",
    "successful_trial_list = trial[a.index]\n",
    "'''\n",
    "\"\"\"\n",
    "terminal_rewards = test_set['rewards'][test_set['terminals'] == True]\n",
    "successful_rewards = terminal_rewards[terminal_rewards != -10.0]\n",
    "successful_trial_list = test_set['trial'].iloc[successful_rewards.index]\"\"\"\n",
    "#terminal_rewards = rewards[terminals == True]\n",
    "successful_rewards_ind = []\n",
    "for i in range(len(rewards)):\n",
    "        if terminals.iloc[i] == True:\n",
    "                if rewards.iloc[i] != -10.0:\n",
    "                        successful_rewards_ind = np.append(successful_rewards_ind, i)\n",
    "successful_trial_list = trial.iloc[successful_rewards_ind]\n",
    "\n",
    "fig1, action_pattern_x = plt.subplots()\n",
    "fig2, action_pattern_z = plt.subplots()\n",
    "\n",
    "for i in successful_trial_list:\n",
    "        action_pattern_x.plot(np.arange(1, len(actions[trial == i])+1), actions[trial == i][0])\n",
    "        action_pattern_z.plot(np.arange(1, len(actions[trial == i])+1), actions[trial == i][1])\n",
    "\n",
    "action_pattern_x.set_xlabel(\"timesteps\")\n",
    "action_pattern_x.set_ylabel(\"Impulse force\")\n",
    "fig1.suptitle('Impulse force along x-axis at each timestep \\n (30 timesteps before hit, 20 timesteps after)', fontsize=16, y=1.04)\n",
    "action_pattern_z.set_xlabel(\"timesteps\")\n",
    "action_pattern_z.set_ylabel(\"Impulse force\")\n",
    "fig2.suptitle('Impulse force along z-axis at each timestep \\n (30 timesteps before hit, 20 timesteps after)', fontsize=16, y=1.04)\n",
    "\n",
    "\n",
    "\n",
    "#unsuccessful_trial_list = trial.drop(a.index, axis=0)\n",
    "successful_actions = np.zeros((len(successful_trial_list), len(actions[trial == 19]), actions[trial==1].shape[1]))\n",
    "for i, trial_ind in enumerate(successful_trial_list):\n",
    "        successful_actions[i][:][:] = actions[trial == trial_ind]\n",
    "\n",
    "s_a = np.zeros((len(successful_actions), 50, actions.shape[1]))  #trial 1 is always complete (250 elements)\n",
    "u_a = np.zeros((len(trial.unique())-len(successful_actions), 50, actions.shape[1]))\n",
    "j = 0\n",
    "k = 0\n",
    "\n",
    "episode_len = 50\n",
    "for i, trial_ind in enumerate(trial.unique()):\n",
    "        pad_len = episode_len - actions[trial==trial_ind].shape[0]\n",
    "        if np.isin(trial_ind, successful_trial_list):\n",
    "                a = actions.iloc[i*episode_len:(i+1)*episode_len][:]\n",
    "                if pad_len > 0:\n",
    "                        pad_array = np.array([pad_len*[np.nan]])\n",
    "                        a = np.pad(a, pad_width = ((pad_len,0), (0,0)),  mode = 'constant', constant_values = np.nan)\n",
    "                s_a[j][:][:] = a\n",
    "                j+=1\n",
    "        else:\n",
    "                a = actions.iloc[i*episode_len:(i+1)*episode_len][:]\n",
    "                if pad_len > 0:\n",
    "                        pad_array = np.array([pad_len*[np.nan]])\n",
    "                        a = np.pad(a, pad_width = ((pad_len,0), (0,0)),  mode = 'constant', constant_values = np.nan) \n",
    "                u_a[k][:][:] = a\n",
    "                k+=1 \n",
    "successful_mean = np.nanmean(s_a, axis=0, keepdims=True)      #numpy.nanmean()\n",
    "successful_std = np.nanstd(s_a, axis=0)\n",
    "unsuccessful_mean = np.nanmean(u_a, axis=0)     #unsuccessful_trial_list\n",
    "unsuccessful_std = np.nanstd(u_a, axis=0)         #unsuccessful_trial_list\n",
    "\n",
    "#print(successful_mean.shape[0], successful_mean[:][1])\n",
    "successful_mean = successful_mean.reshape(-1)\n",
    "successful_mean = successful_mean.reshape(50,2)\n",
    "\n",
    "unsuccessful_mean = unsuccessful_mean.reshape(-1)\n",
    "unsuccessful_mean = unsuccessful_mean.reshape(50,2)\n",
    "\n",
    "\n",
    "fig3, mean_x = plt.subplots()\n",
    "#mean_x.plot(np.arange(1, len(actions[trial == 1])+1), successful_mean[:,0])   #linestyle='None', marker='^')\n",
    "mean_x.set_xlabel(\"timesteps\")\n",
    "mean_x.set_ylabel(\"Impulse force\")\n",
    "fig3.suptitle('Mean of Successful trial x-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_x.errorbar(np.arange(1, 51), successful_mean[:,0], successful_std[:,0], color='blue', ecolor='orange')#, linestyle='None', marker='^')\n",
    "\n",
    "fig4, mean_x = plt.subplots()\n",
    "mean_x.set_xlabel(\"timesteps\")\n",
    "mean_x.set_ylabel(\"Impulse force\")\n",
    "fig4.suptitle('Mean of Unsuccessful trial x-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_x.errorbar(np.arange(1, 51), unsuccessful_mean[:,0], unsuccessful_std[:,0], color='blue', ecolor='red')  # linestyle='None', marker='^')\n",
    "\n",
    "fig5, mean_z = plt.subplots()\n",
    "mean_z.set_xlabel(\"timesteps\")\n",
    "mean_z.set_ylabel(\"Impulse force\")\n",
    "fig5.suptitle('Mean of Successful trial z-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_z.errorbar(np.arange(1, 51), successful_mean[:,1], successful_std[:,1], color='blue', ecolor='orange')     #, linestyle='None', marker='^')\n",
    "\n",
    "fig6, mean_z = plt.subplots()\n",
    "mean_z.set_xlabel(\"timesteps\")\n",
    "mean_z.set_ylabel(\"Impulse force\")\n",
    "fig6.suptitle('Mean of Unsuccessful trial z-axis Impulse Force over trials \\n 30 timesteps before hit, 20 timesteps after', fontsize=16, y=1.04)\n",
    "mean_z.errorbar(np.arange(1, 51), unsuccessful_mean[:,1], unsuccessful_std[:,1], color='blue', ecolor='red')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "fig1.savefig('analysis/'+initial+'_30_action_Succesful_pattern_x.png', bbox_inches='tight')\n",
    "fig2.savefig('analysis/'+initial+'_30_action_Successful_pattern_z.png', bbox_inches='tight')\n",
    "\n",
    "fig3.savefig('analysis/'+initial+'_30_action_Successful_pattern_mean_x.png', facecolor='w',bbox_inches='tight')\n",
    "fig4.savefig('analysis/'+initial+'_30_action_Unuccessful_pattern_mean_x.png', facecolor='w',bbox_inches='tight')\n",
    "\n",
    "fig5.savefig('analysis/'+initial+'_30_action_Successful_pattern_mean_z.png', facecolor='w',bbox_inches='tight')\n",
    "fig6.savefig('analysis/'+initial+'_30_action_Unsuccessful_pattern_mean_z.png', facecolor='w',bbox_inches='tight')\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ BL\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.1756835, 0.7613559, -0.6845667) (0.6869871, 0.0, 1.3739742)\n",
      "Success Derivation function\n",
      "0 fake successes removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward function\n",
      "start hit timesteps function\n",
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "\n",
      "Imported BL as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n"
     ]
    }
   ],
   "source": [
    "############################# Define Path and Load Dataset ######################################\n",
    "path = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\"\n",
    "path2 = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 2/\"\n",
    "initial = \"BL\"\n",
    "data = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "#save_raw_data(data, initial)\n",
    "#save_RL_reduced_dataset(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced raw data  BL  saved\n"
     ]
    }
   ],
   "source": [
    "save_raw_data(data, initial)    #print(\"Success \", i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline RL Dataset function\n",
      "34103\n",
      "BL reduced Offline dataset saved\n"
     ]
    }
   ],
   "source": [
    "save_RL_reduced_dataset(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ AAB\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.3164156, 0.8362059, -0.6821803) (0.6704871, 0.0, 1.3409743)\n",
      "Success Derivation function\n",
      "9 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported AAB as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  AAB  saved\n",
      "Offline RL Dataset function\n",
      "34859\n",
      "AAB reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ AS\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.4028162, 0.8118547, -0.7396733) (0.6758776, 0.0, 1.3517552)\n",
      "Success Derivation function\n",
      "2 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported AS as reward subject for Right pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  AS  saved\n",
      "Offline RL Dataset function\n",
      "34099\n",
      "AS reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ BL\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.1756835, 0.7613559, -0.6845667) (0.6869871, 0.0, 1.3739742)\n",
      "Success Derivation function\n",
      "0 fake successes removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward function\n",
      "start hit timesteps function\n",
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported BL as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  BL  saved\n",
      "Offline RL Dataset function\n",
      "34103\n",
      "BL reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ CP\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.3678348, 0.8106964, -0.7550356) (0.6725239000000001, 0.0, 1.3450478000000001)\n",
      "Success Derivation function\n",
      "5 fake successes removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward function\n",
      "start hit timesteps function\n",
      "Imported CP as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  CP  saved\n",
      "Offline RL Dataset function\n",
      "34834\n",
      "CP reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ CX\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.3662902, 0.812062, -0.6972519) (0.6802926, 0.0, 1.3605851)\n",
      "Success Derivation function\n",
      "0 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported CX as reward subject for Right pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  CX  saved\n",
      "Offline RL Dataset function\n",
      "33879\n",
      "CX reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ GS\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.3678348, 0.8106964, -0.7550356) (0.6725239000000001, 0.0, 1.3450478000000001)\n",
      "Success Derivation function\n",
      "0 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported GS as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  GS  saved\n",
      "Offline RL Dataset function\n",
      "28894\n",
      "GS reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ HZ\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.37075, 0.7698231, -0.6155565) (0.6766082, 0.0, 1.3532163)\n",
      "Success Derivation function\n",
      "9 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported HZ as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  HZ  saved\n",
      "Offline RL Dataset function\n",
      "33904\n",
      "HZ reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ IK\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.4873467, 0.8013444, -0.7570558) (0.675297, 0.0, 1.350594)\n",
      "Success Derivation function\n",
      "0 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported IK as reward subject for Right pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  IK  saved\n",
      "Offline RL Dataset function\n",
      "34999\n",
      "IK reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ JW\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.3365659, 0.8669214, -0.6844468) (0.6689447, 0.0, 1.3378893)\n",
      "Success Derivation function\n",
      "7 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported JW as reward subject for Right pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  JW  saved\n",
      "Offline RL Dataset function\n",
      "34999\n",
      "JW reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ KO\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.492529, 0.7993889, -0.7085299) (0.6812078, 0.0, 1.3624156)\n",
      "Success Derivation function\n",
      "4 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported KO as reward subject for Right pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  KO  saved\n",
      "Offline RL Dataset function\n",
      "34859\n",
      "KO reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ LR\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.1852261, 0.7212601, -0.6580337) (0.6802861, 0.0, 1.3605721000000002)\n",
      "Success Derivation function\n",
      "9 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported LR as reward subject for Right pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  LR  saved\n",
      "Offline RL Dataset function\n",
      "34859\n",
      "LR reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ MA\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.4028162, 0.8118547, -0.7396733) (0.6758776, 0.0, 1.3517552)\n",
      "Success Derivation function\n",
      "2 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported MA as reward subject for Right pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  MA  saved\n",
      "Offline RL Dataset function\n",
      "33332\n",
      "MA reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ MAI\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.3678348, 0.8106964, -0.7550356) (0.6725239000000001, 0.0, 1.3450478000000001)\n",
      "Success Derivation function\n",
      "7 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported MAI as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  MAI  saved\n",
      "Offline RL Dataset function\n",
      "33728\n",
      "MAI reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ MG\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.3845704, 0.8063962, -0.739677) (0.682193, 0.0, 1.3643861)\n",
      "Success Derivation function\n",
      "0 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported MG as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  MG  saved\n",
      "Offline RL Dataset function\n",
      "34788\n",
      "MG reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ MO\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.3845704, 0.8063962, -0.739677) (0.682193, 0.0, 1.3643861)\n",
      "Success Derivation function\n",
      "0 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: either missed a start-hit index, or a trial was not properly recorded\n",
      "Imported MO as reward subject for Left pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n",
      "reduced raw data  MO  saved\n",
      "Offline RL Dataset function\n",
      "33739\n",
      "MO reduced Offline dataset saved\n",
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ PM\n",
      "Result Single Subject function\n",
      "scaling params:  (-0.1852261, 0.7212601, -0.6580337) (0.6802861, 0.0, 1.3605721000000002)\n",
      "Success Derivation function\n",
      "0 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619/582337986.py:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "############################# Define Path and Load Dataset ######################################\n",
    "\n",
    "path = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\"\n",
    "path2 = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 2/\"\n",
    "pathlist = [path, path2]\n",
    "## WARNING check which type of feedback in which round, and which corner for each subject\n",
    "#corner =  \"left\"\n",
    "#data = resultsSingleSubject(initial, \"reward\", path, corner)\n",
    "'''dic for one subject composed of ~1000 timepoints for one shot, 25 shots in one block, and 10 blocks\n",
    "First 3 blocks are baseline learning, then 6 blocks of adaptation to perturbation, and one final washout block\n",
    "That is 250 shots per subjects, 300'564 points in the dictionnary'''\n",
    "\n",
    "# Environment State Properties\n",
    "'''\n",
    "corner = \"all\"\n",
    "state_dim=14\n",
    "action_dim=2\n",
    "max_action = 1\n",
    "normalize = True\n",
    "# Agent parameters\n",
    "kwargs = {\n",
    "\t\"state_dim\": state_dim,\n",
    "\t\"action_dim\": action_dim,\n",
    "\t\"max_action\": max_action,\n",
    "\t\"discount\": args['discount'],\n",
    "\t\"tau\": args['tau'],\n",
    "\t# TD3\n",
    "\t\"policy_noise\": args['policy_noise'] * max_action,\n",
    "\t\"noise_clip\": args['noise_clip'] * max_action,\n",
    "\t\"policy_freq\": args['policy_freq'],\n",
    "\t# TD3 + BC\n",
    "\t\"alpha\": args['alpha']\n",
    "\t}\n",
    "'''\n",
    "\"\"\"\n",
    "#dataset = pd.read_csv(\"RL_dataset/AAB.csv\")\n",
    "for i, initial in enumerate(sorted(os.listdir(path2))):\n",
    "\tpathSubj = path2 + str(initial)\n",
    "\tfor fil in range(len(sorted(os.listdir(pathSubj + '/Game/')))):\n",
    "\t\tif sorted(os.listdir(pathSubj + '/Game/'))[fil].find(\"Block2\") > -1:\n",
    "\t\t\tblockFile = sorted(os.listdir(pathSubj + '/Game/'))[fil]\n",
    "\t\t\t\n",
    "\t\t\tif blockFile.find('Reward') > -1:\n",
    "\t\t\t\tprint(path, initial)\n",
    "\t\t\t\tdata = resultsMultipleSubjects(path2, initial, 'reward', 'all')\n",
    "\t\t\t\tsave_raw_data(data, initial)\n",
    "\t\t\t\tsave_RL_reduced_dataset(initial)\n",
    "\"\"\"\t\t\t\t\t\t\n",
    "for path in pathlist:\n",
    "\tfor i, initial in enumerate(sorted(os.listdir(path))):\n",
    "\t\t#if initial != \"AAB\" and initial != \"AS\":\n",
    "\t\tpathSubj = path + str(initial)\n",
    "\t\tfor fil in range(len(sorted(os.listdir(pathSubj + '/Game/')))):\n",
    "\t\t\tif sorted(os.listdir(pathSubj + '/Game/'))[fil].find(\"Block2\") > -1:\n",
    "\t\t\t\tblockFile = sorted(os.listdir(pathSubj + '/Game/'))[fil]\n",
    "\t\t\t\t\n",
    "\t\t\t\tif blockFile.find('Reward') > -1:\n",
    "\t\t\t\t\tdata = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "\t\t\t\t\tsave_raw_data(data, initial)\n",
    "\t\t\t\t\tsave_RL_reduced_dataset(initial)\n",
    "\n",
    "#initial = \"BL\"\n",
    "#data = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "#data = resultsMultipleSubjects([path, path2], 'reward', 'all')\n",
    "\n",
    "#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "#replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "#replay_buffer.convert_D4RL(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_data(data, initial):\n",
    "    reward_ = []\n",
    "    #rewards = []\n",
    "    cueballpos = []\n",
    "    cueballvel = []\n",
    "    redballpos = []\n",
    "    targetcornerpos = []\n",
    "    cueposfront = []\n",
    "    cueposback = []\n",
    "    cuedirection = []\n",
    "    cuevel = []\n",
    "\n",
    "    total_len_trajectories = 0\n",
    "    \n",
    "    k=0\n",
    "    for i in range(data[initial][\"start_ind\"].shape[0]):\n",
    "        #reward_ = []\n",
    "        for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]):\n",
    "            total_len_trajectories += 1\n",
    "            if j == data[initial][\"hit_ind\"][i]-1:\n",
    "                #if i == 12 :\n",
    "                    #print(\"right hit index wtf is going on\", j, k)\n",
    "                #done_bool = True\n",
    "                reward = data[initial][\"rewards\"][i]\n",
    "            else:\n",
    "                #if i == 12:\n",
    "                    #print(\"wrong hit index\", j, k)\n",
    "                #done_bool = False\n",
    "                ## Discounted reward ##\n",
    "                #gamma = 0.99\n",
    "                #reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "                reward = 0\n",
    "            #if i == 12:\n",
    "                #print(reward)\n",
    "            #+=1\n",
    "            reward_.append(reward)\n",
    "        #rewards.append(np.array(reward_))\n",
    "        cueballpos.append(np.array((data[initial][\"cueballpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cueballvel.append(np.array((data[initial][\"cueballvel\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballvel\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballvel\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueballvel\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        redballpos.append(np.array((data[initial][\"redballpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"redballpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"redballpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"redballpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        targetcornerpos.append(np.array((data[initial][\"targetcornerpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"targetcornerpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"targetcornerpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"targetcornerpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cueposfront.append(np.array((data[initial][\"cueposfront\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposfront\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposfront\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposfront\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cueposback.append(np.array((data[initial][\"cueposback\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposback\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposback\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cueposback\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cuedirection.append(np.array((data[initial][\"cuedirection\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuedirection\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuedirection\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuedirection\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "        cuevel.append(np.array((data[initial][\"cuevel\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuevel\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuevel\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]], data[initial][\"cuevel\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]])))\n",
    "\n",
    "    #for i in range(len(reward_)):\n",
    "        #print(reward_[i])\n",
    "    dic = {'rewards': np.array(reward_),    #rewards,  \n",
    "    'cueballpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cueballvel': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'redballpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'targetcornerpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cueposfront': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cueposback': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cuedirection': np.zeros(total_len_trajectories, dtype=object),\n",
    "    'cuevel': np.zeros(total_len_trajectories, dtype=object)}\n",
    "\n",
    "    transition_num = 0\n",
    "    #print(\"cuballpos shape 1: \", cueballpos[1].shape[1], cueballpos[1].shape, data[initial][\"start_ind\"][1]-data[initial][\"hit_ind\"][1], rewards[1].shape)\n",
    "    for i in range(data[initial][\"start_ind\"].shape[0]):\n",
    "            for j in range(cueballpos[i].shape[1]):\n",
    "                    dic['cueballpos'][transition_num] = [cueballpos[i][0][j],cueballpos[i][1][j], cueballpos[i][2][j], cueballpos[i][3][j]]\n",
    "                    dic['cueballvel'][transition_num] = [cueballvel[i][0][j],cueballvel[i][1][j], cueballvel[i][2][j], cueballvel[i][3][j]]\n",
    "                    dic['redballpos'][transition_num] = [redballpos[i][0][j], redballpos[i][1][j], redballpos[i][2][j], redballpos[i][3][j]]\n",
    "                    dic['targetcornerpos'][transition_num] = [targetcornerpos[i][0][j], targetcornerpos[i][1][j], targetcornerpos[i][2][j], targetcornerpos[i][3][j]]\n",
    "                    dic['cueposfront'][transition_num] = [cueposfront[i][0][j], cueposfront[i][1][j], cueposfront[i][2][j], cueposfront[i][3][j]]\n",
    "                    dic['cueposback'][transition_num] = [cueposback[i][0][j], cueposback[i][1][j], cueposback[i][2][j], cueposback[i][3][j]]\n",
    "                    dic['cuedirection'][transition_num] = [cuedirection[i][0][j], cuedirection[i][1][j], cuedirection[i][2][j], cuedirection[i][3][j]]\n",
    "                    dic['cuevel'][transition_num] = [cuevel[i][0][j], cuevel[i][1][j], cuevel[i][2][j], cuevel[i][3][j]]\n",
    "                    transition_num += 1\n",
    "            #if i%50 == 0:\n",
    "                    #print(i)\n",
    "    pd_dataset = pd.DataFrame.from_dict(dic)\n",
    "    pd_dataset.to_csv(\"RL_dataset/reduced_data/\"+initial+\"_reduced_data.csv\")\n",
    "    #pd_dataset.to_csv(\"RL_dataset/raw_data/\"+initial+\"_raw_data.csv\")\n",
    "    #pd_dataset.to_csv(\"RL_dataset/\"+initial+\"_raw.csv\")\n",
    "    print(\"reduced raw data \", initial, \" saved\")\n",
    "    #return pd_dataset, dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save RL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_RL_reduced_dataset(initial):\n",
    "        df = pd.read_csv(\"RL_dataset/reduced_data/\"+initial+\"_reduced_data.csv\", header = 0, \\\n",
    "        #df = pd.read_csv(\"RL_dataset/\"+initial+\"_raw.csv\", header = 0, \\\n",
    "                names = ['rewards','cueballpos', 'cueballvel','redballpos', 'targetcornerpos', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel'], usecols = [1,2,3,4,5,6,7,8,9], lineterminator = \"\\n\")\n",
    "        df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "        rewards= pd.DataFrame.from_records(np.array(df['rewards'].astype(str).str.split(','))).astype(float)\n",
    "        cueballpos = pd.DataFrame.from_records(np.array(df['cueballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cueballvel = pd.DataFrame.from_records(np.array(df['cueballvel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        redballpos = pd.DataFrame.from_records(np.array(df['redballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        targetcornerpos = pd.DataFrame.from_records(np.array(df['targetcornerpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cueposfront = pd.DataFrame.from_records(np.array(df['cueposfront'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cueposback = pd.DataFrame.from_records(np.array(df['cueposback'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cuedirection = pd.DataFrame.from_records(np.array(df['cuedirection'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "        cuevel = pd.DataFrame.from_records(np.array(df['cuevel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "\n",
    "        ball_data = {'cueballpos': cueballpos,\n",
    "             'cueballvel': cueballvel,\n",
    "            'redballpos': redballpos,\n",
    "            'targetcornerpos': targetcornerpos\n",
    "            }\n",
    "        cue_data = {'cueposfront': cueposfront,\n",
    "                'cueposback': cueposback,\n",
    "                'cuedirection': cuedirection\n",
    "                }\n",
    "        dataset = Offline_Reduced(ball_data, cue_data, cuevel, rewards[0])\n",
    "        #dataset = Offline_One(ball_data, cue_data, cuevel, rewards[0])  #rewars[0] to get float value from dataframe\n",
    "        # Dataframe does not accept 2-d arrays\n",
    "        # transform 2-d array (n states times 14 dimensions (state)) to 1-d array of list (of length 14)\n",
    "        #dataset_RL = Offline_RL_dataset(data,rewards,cuedirection, cuevel,cueballpos, terminate_on_end=True)\n",
    "        #Offline_RL_load(rewards,cueballpos,redballpos, targetcornerpos, cueposfront, cueposback, cuedirection, cuevel, terminate_on_end=True)\n",
    "        #dataset = Offline_One_big_dict(data, initial)\n",
    "\n",
    "        new_d = {'trial': dataset[\"trial\"],\n",
    "                'states': np.zeros(dataset[\"states\"].shape[0], dtype=object),\n",
    "                'actions': np.zeros(dataset[\"actions\"].shape[0], dtype=object),\n",
    "                'new_states': np.zeros(dataset[\"new_states\"].shape[0], dtype=object),\n",
    "                'rewards': dataset[\"rewards\"],\n",
    "                'terminals': dataset[\"terminals\"]}\n",
    "        for i in range(dataset[\"states\"].shape[0]):\n",
    "                new_d['states'][i] = dataset[\"states\"][i][:].tolist()\n",
    "                new_d['actions'][i] = dataset[\"actions\"][i][:].tolist()\n",
    "                new_d['new_states'][i] = dataset[\"new_states\"][i][:].tolist()\n",
    "\n",
    "        pd_dataset = pd.DataFrame.from_dict(new_d)\n",
    "        pd_dataset.to_csv(\"RL_dataset/Offline_reduced/\"+initial+\"_Offline_reduced.csv\")\n",
    "        #pd_dataset.to_csv(\"RL_dataset/\"+initial+\".csv\")\n",
    "        print(initial, \"reduced Offline dataset saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "initial=\"AS\"\n",
    "df = pd.read_csv(\"RL_dataset/reduced_data/\"+initial+\"_reduced_data.csv\", header = 0, \\\n",
    "        names = ['rewards','cueballpos','redballpos', 'targetcornerpos', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel'], usecols = [1,2,3,4,5,6,7,8], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "rewards= pd.DataFrame.from_records(np.array(df['rewards'].astype(str).str.split(','))).astype(float)\n",
    "cueballpos = pd.DataFrame.from_records(np.array(df['cueballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "redballpos = pd.DataFrame.from_records(np.array(df['redballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "targetcornerpos = pd.DataFrame.from_records(np.array(df['targetcornerpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposfront = pd.DataFrame.from_records(np.array(df['cueposfront'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposback = pd.DataFrame.from_records(np.array(df['cueposback'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuedirection = pd.DataFrame.from_records(np.array(df['cuedirection'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuevel = pd.DataFrame.from_records(np.array(df['cuevel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = {'cueballpos': cueballpos,\n",
    "             'redballpos': redballpos, \n",
    "             'targetcornerpos': targetcornerpos,\n",
    "             'cueposfront': cueposfront, \n",
    "             'cueposback': cueposback,\n",
    "             'cuedirection': cuedirection,\n",
    "              'cuevel': cuevel}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "initial = \"AAB\"\n",
    "df = pd.read_csv(\"RL_dataset/Offline_reduced/\"+initial+\"_Offline_reduced.csv\", header = 0, \\\n",
    "        names = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "states = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "actions= pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "new_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "trial = df['trial'].astype(int)\n",
    "#Train/Test split\n",
    "trial_ind = np.arange(1,len(trial.unique())+1)\n",
    "train_trial = np.random.choice(trial_ind, size=200, replace=False)  #distrib proba for each value, could be useful to weight more \"important\" trajectories\n",
    "test_trial = np.delete(trial_ind, train_trial-1)\n",
    "\n",
    "train_ind = trial.isin(train_trial)\n",
    "test_ind = trial.isin(test_trial)\n",
    "\n",
    "train_set = {'trial': trial[train_ind],\n",
    "                'states': states[train_ind],\n",
    "                'actions': actions[train_ind],\n",
    "                'new_states': new_states[train_ind],\n",
    "                'rewards': df['rewards'][train_ind],\n",
    "                'terminals': df['terminals'][train_ind]}\n",
    "\n",
    "test_set = {'trial': trial[test_ind],\n",
    "                'states': states[test_ind],\n",
    "                'actions': actions[test_ind],\n",
    "                'new_states': new_states[test_ind],\n",
    "                'rewards': df['rewards'][test_ind],\n",
    "                'terminals': df['terminals'][test_ind]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dic):\n",
    "    ind = np.random.randint(1, 250) \n",
    "    return (torch.FloatTensor(dic['states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['actions'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.FloatTensor(dic['new_states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['rewards'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.Tensor(dic['terminals'][dic['trial'] == ind].to_numpy()).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(dic, ind):\n",
    "    return (torch.FloatTensor(dic['states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['actions'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.FloatTensor(dic['new_states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['rewards'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.Tensor(dic['terminals'][dic['trial'] == ind].to_numpy()).to(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool Graphic Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "saved GIF\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "states,_,_,_,_ = get_trajectory(train_set, train_set[\"trial\"].iloc[20])\n",
    "states = states.detach().numpy()\n",
    "print(len(states))\n",
    "#cueposfront = policy.select_action(states).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "def animate(i):\n",
    "\tax.clear()\n",
    "\t#cueposfront = policy.select_action(states)\n",
    "\t#line.set_xdata(cueposfront[i,0].detach().numpy())\n",
    "\t#line.set_ydata(cueposfront[i,1].detach().numpy())  # update the data.\n",
    "\t#line.set_data(cueposfront[i,0].detach().numpy()+i/10, cueposfront[i,1].detach().numpy()+i/10)\n",
    "\tx_values = [states[i,8], states[i,10]]\n",
    "\tz_values = [states[i,9], states[i,11]]\n",
    "\t#line.set_data(x_values, z_values)\n",
    "\tax.plot(x_values, z_values, color=\"blue\", label = 'cue stick')\n",
    "\n",
    "\tx_val = states[i,0]\n",
    "\tz_val = states[i,1]\n",
    "\t#point.set_data(x_values, z_values)\n",
    "\tax.scatter(x_val, z_val, s=60, facecolors='black', edgecolors='black', label = 'cueball')\n",
    "\t\n",
    "\t\n",
    "\tax.scatter(states[i][4], states[i][5], s=60, facecolors='red', edgecolors='red', label = 'target ball')\n",
    "\tax.scatter(states[i][6], states[i][7], s=400, facecolors='none', edgecolors='green', label = 'pocket')\t#label = str(i), \n",
    "\tax.set(xlim=(-1, 1), ylim=(-2, 1))\n",
    "\tax.legend()\n",
    "\t#return line, point,\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate,  frames = len(states), interval=20, repeat=False)\t#init_func=init, blit=True\n",
    "plt.close()\n",
    "#from matplotlib.animation import PillowWriter\n",
    "anim.save('training_plots/Agent_policy2.gif', writer='imagemagick')\t#dpi=300, writer=PillowWriter(fps=1))\t#imagemagick\n",
    "print(\"saved GIF\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Raw dataset and RL dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Offline_RL_dataset(nb_trials=list_data['cueballpos'][\"trial\"].iloc[-1])\n",
    "train_set.get_trajectories(list_data, rewards)\n",
    "#train_set.compute_mean_std(list_data)\n",
    "#train_set.normalize_states()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "states, actions, new_states, rewards, terminals = sample(train_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "for t in range(int(args[\"max_timesteps\"])):\n",
    "\tpolicy.train(replay_buffer, args[\"batch_size\"])\n",
    "\t'''\n",
    "\t# Evaluate episode\n",
    "\tif (t + 1) % args[\"eval_freq\"] == 0:\n",
    "\t\tprint(f\"Time steps: {t+1}\")\n",
    "\t\tevaluations.append(eval_policy(policy, args.env, args.seed, mean, std))\n",
    "\t\t#np.save(f\"./results/{file_name}\", evaluations)\n",
    "\t\t#if args.save_model: policy.save(f\"./models/{file_name}\")\n",
    "\t'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns score\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, eval_dataset, mean, std, eval_episodes=2):\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, done = eval_env.reset(), False\n",
    "\t\twhile not done:\n",
    "\t\t\tstate = (np.array(state).reshape(1,-1) - mean)/std\n",
    "\t\t\taction = policy.select_action(state)\n",
    "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\td4rl_score = eval_env.get_normalized_score(avg_reward) * 100\n",
    "\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}, D4RL score: {d4rl_score:.3f}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\treturn d4rl_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_j_steps = 25\n",
    "gamma = 0.98\n",
    "mag = MAGIC(gamma)\n",
    "info = [data.actions(),\n",
    "                data.rewards(),\n",
    "                data.base_propensity(),\n",
    "                data.target_propensity(),\n",
    "                Qs\n",
    "                ]\n",
    "magic_evaluation = mag.evaluate(info, num_j_steps, True)\n",
    "print(magic_evaluation[0], (magic_evaluation[0] - true )**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import _collections_abc\n",
    "\n",
    "def _count_elements(mapping, iterable):\n",
    "    'Tally elements from the iterable.'\n",
    "    mapping_get = mapping.get\n",
    "    for elem in iterable:\n",
    "        mapping[elem] = mapping_get(elem, 0) + 1\n",
    "'''\n",
    "try:                                    # Load C helper function if available\n",
    "    from _collections import _count_elements\n",
    "except ImportError:\n",
    "    pass\n",
    "'''\n",
    "\n",
    "class itemgetter:\n",
    "    \"\"\"\n",
    "    Return a callable object that fetches the given item(s) from its operand.\n",
    "    After f = itemgetter(2), the call f(r) returns r[2].\n",
    "    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\n",
    "    \"\"\"\n",
    "    __slots__ = ('_items', '_call')\n",
    "\n",
    "    def __init__(self, item, *items):\n",
    "        if not items:\n",
    "            self._items = (item,)\n",
    "            def func(obj):\n",
    "                return obj[item]\n",
    "            self._call = func\n",
    "        else:\n",
    "            self._items = items = (item,) + items\n",
    "            def func(obj):\n",
    "                return tuple(obj[i] for i in items)\n",
    "            self._call = func\n",
    "\n",
    "    def __call__(self, obj):\n",
    "        return self._call(obj)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s.%s(%s)' % (self.__class__.__module__,\n",
    "                              self.__class__.__name__,\n",
    "                              ', '.join(map(repr, self._items)))\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, self._items\n",
    "\n",
    "\n",
    "class Counter(dict):\n",
    "    '''Dict subclass for counting hashable items.  Sometimes called a bag\n",
    "    or multiset.  Elements are stored as dictionary keys and their counts\n",
    "    are stored as dictionary values.\n",
    "    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n",
    "    >>> c.most_common(3)                # three most common elements\n",
    "    [('a', 5), ('b', 4), ('c', 3)]\n",
    "    >>> sorted(c)                       # list all unique elements\n",
    "    ['a', 'b', 'c', 'd', 'e']\n",
    "    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n",
    "    'aaaaabbbbcccdde'\n",
    "    >>> sum(c.values())                 # total of all counts\n",
    "    15\n",
    "    >>> c['a']                          # count of letter 'a'\n",
    "    5\n",
    "    >>> for elem in 'shazam':           # update counts from an iterable\n",
    "    ...     c[elem] += 1                # by adding 1 to each element's count\n",
    "    >>> c['a']                          # now there are seven 'a'\n",
    "    7\n",
    "    >>> del c['b']                      # remove all 'b'\n",
    "    >>> c['b']                          # now there are zero 'b'\n",
    "    0\n",
    "    >>> d = Counter('simsalabim')       # make another counter\n",
    "    >>> c.update(d)                     # add in the second counter\n",
    "    >>> c['a']                          # now there are nine 'a'\n",
    "    9\n",
    "    >>> c.clear()                       # empty the counter\n",
    "    >>> c\n",
    "    Counter()\n",
    "    Note:  If a count is set to zero or reduced to zero, it will remain\n",
    "    in the counter until the entry is deleted or the counter is cleared:\n",
    "    >>> c = Counter('aaabbc')\n",
    "    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n",
    "    >>> c.most_common()                 # 'b' is still in, but its count is zero\n",
    "    [('a', 3), ('c', 1), ('b', 0)]\n",
    "    '''\n",
    "    # References:\n",
    "    #   http://en.wikipedia.org/wiki/Multiset\n",
    "    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n",
    "    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n",
    "    #   http://code.activestate.com/recipes/259174/\n",
    "    #   Knuth, TAOCP Vol. II section 4.6.3\n",
    "\n",
    "    def __init__(self, iterable=None, /, **kwds):\n",
    "        '''Create a new, empty Counter object.  And if given, count elements\n",
    "        from an input iterable.  Or, initialize the count from another mapping\n",
    "        of elements to their counts.\n",
    "        >>> c = Counter()                           # a new, empty counter\n",
    "        >>> c = Counter('gallahad')                 # a new counter from an iterable\n",
    "        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n",
    "        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.update(iterable, **kwds)\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        'The count of elements not in the Counter is zero.'\n",
    "        # Needed so that self[missing_item] does not raise KeyError\n",
    "        return 0\n",
    "\n",
    "    def total(self):\n",
    "        'Sum of the counts'\n",
    "        return sum(self.values())\n",
    "\n",
    "    def most_common(self, n=None):\n",
    "        '''List the n most common elements and their counts from the most\n",
    "        common to the least.  If n is None, then list all element counts.\n",
    "        >>> Counter('abracadabra').most_common(3)\n",
    "        [('a', 5), ('b', 2), ('r', 2)]\n",
    "        '''\n",
    "        # Emulate Bag.sortedByCount from Smalltalk\n",
    "        if n is None:\n",
    "            return sorted(self.items(), key=_itemgetter(1), reverse=True)\n",
    "\n",
    "        # Lazy import to speedup Python startup time\n",
    "        import heapq\n",
    "        return heapq.nlargest(n, self.items(), key=_itemgetter(1))\n",
    "\n",
    "    def elements(self):\n",
    "        '''Iterator over elements repeating each as many times as its count.\n",
    "        >>> c = Counter('ABCABC')\n",
    "        >>> sorted(c.elements())\n",
    "        ['A', 'A', 'B', 'B', 'C', 'C']\n",
    "        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n",
    "        >>> import math\n",
    "        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n",
    "        >>> math.prod(prime_factors.elements())\n",
    "        1836\n",
    "        Note, if an element's count has been set to zero or is a negative\n",
    "        number, elements() will ignore it.\n",
    "        '''\n",
    "        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n",
    "        return _chain.from_iterable(_starmap(_repeat, self.items()))\n",
    "\n",
    "    # Override dict methods where necessary\n",
    "\n",
    "    @classmethod\n",
    "    def fromkeys(cls, iterable, v=None):\n",
    "        # There is no equivalent method for counters because the semantics\n",
    "        # would be ambiguous in cases such as Counter.fromkeys('aaabbc', v=2).\n",
    "        # Initializing counters to zero values isn't necessary because zero\n",
    "        # is already the default value for counter lookups.  Initializing\n",
    "        # to one is easily accomplished with Counter(set(iterable)).  For\n",
    "        # more exotic cases, create a dictionary first using a dictionary\n",
    "        # comprehension or dict.fromkeys().\n",
    "        raise NotImplementedError(\n",
    "            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')\n",
    "\n",
    "    def update(self, iterable=None, /, **kwds):\n",
    "        '''Like dict.update() but add counts instead of replacing them.\n",
    "        Source can be an iterable, a dictionary, or another Counter instance.\n",
    "        >>> c = Counter('which')\n",
    "        >>> c.update('witch')           # add elements from another iterable\n",
    "        >>> d = Counter('watch')\n",
    "        >>> c.update(d)                 # add elements from another counter\n",
    "        >>> c['h']                      # four 'h' in which, witch, and watch\n",
    "        4\n",
    "        '''\n",
    "        # The regular dict.update() operation makes no sense here because the\n",
    "        # replace behavior results in the some of original untouched counts\n",
    "        # being mixed-in with all of the other counts for a mismash that\n",
    "        # doesn't have a straight-forward interpretation in most counting\n",
    "        # contexts.  Instead, we implement straight-addition.  Both the inputs\n",
    "        # and outputs are allowed to contain zero and negative counts.\n",
    "\n",
    "        if iterable is not None:\n",
    "            if isinstance(iterable, _collections_abc.Mapping):\n",
    "                if self:\n",
    "                    self_get = self.get\n",
    "                    for elem, count in iterable.items():\n",
    "                        self[elem] = count + self_get(elem, 0)\n",
    "                else:\n",
    "                    # fast path when counter is empty\n",
    "                    super().update(iterable)\n",
    "            else:\n",
    "                _count_elements(self, iterable)\n",
    "        if kwds:\n",
    "            self.update(kwds)\n",
    "\n",
    "    def subtract(self, iterable=None, /, **kwds):\n",
    "        '''Like dict.update() but subtracts counts instead of replacing them.\n",
    "        Counts can be reduced below zero.  Both the inputs and outputs are\n",
    "        allowed to contain zero and negative counts.\n",
    "        Source can be an iterable, a dictionary, or another Counter instance.\n",
    "        >>> c = Counter('which')\n",
    "        >>> c.subtract('witch')             # subtract elements from another iterable\n",
    "        >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n",
    "        >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n",
    "        0\n",
    "        >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n",
    "        -1\n",
    "        '''\n",
    "        if iterable is not None:\n",
    "            self_get = self.get\n",
    "            if isinstance(iterable, _collections_abc.Mapping):\n",
    "                for elem, count in iterable.items():\n",
    "                    self[elem] = self_get(elem, 0) - count\n",
    "            else:\n",
    "                for elem in iterable:\n",
    "                    self[elem] = self_get(elem, 0) - 1\n",
    "        if kwds:\n",
    "            self.subtract(kwds)\n",
    "\n",
    "    def copy(self):\n",
    "        'Return a shallow copy.'\n",
    "        return self.__class__(self)\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (dict(self),)\n",
    "\n",
    "    def __delitem__(self, elem):\n",
    "        'Like dict.__delitem__() but does not raise KeyError for missing values.'\n",
    "        if elem in self:\n",
    "            super().__delitem__(elem)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self:\n",
    "            return f'{self.__class__.__name__}()'\n",
    "        try:\n",
    "            # dict() preserves the ordering returned by most_common()\n",
    "            d = dict(self.most_common())\n",
    "        except TypeError:\n",
    "            # handle case where values are not orderable\n",
    "            d = dict(self)\n",
    "        return f'{self.__class__.__name__}({d!r})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        'True if all counts agree. Missing counts are treated as zero.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] == other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        'True if any counts disagree. Missing counts are treated as zero.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return not self == other\n",
    "\n",
    "    def __le__(self, other):\n",
    "        'True if all counts in self are a subset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] <= other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        'True if all counts in self are a proper subset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return self <= other and self != other\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        'True if all counts in self are a superset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] >= other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        'True if all counts in self are a proper superset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return self >= other and self != other\n",
    "\n",
    "    def __add__(self, other):\n",
    "        '''Add counts from two counters.\n",
    "        >>> Counter('abbb') + Counter('bcc')\n",
    "        Counter({'b': 4, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            newcount = count + other[elem]\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        ''' Subtract count, but keep only results with positive counts.\n",
    "        >>> Counter('abbbc') - Counter('bccd')\n",
    "        Counter({'b': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            newcount = count - other[elem]\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count < 0:\n",
    "                result[elem] = 0 - count\n",
    "        return result\n",
    "\n",
    "    def __or__(self, other):\n",
    "        '''Union is the maximum of value in either of the input counters.\n",
    "        >>> Counter('abbb') | Counter('bcc')\n",
    "        Counter({'b': 3, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            newcount = other_count if count < other_count else count\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __and__(self, other):\n",
    "        ''' Intersection is the minimum of corresponding counts.\n",
    "        >>> Counter('abbb') & Counter('bcc')\n",
    "        Counter({'b': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            newcount = count if count < other_count else other_count\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        return result\n",
    "\n",
    "    def __pos__(self):\n",
    "        'Adds an empty counter, effectively stripping negative and zero counts'\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            if count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __neg__(self):\n",
    "        '''Subtracts from an empty counter.  Strips positive and zero counts,\n",
    "        and flips the sign on negative counts.\n",
    "        '''\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            if count < 0:\n",
    "                result[elem] = 0 - count\n",
    "        return result\n",
    "\n",
    "    def _keep_positive(self):\n",
    "        '''Internal method to strip elements with a negative or zero count'''\n",
    "        nonpositive = [elem for elem, count in self.items() if not count > 0]\n",
    "        for elem in nonpositive:\n",
    "            del self[elem]\n",
    "        return self\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        '''Inplace add from another counter, keeping only positive counts.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c += Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 4, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, count in other.items():\n",
    "            self[elem] += count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        '''Inplace subtract counter, but keep only results with positive counts.\n",
    "        >>> c = Counter('abbbc')\n",
    "        >>> c -= Counter('bccd')\n",
    "        >>> c\n",
    "        Counter({'b': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, count in other.items():\n",
    "            self[elem] -= count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __ior__(self, other):\n",
    "        '''Inplace union is the maximum of value from either counter.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c |= Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 3, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, other_count in other.items():\n",
    "            count = self[elem]\n",
    "            if other_count > count:\n",
    "                self[elem] = other_count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __iand__(self, other):\n",
    "        '''Inplace intersection is the minimum of corresponding counts.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c &= Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 1})\n",
    "        '''\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            if other_count < count:\n",
    "                self[elem] = other_count\n",
    "        return self._keep_positive()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "class getQs(object):\n",
    "\tdef __init__(self, data, pi_e, processor, action_space_dim = 3):\n",
    "\t\tself.data = data\n",
    "\t\tself.pi_e = pi_e\n",
    "\t\tself.processor = processor\n",
    "\t\tself.action_space_dim = action_space_dim\n",
    "\n",
    "\tdef get(self, model):\n",
    "\t\tQs = []\n",
    "\t\tbatchsize = 1\n",
    "\t\tnum_batches = int(np.ceil(len(self.data)/batchsize))\n",
    "\t\t# frames = np.array([x['frames'] for x in self.trajectories])\n",
    "\t\tfor batchnum in trange(num_batches, desc='Batch'):\n",
    "\t\t\tlow_ = batchsize*batchnum\n",
    "\t\t\thigh_ = min(batchsize*(batchnum+1), len(self.data))\n",
    "\n",
    "\t\t\tpos = self.data.states(False, low_=low_,high_=high_)\n",
    "\t\t\tacts = self.data.actions()[low_:high_]\n",
    "\n",
    "\n",
    "\t\t\t# episodes = self.trajectories[low_:high_]\n",
    "\t\t\t# pos = np.vstack([np.vstack(x['x']) for x in episodes])\n",
    "\t\t\t# N = np.hstack([[low_ + n]*len(x['x']) for n,x in enumerate(episodes)])\n",
    "\t\t\t# acts = np.hstack([x['a'] for x in episodes])\n",
    "\t\t\t# pos = np.array([np.array(frames[int(N[idx])])[pos[idx].astype(int)] for idx in range(len(pos))])\n",
    "\t\t\ttraj_Qs = model.Q(self.pi_e, self.processor(pos))\n",
    "\n",
    "\t\t\ttraj_Qs = traj_Qs.reshape(-1, self.action_space_dim)\n",
    "\t\t\t# lengths = self.data.lengths()\n",
    "\n",
    "\t\t\t# endpts = np.cumsum(np.hstack([[0], lengths]))\n",
    "\t\t\t# for start,end in zip(endpts[:-1], endpts[1:]):\n",
    "\t\t\t# \tQs.append(traj_Qs[start:end])\n",
    "\t\t\tQs.append(traj_Qs)\n",
    "\n",
    "\t\treturn Qs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class defaultCNN(nn.Module):\n",
    "    def __init__(self, shape, action_space_dim):\n",
    "        super(defaultCNN, self).__init__()\n",
    "        self.c, self.h, self.w = shape\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(self.c, 16, (2,2)),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*(self.h-1)*(self.w-1), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=.001)\n",
    "            torch.nn.init.normal_(m.bias, mean=0.0, std=.001)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        output = self.net(state)\n",
    "        return torch.masked_select(output, action)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def predict_w_softmax(self, state):\n",
    "        return nn.Softmax()(self.net(state))\n",
    "\n",
    "class defaultModelBasedCNN(nn.Module):\n",
    "    def __init__(self, shape, action_space_dim):\n",
    "        super(defaultModelBasedCNN, self).__init__()\n",
    "        self.c, self.h, self.w = shape\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.c, 4, (5, 5)),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(4, 8, (3, 3)),\n",
    "        )\n",
    "\n",
    "        self.states_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, (3, 3)),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(16, action_space_dim, (5, 5)),\n",
    "        )\n",
    "        \n",
    "        self.rewards_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*(self.h-4-2)*(self.w-4-2), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim),\n",
    "        )\n",
    "\n",
    "        self.dones_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*(self.h-4-2)*(self.w-4-2), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=.001)\n",
    "            torch.nn.init.normal_(m.bias, mean=0.0, std=.001)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        T, R, D = self.states_head(self.features(state)), self.rewards_head(self.features(state)), self.dones_head(self.features(state))\n",
    "        return T[np.arange(len(action)), action.float().argmax(1), ...][:,None,:,:], torch.masked_select(R, action), torch.masked_select(D, action)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.states_head(self.features(state)), self.rewards_head(self.features(state)), self.dones_head(self.features(state))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std):\n",
    "        super(PropensityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "        self.std = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x):\n",
    "        # Sample from the Gaussian distribution with mean predicted by the model and fixed standard deviation\n",
    "        mean = self.forward(x)\n",
    "        std = torch.tensor(self.std).expand_as(mean)\n",
    "        return torch.normal(mean, std)\n",
    "\n",
    "# Define the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim, action_dim, std=0.1)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop through the dataset and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the loss and backpropagate\n",
    "    loss = criterion(model(state), action)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std):\n",
    "        super(PropensityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "        self.std = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x):\n",
    "        # Sample from the Gaussian distribution with mean predicted by the model and fixed standard deviation\n",
    "        mean = self.forward(x)\n",
    "        std = torch.tensor(self.std).expand_as(mean)\n",
    "        return torch.normal(mean, std)\n",
    "\n",
    "# Define the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim, action_dim, std=0.1)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop through the dataset and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the loss and backpropagate\n",
    "    loss = criterion(model(state), action)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std=1.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_dim)\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x).numpy()\n",
    "    \n",
    "    def log_prob(self, x, actions):\n",
    "        means = self.forward(x)\n",
    "        log_prstate = -0.5 * ((actions - means) / self.std) ** 2 - 0.5 * np.log(2 * np.pi) - np.log(self.std)\n",
    "        return log_prstate.sum(1, keepdim=True)\n",
    "\n",
    "# Create the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim=state_dim, action_dim=action_dim)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Loop over the data and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Compute the log probabilities of the actions\n",
    "    log_prstate = model.log_prob(state, action)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = -log_prstate.mean()\n",
    "    \n",
    "    # Perform backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Use the model to predict the propensity of an action given a state\n",
    "#state = np.array([[1, 2, 3, 4]])\n",
    "#print(model.predict(torch.from_numpy(state).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_propensity(self, behavior_data):    #cfg):\n",
    "    # WARN: Only works in tabular env with discrete action space. Current implementation is a max likelihood\n",
    "\n",
    "    model = defaultCNN(self.states()[0][0].shape, self.n_actions) #cfg.to_regress_pi_b['model']\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    self.processed_data = self.fill()\n",
    "\n",
    "    batch_size = 32 #cfg.to_regress_pi_b['batch_size']\n",
    "    dataset_length = self.num_tuples()\n",
    "    perm = np.random.permutation(range(dataset_length))\n",
    "    eighty_percent_of_set = int(.8*len(perm))\n",
    "    training_idxs = perm[:eighty_percent_of_set]\n",
    "    validation_idxs = perm[eighty_percent_of_set:]\n",
    "    training_steps_per_epoch = int(np.ceil(len(training_idxs)/float(batch_size)))\n",
    "    validation_steps_per_epoch = int(np.ceil(len(validation_idxs)/float(batch_size)))\n",
    "\n",
    "    for k in tqdm(range(100)):#cfg.to_regress_pi_b['max_epochs']\n",
    "        \n",
    "        train_gen = self.generator(training_idxs, fixed_permutation=True, batch_size=batch_size)\n",
    "        val_gen = self.generator(validation_idxs, fixed_permutation=True, batch_size=batch_size)\n",
    "\n",
    "        # TODO: earlyStopping, LR reduction\n",
    "\n",
    "        for step in range(training_steps_per_epoch):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                (inp, out) = next(train_gen)\n",
    "                states = torch.from_numpy(inp).float()\n",
    "                actions = torch.from_numpy(out).float().argmax(1)\n",
    "\n",
    "            prediction = model.predict_w_softmax(states)\n",
    "            \n",
    "            loss = nn.NLLLoss()(torch.log(prediction), actions)\n",
    "                                        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), 1.0)   #cfg.to_regress_pi_b['clipnorm']\n",
    "            optimizer.step()\n",
    "\n",
    "    for episode_num, states in enumerate(np.squeeze(self.states())):\n",
    "        base_propensity = []\n",
    "        for state in states:\n",
    "            base_propensity.append(model.predict_w_softmax(torch.from_numpy(state[None,None,...]).float()).detach().numpy()[0].tolist())\n",
    "\n",
    "        self.trajectories[episode_num]['base_propensity'] = base_propensity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAGIC algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omega(self):\n",
    "        return np.array([[episode['target_propensity'][idx][int(act)]/episode['base_propensity'][idx][int(act)] for idx,act in enumerate(episode['a'])] for episode in self.trajectories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGIC(object):\n",
    "    \"\"\"Algorithm: MAGIC.\n",
    "    \"\"\"\n",
    "    NUM_SUBSETS_FOR_CB_ESTIMATES = 25\n",
    "    CONFIDENCE_INTERVAL = 0.9\n",
    "    NUM_BOOTSTRAP_SAMPLES = 50\n",
    "    BOOTSTRAP_SAMPLE_PCT = 0.5\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "            Discount factor.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def evaluate(self, info, num_j_steps, is_wdr, return_Qs = False):\n",
    "        \"\"\"Get MAGIC estimate from Q + IPS.\n",
    "        Parameters\n",
    "        ----------\n",
    "        info : list\n",
    "            [list of actions, list of rewards, list of base propensity, list of target propensity, list of Qhat]\n",
    "        num_j_steps : int\n",
    "            Parameter to MAGIC algorithm\n",
    "        is_wdr : bool\n",
    "            Use Weighted Doubly Robust?\n",
    "        return_Qs : bool\n",
    "            Return trajectory-wise estimate alongside full DR estimate?\n",
    "            Default: False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            [MAGIC estimate, normalized MAGIC, std error, normalized std error]\n",
    "            If return_Qs is true, also returns trajectory-wise estimate\n",
    "        \"\"\"\n",
    "\n",
    "        (actions,\n",
    "        rewards,\n",
    "        base_propensity,\n",
    "        target_propensities,\n",
    "        estimated_q_values) = MAGIC.transform_to_equal_length_trajectories(*info)\n",
    "\n",
    "        num_trajectories = actions.shape[0]\n",
    "        trajectory_length = actions.shape[1]\n",
    "\n",
    "        j_steps = [float(\"inf\")]\n",
    "\n",
    "        if num_j_steps > 1:\n",
    "            j_steps.append(-1)\n",
    "        if num_j_steps > 2:\n",
    "            interval = trajectory_length // (num_j_steps - 1)\n",
    "            j_steps.extend([i * interval for i in range(1, num_j_steps - 1)])\n",
    "\n",
    "        base_propensity_for_logged_action = np.sum(\n",
    "            np.multiply(base_propensity, actions), axis=2\n",
    "        )\n",
    "        target_propensity_for_logged_action = np.sum(\n",
    "            np.multiply(target_propensities, actions), axis=2\n",
    "        )\n",
    "        estimated_q_values_for_logged_action = np.sum(\n",
    "            np.multiply(estimated_q_values, actions), axis=2\n",
    "        )\n",
    "        estimated_state_values = np.sum(\n",
    "            np.multiply(target_propensities, estimated_q_values), axis=2\n",
    "        )\n",
    "\n",
    "        importance_weights = target_propensity_for_logged_action / base_propensity_for_logged_action\n",
    "        importance_weights[np.isnan(importance_weights)] = 0.\n",
    "        importance_weights = np.cumprod(importance_weights, axis=1)\n",
    "        importance_weights = MAGIC.normalize_importance_weights(\n",
    "            importance_weights, is_wdr\n",
    "        )\n",
    "\n",
    "        importance_weights_one_earlier = (\n",
    "            np.ones([num_trajectories, 1]) * 1.0 / num_trajectories\n",
    "        )\n",
    "        importance_weights_one_earlier = np.hstack(\n",
    "            [importance_weights_one_earlier, importance_weights[:, :-1]]\n",
    "        )\n",
    "\n",
    "        discounts = np.logspace(\n",
    "            start=0, stop=trajectory_length - 1, num=trajectory_length, base=self.gamma\n",
    "        )\n",
    "\n",
    "        j_step_return_trajectories = []\n",
    "        for j_step in j_steps:\n",
    "            j_step_return_trajectories.append(\n",
    "                MAGIC.calculate_step_return(\n",
    "                    rewards,\n",
    "                    discounts,\n",
    "                    importance_weights,\n",
    "                    importance_weights_one_earlier,\n",
    "                    estimated_state_values,\n",
    "                    estimated_q_values_for_logged_action,\n",
    "                    j_step,\n",
    "                )\n",
    "            )\n",
    "        j_step_return_trajectories = np.array(j_step_return_trajectories)\n",
    "\n",
    "        j_step_returns = np.sum(j_step_return_trajectories, axis=1)\n",
    "\n",
    "        if len(j_step_returns) == 1:\n",
    "            weighted_doubly_robust = j_step_returns[0]\n",
    "            weighted_doubly_robust_std_error = 0.0\n",
    "        else:\n",
    "            # break trajectories into several subsets to estimate confidence bounds\n",
    "            infinite_step_returns = []\n",
    "            num_subsets = int(\n",
    "                min(\n",
    "                    num_trajectories / 2,\n",
    "                    MAGIC.NUM_SUBSETS_FOR_CB_ESTIMATES,\n",
    "                )\n",
    "            )\n",
    "            interval = num_trajectories / num_subsets\n",
    "            for i in range(num_subsets):\n",
    "                trajectory_subset = np.arange(\n",
    "                    int(i * interval), int((i + 1) * interval)\n",
    "                )\n",
    "                importance_weights = (\n",
    "                    target_propensity_for_logged_action[trajectory_subset]\n",
    "                    / base_propensity_for_logged_action[trajectory_subset]\n",
    "                )\n",
    "                importance_weights[np.isnan(importance_weights)] = 0.\n",
    "                importance_weights = np.cumprod(importance_weights, axis=1)\n",
    "                importance_weights = MAGIC.normalize_importance_weights(\n",
    "                    importance_weights, is_wdr\n",
    "                )\n",
    "                importance_weights_one_earlier = (\n",
    "                    np.ones([len(trajectory_subset), 1]) * 1.0 / len(trajectory_subset)\n",
    "                )\n",
    "                importance_weights_one_earlier = np.hstack(\n",
    "                    [importance_weights_one_earlier, importance_weights[:, :-1]]\n",
    "                )\n",
    "                infinite_step_return = np.sum(\n",
    "                    MAGIC.calculate_step_return(\n",
    "                        rewards[trajectory_subset],\n",
    "                        discounts,\n",
    "                        importance_weights,\n",
    "                        importance_weights_one_earlier,\n",
    "                        estimated_state_values[trajectory_subset],\n",
    "                        estimated_q_values_for_logged_action[trajectory_subset],\n",
    "                        float(\"inf\"),\n",
    "                    )\n",
    "                )\n",
    "                infinite_step_returns.append(infinite_step_return)\n",
    "\n",
    "            # Compute weighted_doubly_robust mean point estimate using all data\n",
    "            weighted_doubly_robust, xs = self.compute_weighted_doubly_robust_point_estimate(\n",
    "                j_steps,\n",
    "                num_j_steps,\n",
    "                j_step_returns,\n",
    "                infinite_step_returns,\n",
    "                j_step_return_trajectories,\n",
    "            )\n",
    "\n",
    "            # Use bootstrapping to compute weighted_doubly_robust standard error\n",
    "            bootstrapped_means = []\n",
    "            sample_size = int(\n",
    "                MAGIC.BOOTSTRAP_SAMPLE_PCT\n",
    "                * num_subsets\n",
    "            )\n",
    "            for _ in range(\n",
    "                MAGIC.NUM_BOOTSTRAP_SAMPLES\n",
    "            ):\n",
    "                random_idxs = np.random.choice(num_j_steps, sample_size, replace=False)\n",
    "                random_idxs.sort()\n",
    "                wdr_estimate = self.compute_weighted_doubly_robust_point_estimate(\n",
    "                    j_steps=[j_steps[i] for i in random_idxs],\n",
    "                    num_j_steps=sample_size,\n",
    "                    j_step_returns=j_step_returns[random_idxs],\n",
    "                    infinite_step_returns=infinite_step_returns,\n",
    "                    j_step_return_trajectories=j_step_return_trajectories[random_idxs],\n",
    "                )\n",
    "                bootstrapped_means.append(wdr_estimate)\n",
    "            weighted_doubly_robust_std_error = np.std(bootstrapped_means)\n",
    "\n",
    "        episode_values = np.sum(np.multiply(rewards, discounts), axis=1)\n",
    "        denominator = np.nanmean(episode_values)\n",
    "        if abs(denominator) < 1e-6:\n",
    "            return [0]*4\n",
    "\n",
    "        # print (weighted_doubly_robust,\n",
    "        #         weighted_doubly_robust / denominator,\n",
    "        #         weighted_doubly_robust_std_error,\n",
    "        #         weighted_doubly_robust_std_error / denominator)\n",
    "\n",
    "        if return_Qs:\n",
    "            return [weighted_doubly_robust,\n",
    "                    weighted_doubly_robust / denominator,\n",
    "                    weighted_doubly_robust_std_error,\n",
    "                    weighted_doubly_robust_std_error / denominator], np.dot(xs, j_step_return_trajectories)\n",
    "        else:\n",
    "            return [weighted_doubly_robust,\n",
    "                    weighted_doubly_robust / denominator,\n",
    "                    weighted_doubly_robust_std_error,\n",
    "                    weighted_doubly_robust_std_error / denominator]\n",
    "\n",
    "    def compute_weighted_doubly_robust_point_estimate(\n",
    "        self,\n",
    "        j_steps,\n",
    "        num_j_steps,\n",
    "        j_step_returns,\n",
    "        infinite_step_returns,\n",
    "        j_step_return_trajectories,\n",
    "    ):\n",
    "        low_bound, high_bound = MAGIC.confidence_bounds(\n",
    "            infinite_step_returns,\n",
    "            MAGIC.CONFIDENCE_INTERVAL,\n",
    "        )\n",
    "        # decompose error into bias + variance\n",
    "        j_step_bias = np.zeros([num_j_steps])\n",
    "        where_lower = np.where(j_step_returns < low_bound)[0]\n",
    "        j_step_bias[where_lower] = low_bound - j_step_returns[where_lower]\n",
    "        where_higher = np.where(j_step_returns > high_bound)[0]\n",
    "        j_step_bias[where_higher] = j_step_returns[where_higher] - high_bound\n",
    "\n",
    "        covariance = np.cov(j_step_return_trajectories)\n",
    "        error = covariance + j_step_bias.T * j_step_bias\n",
    "\n",
    "        # minimize mse error\n",
    "        constraint = {\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - 1.0}\n",
    "\n",
    "        x = np.zeros([len(j_steps)])\n",
    "        res = sp.optimize.minimize(\n",
    "            mse_loss,\n",
    "            x,\n",
    "            args=error,\n",
    "            constraints=constraint,\n",
    "            bounds=[(0, 1) for _ in range(x.shape[0])],\n",
    "        )\n",
    "        x = np.array(res.x)\n",
    "        return float(np.dot(x, j_step_returns)), x\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_to_equal_length_trajectories(\n",
    "        actions,\n",
    "        rewards,\n",
    "        logged_propensities,\n",
    "        target_propensities,\n",
    "        estimated_q_values,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Take in samples (action, rewards, propensities, etc.) and output lists\n",
    "        of equal-length trajectories (episodes) according to terminals.\n",
    "        As the raw trajectories are of various lengths, the shorter ones are\n",
    "        filled with zeros(ones) at the end.\n",
    "        \"\"\"\n",
    "        num_actions = len(target_propensities[0][0])\n",
    "\n",
    "        def to_equal_length(x, fill_value):\n",
    "            x_equal_length = np.array(\n",
    "                list(itertools.zip_longest(*x, fillvalue=fill_value))\n",
    "            ).swapaxes(0, 1)\n",
    "            return x_equal_length\n",
    "\n",
    "        action_trajectories = to_equal_length(\n",
    "            [np.eye(num_actions)[act] for act in actions], np.zeros([num_actions])\n",
    "        )\n",
    "        reward_trajectories = to_equal_length(rewards, 0)\n",
    "        logged_propensity_trajectories = to_equal_length(\n",
    "            logged_propensities, np.zeros([num_actions])\n",
    "        )\n",
    "        target_propensity_trajectories = to_equal_length(\n",
    "            target_propensities, np.zeros([num_actions])\n",
    "        )\n",
    "\n",
    "        # Hack for now. Delete.\n",
    "        estimated_q_values = [[np.hstack(y).tolist() for y in x] for x in estimated_q_values]\n",
    "\n",
    "        Q_value_trajectories = to_equal_length(\n",
    "            estimated_q_values, np.zeros([num_actions])\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            action_trajectories,\n",
    "            reward_trajectories,\n",
    "            logged_propensity_trajectories,\n",
    "            target_propensity_trajectories,\n",
    "            Q_value_trajectories,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_importance_weights(\n",
    "        importance_weights, is_wdr\n",
    "    ):\n",
    "        if is_wdr:\n",
    "            sum_importance_weights = np.sum(importance_weights, axis=0)\n",
    "            where_zeros = np.where(sum_importance_weights == 0.0)[0]\n",
    "            sum_importance_weights[where_zeros] = len(importance_weights)\n",
    "            importance_weights[:, where_zeros] = 1.0\n",
    "            importance_weights /= sum_importance_weights\n",
    "            return importance_weights\n",
    "        else:\n",
    "            importance_weights /= importance_weights.shape[0]\n",
    "            return importance_weights\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_step_return(\n",
    "        rewards,\n",
    "        discounts,\n",
    "        importance_weights,\n",
    "        importance_weights_one_earlier,\n",
    "        estimated_state_values,\n",
    "        estimated_q_values,\n",
    "        j_step,\n",
    "    ):\n",
    "        trajectory_length = len(rewards[0])\n",
    "        num_trajectories = len(rewards)\n",
    "        j_step = int(min(j_step, trajectory_length - 1))\n",
    "\n",
    "        weighted_discounts = np.multiply(discounts, importance_weights)\n",
    "        weighted_discounts_one_earlier = np.multiply(\n",
    "            discounts, importance_weights_one_earlier\n",
    "        )\n",
    "\n",
    "        importance_sampled_cumulative_reward = np.sum(\n",
    "            np.multiply(weighted_discounts[:, : j_step + 1], rewards[:, : j_step + 1]),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        if j_step < trajectory_length - 1:\n",
    "            direct_method_value = (\n",
    "                weighted_discounts_one_earlier[:, j_step + 1]\n",
    "                * estimated_state_values[:, j_step + 1]\n",
    "            )\n",
    "        else:\n",
    "            direct_method_value = np.zeros([num_trajectories])\n",
    "\n",
    "        control_variate = np.sum(\n",
    "            np.multiply(\n",
    "                weighted_discounts[:, : j_step + 1], estimated_q_values[:, : j_step + 1]\n",
    "            )\n",
    "            - np.multiply(\n",
    "                weighted_discounts_one_earlier[:, : j_step + 1],\n",
    "                estimated_state_values[:, : j_step + 1],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        j_step_return = (\n",
    "            importance_sampled_cumulative_reward + direct_method_value - control_variate\n",
    "        )\n",
    "\n",
    "        return j_step_return\n",
    "\n",
    "    @staticmethod\n",
    "    def confidence_bounds(x, confidence):\n",
    "        n = len(x)\n",
    "        m, se = np.mean(x), sp.stats.sem(x)\n",
    "        h = se * sp.stats.t._ppf((1 + confidence) / 2.0, n - 1)\n",
    "        return m - h, m + h\n",
    "\n",
    "\n",
    "def mse_loss(x, error):\n",
    "    return np.dot(np.dot(x, error), x.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"RL_dataset/raw_data/\"+initial+\"_raw_data.csv\", header = 0, \\\n",
    "                names = ['rewards','cueballpos', 'cueballvel','redballpos', 'targetcornerpos', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel'], usecols = [1,2,3,4,5,6,7,8,9], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "rewards= pd.DataFrame.from_records(np.array(df['rewards'].astype(str).str.split(','))).astype(float)\n",
    "cueballpos = pd.DataFrame.from_records(np.array(df['cueballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueballvel = pd.DataFrame.from_records(np.array(df['cueballvel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "redballpos = pd.DataFrame.from_records(np.array(df['redballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "targetcornerpos = pd.DataFrame.from_records(np.array(df['targetcornerpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposfront = pd.DataFrame.from_records(np.array(df['cueposfront'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposback = pd.DataFrame.from_records(np.array(df['cueposback'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuedirection = pd.DataFrame.from_records(np.array(df['cuedirection'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuevel = pd.DataFrame.from_records(np.array(df['cuevel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "\n",
    "ball_data = {'cueballpos': cueballpos,\n",
    "        'cueballvel': cueballvel,\n",
    "    'redballpos': redballpos,\n",
    "    'targetcornerpos': targetcornerpos\n",
    "    }\n",
    "cue_data = {'cueposfront': cueposfront,\n",
    "        'cueposback': cueposback,\n",
    "        'cuedirection': cuedirection\n",
    "        }\n",
    "#dataset = Offline_Reduced(ball_data, cue_data, cuevel, rewards[0])\n",
    "action_features = compute_action_feature(cue_data, cuevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cue pos at t-1, cuevel at t-1, cue acceleration?, angle, magnitude, cue pos last 3 timesteps\n",
    "def compute_action_feature(cue_data, cuevel): #cuevel, cueDirection):\n",
    "    cueAngle = min_max_standardisation(compute_angle(cue_data[\"cuedirection\"])) #cueDirection\n",
    "\n",
    "    cueMagnitude,_ = compute_impulseForce(cuevel, cue_data[\"cuedirection\"])\n",
    "    cueMagnitude = min_max_standardisation(cueMagnitude)\n",
    "\n",
    "\n",
    "    cue_pos_front_t1 = np.zeros((250,3))\n",
    "    cue_pos_back_t1 = np.zeros((250,3))\n",
    "    cue_pos_front_t3 = np.zeros((250,3,3))\n",
    "    cue_pos_back_t3 = np.zeros((250,3,3))\n",
    "\n",
    "    for i in cue_data[\"cueposfront\"][\"trial\"].unique():\n",
    "        for j in range(len(cue_data[\"cueposfront\"][cue_data[\"cueposfront\"][\"trial\"] == i])):\n",
    "            if j < 1:\n",
    "                cue_pos_front_t1[j][:] = cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j]\n",
    "                cue_pos_back_t1[j][:] = cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j]\n",
    "\n",
    "                cue_pos_front_t3[j][:][:] = np.array([cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j],cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j],cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j]])\n",
    "                cue_pos_back_t3[j][:][:] = np.array([cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j],cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j],cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j]])\n",
    "\n",
    "            elif j < 3:    \n",
    "                cue_pos_front_t1[j][:] = cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-1]\n",
    "                cue_pos_back_t1[j][:] = cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-1]\n",
    "                if j==1:\n",
    "                    cue_pos_front_t3[j][:][:] = np.array([cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-1], cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-1], cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-1]])\n",
    "                    cue_pos_back_t3[j][:][:] = np.array([cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-1], cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-1], cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-1]])\n",
    "                elif j == 2:\n",
    "                    cue_pos_front_t3[j][:][:] = np.array([cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-1], cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-2], cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-2]])\n",
    "                    cue_pos_back_t3[j][:][:] = np.array([cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-1], cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-2], cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-2]])\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                cue_pos_front_t1[j][:] = cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-1]\n",
    "                cue_pos_back_t1[j][:] = cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-1]\n",
    "                cue_pos_front_t3[j][:][:] = np.array([cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-1], cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-2], cue_data[\"cueposfront\"][[\"x\",\"y\",\"z\"]].iloc[j-3]])\n",
    "                cue_pos_back_t3[j][:][:] = np.array([cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-1], cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-2], cue_data[\"cueposback\"][[\"x\",\"y\",\"z\"]].iloc[j-3]])\n",
    "\n",
    "\n",
    "    actions = np.hstack((cueAngle.reshape(-1,1), cueMagnitude.to_numpy().reshape(-1,1)))\n",
    "\n",
    "    actions = {\n",
    "        \"cueposfront_t-1\": cue_pos_front_t1,\n",
    "        \"cueposback_t-1\": cue_pos_back_t1,\n",
    "        \"cueposfront_t-3\": cue_pos_front_t3,\n",
    "        \"cueposback_t-3\": cue_pos_back_t3,\n",
    "        \"cuevel\": cuevel,\n",
    "        \"cueMagnitude\": cueMagnitude.to_numpy().reshape(-1,1),\n",
    "        \"cueAngle\": cueAngle.reshape(-1,1),\n",
    "        \"cuedirection\": cue_data[\"cuedirection\"]\n",
    "    }\n",
    "    return actions  #cueAngle, cueMagnitude\n",
    "\n",
    "def min_max_standardisation(array):\n",
    "    #array must be 1-dimensional\n",
    "    array = (array - array.min()) / (array.max() - array.min())\n",
    "    return array\n",
    "\n",
    "def compute_angle(cueDirection):\n",
    "    cueAngle = np.rad2deg(np.arctan2(cueDirection['z'].values, cueDirection['x'].values))\n",
    "    return cueAngle\n",
    "\n",
    "def compute_impulseForce(cuevel, cuedirection):\n",
    "    impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "    shotMagnitude = np.zeros(1)\n",
    "    shotDir = np.zeros(cuedirection.shape)\n",
    "    #Reward: magnitude range\n",
    "    lbMagnitude = 0.1   #0.516149\n",
    "    ubMagnitude = 3 #0.882607\n",
    "\n",
    "    shotMagnitude = np.sqrt(np.square(cuevel).sum(axis=1))\n",
    "    #np.linalg.norm(cuevel, axis=1)\n",
    "    for i in range(cuevel.shape[0]):\n",
    "        if shotMagnitude[i] > ubMagnitude:\n",
    "            shotMagnitude[i] = ubMagnitude\n",
    "        elif shotMagnitude[i] < lbMagnitude:\n",
    "            shotMagnitude[i] = 0\n",
    "\n",
    "        shotDir[i][0] = cuedirection[\"x\"].iloc[i]\n",
    "        shotDir[i][1] = cuedirection[\"z\"].iloc[i]\n",
    "        if shotMagnitude[i] == 0:\n",
    "            impulseForce[i][:] = 0\n",
    "        else:\n",
    "            impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "    return shotMagnitude, impulseForce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbolli/.local/lib/python3.8/site-packages/statsmodels/compat/pandas.py:61: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    },
    {
     "ename": "PatsyError",
     "evalue": "Error evaluating factor: NameError: name 'weight' is not defined\n    weight ~ group\n    ^^^^^^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         return eval(code, {}, VarLookupDict([inner_namespace]\n\u001b[0m\u001b[1;32m    166\u001b[0m                                             + self._namespaces))\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weight' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_390/414782368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight ~ group'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maov_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manova_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maov_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         tmp = handle_formula_data(data, None, formula, depth=eval_env,\n\u001b[0m\u001b[1;32m    201\u001b[0m                                   missing=missing)\n\u001b[1;32m    202\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesign_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/statsmodels/formula/formulatools.py\u001b[0m in \u001b[0;36mhandle_formula_data\u001b[0;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_using_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n\u001b[0m\u001b[1;32m     64\u001b[0m                                NA_action=na_action)\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \"\"\"\n\u001b[1;32m    308\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[0m\u001b[1;32m    310\u001b[0m                                       NA_action, return_type)\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n\u001b[0m\u001b[1;32m    165\u001b[0m                                       NA_action)\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdesign_infos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelDesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         return design_matrix_builders([formula_like.lhs_termlist,\n\u001b[0m\u001b[1;32m     67\u001b[0m                                        formula_like.rhs_termlist],\n\u001b[1;32m     68\u001b[0m                                       \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/build.py\u001b[0m in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;31m# on some data to find out what type of data they return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     (num_column_counts,\n\u001b[0;32m--> 693\u001b[0;31m      \u001b[0mcat_levels_contrasts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_examine_factor_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_factors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m                                                    \u001b[0mfactor_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                                                    \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/build.py\u001b[0m in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamine_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mguess_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         return self._eval(memorize_state[\"eval_code\"],\n\u001b[0m\u001b[1;32m    565\u001b[0m                           \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                           data)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/eval.py\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0minner_namespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVarLookupDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"transforms\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         return call_and_wrap_exc(\"Error evaluating factor\",\n\u001b[0m\u001b[1;32m    548\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                                  \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_env\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                  origin)\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Use 'exec' to hide this syntax from the Python 2 parser:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raise new_exc from e\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# In python 2, we just let the original exception escape -- better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/patsy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'weight' is not defined\n    weight ~ group\n    ^^^^^^"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "mod = ols('weight ~ group', data=actions).fit()\n",
    "aov_table = sm.stats.anova_lm(mod, typ=2)\n",
    "print(aov_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
