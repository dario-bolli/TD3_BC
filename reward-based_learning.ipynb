{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbolli/.local/lib/python3.8/site-packages/statsmodels/compat/pandas.py:61: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n",
      "/home/dbolli/.local/lib/python3.8/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.5.2, the latest is 0.5.3.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n",
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow.envs'\n",
      "/home/dbolli/.local/lib/python3.8/site-packages/glfw/__init__.py:912: GLFWError: (65544) b'X11: Failed to open display 172.22.240.1:0.0'\n",
      "  warnings.warn(message, GLFWError)\n",
      "/home/dbolli/.local/lib/python3.8/site-packages/gym/envs/registration.py:415: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import scipy.stats as ss\n",
    "import pingouin as pg\n",
    "\n",
    "import scipy.signal as sci\n",
    "import math\n",
    "import scipy as sp\n",
    "\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "\n",
    "import re\n",
    "\n",
    "import copy\n",
    "import argparse\n",
    "import d4rl\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_impulseForce(cuevel, cuedirection):\n",
    "    #numVelocitiesAverage = 5\n",
    "    \n",
    "    impulseForce = np.zeros(cuevel.shape)\n",
    "    shotMagnitude = np.zeros(1)\n",
    "    shotDir = np.zeros(cuedirection.shape)\n",
    "    #Reward: magnitude range\n",
    "    lbMagnitude = 0.516149\n",
    "    ubMagnitude = 0.882607\n",
    "\n",
    "    shotMagnitude = np.linalg.norm(cuevel)\n",
    "    if shotMagnitude > ubMagnitude:\n",
    "        shotMagnitude = ubMagnitude\n",
    "        #print(\"upper bounded\")\n",
    "    elif shotMagnitude < lbMagnitude:\n",
    "        shotMagnitude = lbMagnitude\n",
    "        #print(\"lower bounded\")\n",
    "\n",
    "    for i in range(len(cuevel)):\n",
    "        '''if i < numVelocitiesAverage:\n",
    "            VelocityList = cuevel[[\"x\",\"z\"]].iloc[:i]   #Along y axis as well?\n",
    "            cueDirList = cuedirection[[\"x\",\"z\"]].iloc[:i]\n",
    "        else:\n",
    "            VelocityList = cuevel[[\"x\",\"z\"]].iloc[i-(numVelocitiesAverage+1):i+1]\n",
    "            cueDirList = cuedirection[[\"x\",\"z\"]].iloc[i-(numVelocitiesAverage+1):i+1]'''\n",
    "    \n",
    "    #avgVelocity = np.median(VelocityList) #median cue stick velocity from last 10 frames (~0.11 sec)\n",
    "    #avgDir = np.median(cueDirList) #median direction of cue stick from last 10 frames\n",
    "\n",
    "        shotDir[i] = cuedirection.iloc[i]\n",
    "        impulseForce[i] = shotMagnitude * shotDir[i]\n",
    "    return impulseForce\n",
    "    \n",
    "def def_reward(SuccessFunnel_table, SuccessMedian_table):\n",
    "    print(\"Reward function\")\n",
    "    reward = np.zeros(len(SuccessFunnel_table))\n",
    "    for i in range(len(SuccessFunnel_table)):\n",
    "        if SuccessFunnel_table.iloc[i] == 1:\n",
    "            reward[i] = 100\n",
    "        elif SuccessMedian_table.iloc[i] == 1:\n",
    "            reward[i] = 20\n",
    "        else:\n",
    "            reward[i] = -10\n",
    "\n",
    "    return reward\n",
    "\n",
    "def static_for_n_timesteps(cueballvel, redballvel, timestep, n):\n",
    "    err = 0.001\n",
    "    count=0\n",
    "    for i in range(timestep, timestep+n, 1):\n",
    "        if np.linalg.norm(cueballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) < err and np.linalg.norm(redballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) < err:\n",
    "            count += 1\n",
    "    if count == n:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def start_hit_timesteps(subjData): #, TrialNumber):\n",
    "    \n",
    "    print(\"start hit timesteps function\")\n",
    "    cueballPos = subjData[\"cbpos\"]\n",
    "    cueballvel = subjData[\"cbvel\"]\n",
    "    redballvel = subjData[\"rbvel\"]\n",
    "    cuevel = subjData[\"cuevel\"]\n",
    "    #Find when cue hits cueball\n",
    "    hit_ind=[]\n",
    "    start_ind=[]\n",
    "    n = 10\n",
    "    threshold = 0.1\n",
    "\n",
    "    miss_hit = False\n",
    "\n",
    "    prev_trial = 0\n",
    "    block_until_next_trial = True\n",
    "    for i in range(len(cueballPos)):\n",
    "        #The first 200 timesteps approximately are calibration and parasite movements\n",
    "        if i > 200:   #!= 0:\n",
    "            #New trial started\n",
    "            if cueballPos[\"trial\"].iloc[i] > prev_trial:\n",
    "                if miss_hit == True and len(hit_ind) < len(start_ind):   \n",
    "                    hit_ind = np.append(hit_ind, start_ind[-1]+350)#on average hitting cueball after 350 timesteps\n",
    "                    miss_hit = False \n",
    "                if static_for_n_timesteps(cueballvel, redballvel, i, n):\n",
    "                    #Wait for cueball vel y-axis and redball vel y-axis to be zero after new trial started\n",
    "                    start_ind = np.append(start_ind, i)\n",
    "                    prev_trial = cueballPos[\"trial\"].iloc[i]\n",
    "                    block_until_next_trial = False\n",
    "                    miss_hit = True\n",
    "            elif cueballPos[\"trial\"].iloc[i] == prev_trial:\n",
    "                if np.linalg.norm(cueballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) > threshold and block_until_next_trial == False:\n",
    "                    #Add 6 timesteps for margin\n",
    "                    hit_ind = np.append(hit_ind, i+5)\n",
    "                    block_until_next_trial = True\n",
    "                    miss_hit = False\n",
    "\n",
    "        #if last Trial is missed\n",
    "        if i == len(cueballPos.index) and miss_hit:\n",
    "            hit_ind = np.append(hit_ind, start_ind[-1]+350)#on average hitting cueball after 350 timesteps\n",
    "\n",
    "    if len(start_ind) != 250 or len(hit_ind) != 250:\n",
    "        print(\"WARNING: either missed a start-hit index, or a trial was not properly recorded\")\n",
    "        #raise ValueError( \"Missed an index. start ind size\", len(start_ind), \"and hit ind size: \", len(hit_ind))\n",
    "    for i in range(len(start_ind)):\n",
    "        if start_ind[i] >= hit_ind[i]:\n",
    "            raise ValueError(\"start ind > hit_ind\", i , start_ind, hit_ind)\n",
    "    return start_ind.astype(int), hit_ind.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_hit_reduced_timesteps(subjData):\n",
    "    \n",
    "    print(\"start hit timesteps function\")\n",
    "    cueballPos = subjData[\"cbpos\"]\n",
    "    cueballvel = subjData[\"cbvel\"]\n",
    "    redballvel = subjData[\"rbvel\"]\n",
    "    cuevel = subjData[\"cuevel\"]\n",
    "    #Find when cue hits cueball\n",
    "    hit_ind=[]\n",
    "    start_ind=[]\n",
    "    n = 10\n",
    "    threshold = 0.1\n",
    "\n",
    "    start_movement = False\n",
    "\n",
    "    prev_trial = 0\n",
    "    for i in range(len(cueballPos)):\n",
    "        #The first 200 timesteps approximately are calibration and parasite movements\n",
    "        if i > 200:   #!= 0:\n",
    "            #New trial started\n",
    "            if cueballPos[\"trial\"].iloc[i] > prev_trial:\n",
    "                if static_for_n_timesteps(cueballvel, redballvel, i, n):\n",
    "                    #Wait for cueball vel y-axis and redball vel y-axis to be zero after new trial started\n",
    "                    start_movement = True\n",
    "                if start_movement == True and np.linalg.norm(cueballvel[[\"x\", \"y\", \"z\"]].iloc[i].to_numpy()) > threshold:\n",
    "                    start_ind = np.append(start_ind, i-3)\n",
    "                    hit_ind = np.append(hit_ind, i+1)\n",
    "                    prev_trial = cueballPos[\"trial\"].iloc[i]\n",
    "\n",
    "    if len(start_ind) != 250 or len(hit_ind) != 250:\n",
    "        print(\"WARNING: either missed a start-hit index, or a trial was not properly recorded\")\n",
    "    for i in range(len(start_ind)):\n",
    "        if start_ind[i] >= hit_ind[i]:\n",
    "            raise ValueError(\"start ind > hit_ind\", i , start_ind[i], hit_ind[i])\n",
    "    return start_ind.astype(int), hit_ind.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error task\n",
    "\n",
    "def importDataError (initials, path, pocket, lbThreshold = -9.364594, ubThreshold = 4.675092):\n",
    "\n",
    "    '''Imports Behavioural Data for a subject performing Error task\n",
    "\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right corresponding to the pocket\n",
    "    - lbThreshold (float), ubThreshold (float): \n",
    "        lower and upper bounds of the interval of possible angles (out of the interval it will be considered outlier)\n",
    "    \n",
    "    Output:\n",
    "    - errorSubjectData (dict): combinations initials (key) - dataframe (value) for each subject with chosen pocket for error task.\n",
    "        Every entry of the dictionary stores three data frames:\n",
    "            1. VRData: behavioural data from the unity AAB\n",
    "            2. angleData: angle of the shot and corresponding trial numbers (250x2)\n",
    "            3. successData: binary variable for success and corresponding trial numbers (250x2)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Declare possible pocket and raise error otherwise\n",
    "    pockets = ['left', 'right']\n",
    "    if pocket.lower() in pockets:\n",
    "        pocket = pocket.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid AAB type. Expected one of: %s\" % pockets)\n",
    "\n",
    "\n",
    "    ### Definition of ideal angle and funnel of pocketing ###\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "\n",
    "    ### Import angle data ###\n",
    "    angleData = pd.read_csv(path + initials + \"/AAB/\" + initials + \"_Angle.txt\", header=None, names = ['Angle'])\n",
    "    angleData['Block'] = np.repeat(range(1,11),25)\n",
    "    # Remove outliers setting value to nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) > ubThreshold, 'Angle'] = np.nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) < lbThreshold, 'Angle'] = np.nan   \n",
    "\n",
    "\n",
    "    ### Import success data ###\n",
    "    successData = pd.read_csv(path + initials + \"/AAB/\" + initials + \"_Success.txt\",  sep = '\\t', header = None, \\\n",
    "        names = ['Block','Trial','Angle','Magnitude','RBPosition'], usecols = [0,1,2,3,5])\n",
    "\n",
    "\n",
    "\n",
    "    ### Import Behavioural Data from Blocks ###\n",
    "    VRData = pd.DataFrame()\n",
    "\n",
    "    # Merge all blocks together\n",
    "    for bl in range(1,11):\n",
    "        block = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Error\" +  pocket + \"_Block\" + str(bl) + \".txt\", sep = '\\t')\n",
    "        # remove shots after 25 trials (done by mistake)\n",
    "        while block['TrialNumber'].iloc[-1]!=25:\n",
    "            block.drop(block.tail(1).index,inplace=True)\n",
    "\n",
    "        block['TrialNumber'] = block['TrialNumber'] + (bl-1)*25  \n",
    "        VRData = pd.concat([VRData, block])\n",
    "\n",
    "\n",
    "    ### Store data in a dictionary ###\n",
    "    errorSubjectData = {}\n",
    "    names = ['VRData','Angle','Success']\n",
    "    dfs = [VRData, angleData, successData]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        errorSubjectData[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###\n",
    "    return(errorSubjectData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reward Task\n",
    "def importDataReward (initials, path, pocket, lbThreshold = -9.364594, ubThreshold = 4.675092):\n",
    "\n",
    "    '''Imports Behavioural Data for a subject performing Reward task\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right corresponding to the pocket\n",
    "    - lbThreshold (float), ubThreshold (float): \n",
    "        lower and upper bounds of the interval of possible angles (out of the interval it will be considered outlier)\n",
    "    \n",
    "    Output:\n",
    "    - rewardSubjectData (dict): combinations initials (key) - dataframe (value) for each subject with chosen pocket for reward task\n",
    "        Every entry of the dictionary stores four data frames:\n",
    "            1. VRData: behavioural data from the unity game\n",
    "            2. angleData: angle of the shot and corresponding trial numbers (250x2)\n",
    "            3. successData: binary variable for success and corresponding trial numbers (250x2)\n",
    "            4. rewardMotivation: reward motivation (Funnel or Median) and corresponding trial numbers (250x2)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Declare possible pocket and raise error otherwise\n",
    "    pockets = ['left', 'right']\n",
    "    if pocket.lower() in pockets:\n",
    "        pocket = pocket.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % pockets)\n",
    "\n",
    "    ### Definition of ideal angle and funnel ###\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "    ### Import angle data ###\n",
    "    angleData = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Angle.txt\", header=None, names = ['Angle'])\n",
    "    angleData['Block'] = np.repeat(range(1,11),25)\n",
    "    # Remove outliers setting values to nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) > ubThreshold, 'Angle'] = np.nan\n",
    "    angleData.loc[(angleData.Angle - idealAngle) < lbThreshold, 'Angle'] = np.nan   \n",
    "\n",
    "    ### Import success data ###\n",
    "    successData = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Success.txt\",  sep = '\\t', header = None, names = ['Block','Trial','Angle','Magnitude','RBPosition'], usecols = [0,1,2,3,5])\n",
    "\n",
    "    ### Import Reward Motivation (funnel or improvement) ###\n",
    "    rewardMotivation = pd.read_csv(path + initials + \"/Game/\" + initials + \"_RewardMotivation.txt\",  sep = '\\t', header = None, names = ['Block', 'Trial','Motivation'])\n",
    "    rewardMotivation['Trial'] = (rewardMotivation['Block']-1)*25 + rewardMotivation['Trial']\n",
    "\n",
    "\n",
    "    ### Import Behavioural Data from Blocks ###\n",
    "    VRData = pd.DataFrame()\n",
    "\n",
    "    for bl in range(1,11):\n",
    "        block = pd.read_csv(path + initials + \"/Game/\" + initials + \"_Reward\" + pocket + \"_Block\" + str(bl) + \".txt\", sep = '\\t')\n",
    "        while block['TrialNumber'].iloc[-1]!=25:\n",
    "            block.drop(block.tail(1).index,inplace=True)\n",
    "        if block['TrialNumber'].iloc[-1]==26:\n",
    "            print(\"block: \", bl,block.tail(1).index)\n",
    "        block['TrialNumber'] = block['TrialNumber'] + (bl-1)*25  \n",
    "        VRData = pd.concat([VRData, block])\n",
    "        #print(\"Block\" + str(bl))\n",
    "\n",
    "    \n",
    "    ### Store data in a dictionary ###\n",
    "    rewardSubjectData = {}\n",
    "    names = ['VRData','Angle','Success','Motivation']\n",
    "    dfs = [VRData, angleData, successData, rewardMotivation]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        rewardSubjectData[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###    \n",
    "    return(rewardSubjectData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing the required module\n",
    "import matplotlib.pyplot as plt\n",
    "\t\n",
    "# x axis values\n",
    "x = [1,2,3]\n",
    "# corresponding y axis values\n",
    "y = [2,4,1]\n",
    "\t\n",
    "# plotting the points\n",
    "plt.plot(x, y)\n",
    "\t\n",
    "# naming the x axis\n",
    "plt.xlabel('x - axis')\n",
    "# naming the y axis\n",
    "plt.ylabel('y - axis')\n",
    "\t\n",
    "# giving a title to my graph\n",
    "plt.title('My first graph!')\n",
    "\t\n",
    "# function to show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "    '''Preprocess data from Pool VR returning position of cue ball, red ball, stick and gaze\n",
    "    \n",
    "    Args:\n",
    "    - dataset (pd.Dataframe): VRData (1st dictionary entry of the output of importDataError or importDataReward)\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): four dataframes for one single subject\n",
    "        1. cbpos: position of the cue ball (X, Y, Z), trial number, time (in the game) and real time (N x 6)\n",
    "        2. rbpos: position of the red ball, trial number, time (in the game) and real time (N x 6)\n",
    "        3. stick: position of the stick, trial number, time (in the game) and real time (N x 6)\n",
    "        4. gaze: position of the gaze, trial number, time (in the game) and real time (N x 6)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    ### Create string variables from 3D vectors ###\n",
    "    \n",
    "    dataset['cueballpos_str'] = dataset['cueballpos'].str.split(',')\n",
    "    dataset['cueballvel_str'] = dataset['cueballvel'].str.split(',')\n",
    "    dataset['redballpos_str'] = dataset['redballpos'].str.split(',')\n",
    "    dataset['redballvel_str'] = dataset['redballvel'].str.split(',')\n",
    "    dataset['cueposfront_str'] = dataset['cueposfront'].str.split(',')\n",
    "    dataset['cueposback_str'] = dataset['cueposback'].str.split(',')\n",
    "    dataset['cuevel_str'] = dataset['cuevel'].str.split(',')\n",
    "    dataset['cuedirection_str'] = dataset['cuedirection'].str.split(',')\n",
    "    dataset['corner5pos_str'] = dataset['corner5pos'].str.split(',')\n",
    "    dataset['corner6pos_str'] = dataset['corner6pos'].str.split(',')\n",
    "    \n",
    "    dataset['stick'] = dataset['optifront'].str.split(',')\n",
    "    dataset['gaze_str'] = dataset['gaze'].str.split(',')\n",
    "\n",
    "    ### Create datasets from string variables ###\n",
    "    cbpos = pd.DataFrame.from_records(np.array(dataset['cueballpos_str']), columns=['x','y','z']).astype(float)\n",
    "    cbvel = pd.DataFrame.from_records(np.array(dataset['cueballvel_str']), columns=['x','y','z']).astype(float)\n",
    "    rbpos = pd.DataFrame.from_records(np.array(dataset['redballpos_str']), columns=['x','y','z']).astype(float)\n",
    "    rbvel = pd.DataFrame.from_records(np.array(dataset['redballvel_str']), columns=['x','y','z']).astype(float)\n",
    "    cueposfront = pd.DataFrame.from_records(np.array(dataset['cueposfront_str']), columns=['x','y','z']).astype(float)\n",
    "    cueposback = pd.DataFrame.from_records(np.array(dataset['cueposback_str']), columns=['x','y','z']).astype(float)\n",
    "    cuedirection = pd.DataFrame.from_records(np.array(dataset['cuedirection_str']), columns=['x','y','z']).astype(float)\n",
    "    cuevel = pd.DataFrame.from_records(np.array(dataset['cuevel_str']), columns=['x','y','z']).astype(float)\n",
    "    corner5pos = pd.DataFrame.from_records(np.array(dataset['corner5pos_str']), columns=['x','y','z']).astype(float)\n",
    "    corner6pos = pd.DataFrame.from_records(np.array(dataset['corner6pos_str']), columns=['x','y','z']).astype(float)\n",
    "\n",
    "    stick = pd.DataFrame.from_records(np.array(dataset['stick']), columns=['x','y','z']).astype(float)\n",
    "    gaze = pd.DataFrame.from_records(np.array(dataset['gaze_str']), columns=['x','y','z']).astype(float)\n",
    "\n",
    "\n",
    "    ### Standardise w.r.t cue ball initial position and add time and trial number ###\n",
    "    x_std, y_std, z_std = cbpos.iloc[0]\n",
    "    for df in (cbpos, rbpos, cueposfront, cueposback, corner5pos, corner6pos, stick, gaze):    # cbvel, rbvel, cuedirection, cuevel, \n",
    "        df -= (x_std, y_std, z_std)\n",
    "\n",
    "        df['trial'] = np.array(dataset['TrialNumber'])\n",
    "        #df['time'] = np.array(dataset['TrialTime']) \n",
    "        #df['timeReal'] = np.array(dataset['GlobalTime'])  \n",
    "    \n",
    "    #We don't want to standardize the velocities with respect to cue ball initial position\n",
    "    for df in (cbvel, rbvel, cuedirection, cuevel):\n",
    "        df['trial'] = np.array(dataset['TrialNumber'])\n",
    "\n",
    "    ### Create a dictionary for saving dataframes and save them ###\n",
    "    subject = {}\n",
    "    names = ['cbpos', 'cbvel', 'rbpos', 'rbvel', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel', 'corner5pos', 'corner6pos', 'stick','gaze']\n",
    "    dfs = [cbpos, cbvel, rbpos, rbvel, cueposfront, cueposback, cuedirection, cuevel, corner5pos, corner6pos, stick, gaze]\n",
    "    counter = 0\n",
    "\n",
    "    for df in names:\n",
    "        subject[df] = dfs[counter]\n",
    "        counter += 1\n",
    "\n",
    "    ### Return the dictionary ###\n",
    "    return(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successTrialsError(subject):\n",
    "\n",
    "    '''Preprocesses and derive successful trials from VR Data correcting the output for error subjects\n",
    "    \n",
    "    Args:\n",
    "    - subject (dict): output of importDataError or importDataReward\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): copy of the input with correct success definition\n",
    "    \n",
    "    '''\n",
    "\n",
    "    idealAngle = 105.0736\n",
    "    lb_idealAngle = 104.5401 - idealAngle\n",
    "    ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "    \n",
    "    ### Derive Successful trials from Angle dataset\n",
    "    subject['Angle']['Success'] = subject['Angle'].Angle.isin(subject['Success']['Angle']).astype(int)\n",
    "    subject['Angle']['Trial'] = range(1,251)\n",
    "    subject['Angle']['AngleStd'] =  subject['Angle']['Angle'] - idealAngle\n",
    "\n",
    "    gameData = preprocess(subject['VRData'])\n",
    "\n",
    "    succTrials = subject['Angle'].Success.index[subject['Angle'].Success == 1] + 1\n",
    "\n",
    "    for tr in succTrials:\n",
    "        cbTrial = gameData['cbpos'][gameData['cbpos'].trial == tr]\n",
    "\n",
    "        if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n",
    "            subject['Angle'].Success.iloc[tr-1] = 0\n",
    "    print(str(subject['Success'].shape[0]-sum(subject['Angle'].Success)) + \" fake successes removed\")\n",
    "\n",
    "\n",
    "    # Correction if missing successes in perturbation phase for error\n",
    "    df = subject['Angle'][subject['Angle']['Block'] > 3]\n",
    "    df = df[df['Block'] < 10]\n",
    "    if df['Success'].sum()==0:\n",
    "      for tr in range(76, 226):\n",
    "          rbTrial = gameData['rbpos'][gameData['rbpos'].trial == tr]\n",
    "          if (np.abs(np.median(rbTrial['y'].tail(10)) - np.median(rbTrial['y'].head(rbTrial.shape[0]-10))) > 0.02) and (np.abs(np.median(rbTrial['y'].tail(10)) - np.median(rbTrial['y'].head(rbTrial.shape[0]-10))) < 0.5):\n",
    "              subject['Angle'].Success.iloc[tr-1] = 1\n",
    "    \n",
    "    ### Returns preprocessed data with correct success\n",
    "    return(subject)\n",
    "\n",
    "\n",
    "def successTrialsReward(subject, preprocessData):\n",
    "\n",
    "  '''Preprocesses and derive successful trials from VR Data correcting the output for reward subjects, \\\n",
    "    creating three new variables in the Angle dataset: SuccessMedian (binary variable for success attributable to the median), \\\n",
    "    SuccessFunnel (binary variable for success attributable to the funnel) and Target (reference angle to be rewarded)\n",
    "  \n",
    "    Args:\n",
    "    - subject (dict): output of importDataError or importDataReward\n",
    "    \n",
    "    Output:\n",
    "    - subject (dict): copy of the input with correct success definition and three new variables in the Angle dataset\n",
    "   \n",
    "  '''\n",
    "  \n",
    "  print(\"Success Derivation function\")\n",
    "\n",
    "  ### Set threshold for funnel ###\n",
    "  idealAngle = 105.0736\n",
    "  lb_idealAngle = 104.5401 - idealAngle\n",
    "  ub_idealAngle = 105.6072 - idealAngle\n",
    "\n",
    "  ### Derive Successful trials from Angle dataset ###\n",
    "  ind = subject['Angle']['Block'].isin([1,2,3,10])\n",
    "\n",
    "  ### Create new variables in Angle dataset: success overall, success due to funnel and success due to improvement ###\n",
    "  subject['Angle']['Success'] = 0\n",
    "  subject['Angle']['SuccessFunnel'] = 0\n",
    "  subject['Angle']['SuccessMedian'] = 0\n",
    "\n",
    "  ### Add success to angle dataset ###\n",
    "  subject['Angle']['Success'][ind] = subject['Angle'].Angle[ind].isin(subject['Success']['Angle']).astype(int)\n",
    "\n",
    "  ### Add trial and standardised angle ###\n",
    "  subject['Angle']['Trial'] = range(1,251)\n",
    "  subject['Angle']['AngleStd'] =  subject['Angle']['Angle'] - idealAngle\n",
    "\n",
    "\n",
    "\n",
    "  ### Preprocess game data ###\n",
    "  #gameData = preprocess(subject['VRData'])\n",
    "\n",
    "  ### True Success for baseline blocks ###\n",
    "  succTrials = subject['Angle'].Success.index[subject['Angle'].Success == 1] + 1\n",
    "\n",
    "  for tr in succTrials:\n",
    "      cbTrial = preprocessData['cbpos'][preprocessData['cbpos'].trial == tr]\n",
    "\n",
    "      if (cbTrial['y'].iloc[0] - cbTrial['y'].iloc[-1]) >= 0.02:\n",
    "          subject['Angle'].Success.iloc[tr-1] = 0\n",
    "\n",
    "  print(str(succTrials.shape[0]-sum(subject['Angle'].Success)) + \" fake successes removed\")\n",
    "\n",
    "  ### Success for perturbation blocks ###\n",
    "  for tr in subject['Motivation'].Trial:\n",
    "    subject['Angle']['Success'].iloc[tr] = 1\n",
    "\n",
    "    if (subject['Motivation'].Motivation[subject['Motivation'].Trial == tr] == 'Median').all():\n",
    "      subject['Angle']['SuccessMedian'][tr] = 1\n",
    "    else:\n",
    "      subject['Angle']['SuccessFunnel'][tr] = 1 \n",
    "\n",
    "\n",
    "  ### Derive target for reward (median of the past 10 successful trials) ###\n",
    "\n",
    "  target = []\n",
    "  vec_median = list(subject['Angle']['AngleStd'][range(66,76)])\n",
    "\n",
    "  for tr in range(76, 226):\n",
    "    if subject['Angle']['Success'].iloc[tr]==1:\n",
    "        target.append(min(ub_idealAngle, max(np.median(vec_median), ub_idealAngle-5)))\n",
    "        vec_median.remove(max(vec_median))\n",
    "        vec_median.append(subject['Angle']['AngleStd'].iloc[tr])\n",
    "    else:\n",
    "        target.append(min(ub_idealAngle, max(np.median(vec_median), ub_idealAngle-5)))\n",
    "\n",
    "  target_overall = pd.Series(np.concatenate([np.repeat(np.nan, 75), target, np.repeat(np.nan, 25)]))\n",
    "\n",
    "  subject['Angle']['Target'] = target_overall\n",
    "  ### Returns preprocessed data with correct success ###\n",
    "  return(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Single Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsSingleSubject(initials, gameType, path, pocket):\n",
    "    '''\n",
    "    Produces results for single subject starting from raw data\n",
    "\n",
    "    Args:\n",
    "    - initials (str): initials of the subject\n",
    "    - gameType (str): error/reward - game mode\n",
    "    - path (str): path of the data\n",
    "    - pocket (str): left/right - corresponding to the pocket\n",
    "    \n",
    "    Output:\n",
    "    - outputDict (dict): dictionary with two entries:\n",
    "        1. Game (dict): game data (same output as importDataError/importDataReward) with success corrected\n",
    "            - VRData: data from the Unity Game\n",
    "            - Angle: shot directional angles for all trials\n",
    "            - Success: features of all successful trials\n",
    "            - Motivation (only for reward task): reason of successful trial (if funnel or improvement of the median)\n",
    "\n",
    "        2. PreProcessed (dict): pre-processed game data\n",
    "            - cbpos: position of the cue ball for all timeframes and all trials\n",
    "            - rbpos: position of the red ball for all timeframes and all trials\n",
    "            - stick: position of the stick for all timeframes and all trials\n",
    "            - gaze: position of the gaze\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print(\"Result Single Subject function\")\n",
    "    # Declare possible gameType and raise error otherwise\n",
    "    game_types = ['error', 'reward']\n",
    "    if gameType.lower() in game_types:\n",
    "        mode = gameType.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % game_types)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Error Mode #####\n",
    "    if mode == 'error':\n",
    "\n",
    "        # Import Data\n",
    "        gameData = importDataError(initials, path, pocket) \n",
    "        ''' \n",
    "        gameData is a dictionary with keys:\n",
    "        - VRData: data from the Unity Game\n",
    "        - Angle: shot directional angles for all trials\n",
    "        - Success: features of all successful trials\n",
    "        '''\n",
    "\n",
    "        # Preprocess Data\n",
    "        preprocData = preprocess(gameData['VRData'])\n",
    "        '''\n",
    "        preprocData is a dictionary with keys:\n",
    "        - cbpos: position of the cue ball for all timeframes and all trials\n",
    "        - rbpos: position of the red ball for all timeframes and all trials\n",
    "        - stick: position of the stick for all timeframes and all trials\n",
    "        - gaze: position of the gaze\n",
    "        '''\n",
    "\n",
    "        # Update of Success in Import Data\n",
    "        gameData = successTrialsError(gameData)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Reward Mode #####\n",
    "\n",
    "    elif mode == 'reward':\n",
    "\n",
    "        # Import Data\n",
    "        gameData = importDataReward(initials, path, pocket) \n",
    "        ''' \n",
    "        gameData is a dictionary with keys:\n",
    "        - VRData: data from the Unity Game\n",
    "        - Angle: shot directional angles for all trials\n",
    "        - Success: features of all successful trials\n",
    "        - Motivation: reason of successful trial (if funnel or improvement of the median)\n",
    "        '''\n",
    "\n",
    "        # Preprocess Data\n",
    "        preprocData = preprocess(gameData['VRData'])\n",
    "        '''\n",
    "        preprocData is a dictionary with keys:\n",
    "        - cbpos: position of the cue ball for all timeframes and all trials\n",
    "        - rbpos: position of the red ball for all timeframes and all trials\n",
    "        - stick: position of the stick for all timeframes and all trials\n",
    "        - gaze: position of the gaze\n",
    "        '''\n",
    "\n",
    "        # Update of Success in Import Data\n",
    "        angleData = successTrialsReward(gameData, preprocData)\n",
    "\n",
    "\n",
    "    ### List of outputs ###\n",
    "    outputDict = {}\n",
    "    outputDict['Game'] = gameData \n",
    "    outputDict['PreProcessed'] = preprocData\n",
    "    outputDict['Angle'] = angleData\n",
    "    return(outputDict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Multiple Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsMultipleSubjects(path, sub, gameType, pocketSide):\n",
    "    '''\n",
    "    Args:\n",
    "    - pathList (list): list of all paths of the folder with raw data\n",
    "    - gameType (str): error/reward - game type \n",
    "    - pocketSide (str): left/right - game pocket\n",
    " \n",
    "    '''\n",
    "\t\n",
    "    print(\"result Multiple Subject function\")\n",
    "    # Declare possible gameType and raise error otherwise\n",
    "    game_types = ['error', 'reward']\n",
    "    if gameType.lower() in game_types:\n",
    "        mode = gameType.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % game_types)\n",
    "\n",
    "    # Call dataframe for both rounds together\n",
    "    dataset = {}\n",
    "    \n",
    "    # Declare possible pocketSide and raise error otherwise\n",
    "    pocketChoice = ['left', 'right', 'all']\n",
    "    if pocketSide.lower() in pocketChoice:\n",
    "        pocket = pocketSide.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid pocket. Expected one of: %s\" % pocketChoice)\n",
    "\n",
    "\n",
    "    print(path, sub)\n",
    "    if mode == 'reward':\n",
    "\n",
    "        # Derive path for a specific subject and game type\n",
    "        pathSubj = path + str(sub)\n",
    "        for fil in range(len(sorted(os.listdir(pathSubj + '/Game/')))):\n",
    "            if sorted(os.listdir(pathSubj + '/Game/'))[fil].find(\"Block2\") > -1:\n",
    "                blockFile = sorted(os.listdir(pathSubj + '/Game/'))[fil]\n",
    "\n",
    "\n",
    "        if (blockFile.find('Left') > -1 and pocket != 'right'):\n",
    "            pocketSub = 'Left'\n",
    "        elif (blockFile.find('Right') > -1 and pocket != 'left'):\n",
    "            pocketSub = 'Right'\n",
    "        else:\n",
    "            pocketSub = 'None'\n",
    "\n",
    "        if blockFile.find('Reward') > -1:\n",
    "            # Derive initials of the subject\n",
    "            initials = os.path.basename(pathSubj)\n",
    "\n",
    "            # Derive All Results for One Subject\n",
    "            subjData = resultsSingleSubject(initials, 'reward', path, pocketSub)\n",
    "\n",
    "            # Add data to dataframes previously defined\n",
    "            dataset[str(initials)]={}\n",
    "\n",
    "            dataset[str(initials)][\"rewards\"] = def_reward(subjData[\"Game\"][\"Angle\"][\"SuccessMedian\"], subjData[\"Game\"][\"Angle\"][\"SuccessFunnel\"])\n",
    "\n",
    "\n",
    "            dataset[str(initials)][\"cueballpos\"] = subjData[\"PreProcessed\"][\"cbpos\"]\n",
    "            dataset[str(initials)][\"cueballvel\"] = subjData[\"PreProcessed\"][\"cbvel\"]\n",
    "            \n",
    "            dataset[str(initials)][\"start_ind\"], dataset[str(initials)][\"hit_ind\"] = start_hit_reduced_timesteps(subjData[\"PreProcessed\"])  #start_hit_timesteps(subjData[\"PreProcessed\"])\n",
    "        \n",
    "            dataset[str(initials)][\"redballpos\"] = subjData[\"PreProcessed\"][\"rbpos\"]\n",
    "            \n",
    "            if pocketSub == 'Left':\n",
    "                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner5pos\"]\n",
    "            else:\n",
    "                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner6pos\"]\n",
    "\n",
    "            dataset[str(initials)][\"cueposfront\"] = subjData[\"PreProcessed\"][\"cueposfront\"]\n",
    "\n",
    "            dataset[str(initials)][\"cueposback\"] = subjData[\"PreProcessed\"][\"cueposback\"]\n",
    "\n",
    "            dataset[str(initials)][\"cuedirection\"] = subjData[\"PreProcessed\"][\"cuedirection\"]\n",
    "\n",
    "            dataset[str(initials)][\"cuevel\"] = subjData[\"PreProcessed\"][\"cuevel\"]\n",
    "\n",
    "            #dataset[str(initials)][\"rewards\"] = def_reward(subjData['Game'][\"Angle\"][\"Success\"], subjData[\"Game\"][\"Angle\"][\"SuccessFunnel\"], subjData[\"Game\"][\"Angle\"][\"SuccessMedian\"])\n",
    "\n",
    "            print(\"Imported \" + initials + \" as reward subject \" + \"for \" + pocketSub + \" pocket\" + \" in round: \" + path)\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def resultsMultipleSubjects(pathList, gameType, pocketSide):\\n    \\'\\'\\'\\n    Args:\\n    - pathList (list): list of all paths of the folder with raw data\\n    - gameType (str): error/reward - game type \\n    - pocketSide (str): left/right - game pocket\\n \\n    \\'\\'\\'\\n\\t\\n    print(\"result Multiple Subject function\")\\n    # Declare possible gameType and raise error otherwise\\n    game_types = [\\'error\\', \\'reward\\']\\n    if gameType.lower() in game_types:\\n        mode = gameType.lower()\\n    else:\\n        raise ValueError(\"Invalid game type. Expected one of: %s\" % game_types)\\n\\n    # Call dataframe for both rounds together\\n    dataset = {}\\n    \\n    # Declare possible pocketSide and raise error otherwise\\n    pocketChoice = [\\'left\\', \\'right\\', \\'all\\']\\n    if pocketSide.lower() in pocketChoice:\\n        pocket = pocketSide.lower()\\n    else:\\n        raise ValueError(\"Invalid pocket. Expected one of: %s\" % pocketChoice)\\n\\n\\n    k=0\\n    for path in pathList:\\n        for sub in next(os.walk(path))[1]:\\n            print(sub)\\n            if k < 1:\\n                if mode == \\'reward\\':\\n\\n                    # Derive path for a specific subject and game type\\n                    pathSubj = path + str(sub)\\n                    for fil in range(len(sorted(os.listdir(pathSubj + \\'/Game/\\')))):\\n                        if sorted(os.listdir(pathSubj + \\'/Game/\\'))[fil].find(\"Block2\") > -1:\\n                            blockFile = sorted(os.listdir(pathSubj + \\'/Game/\\'))[fil]\\n\\n\\n                    if (blockFile.find(\\'Left\\') > -1 and pocket != \\'right\\'):\\n                        pocketSub = \\'Left\\'\\n                    elif (blockFile.find(\\'Right\\') > -1 and pocket != \\'left\\'):\\n                        pocketSub = \\'Right\\'\\n                    else:\\n                        pocketSub = \\'None\\'\\n\\n                    if blockFile.find(\\'Reward\\') > -1:\\n\\n                        if pocketSub == \\'None\\':\\n                            continue\\n                        else:\\n                            # Derive initials of the subject\\n                            initials = os.path.basename(pathSubj)\\n\\n                            # Derive All Results for One Subject\\n                            subjData = resultsSingleSubject(initials, \\'reward\\', path, pocketSub)\\n\\n                            # Add data to dataframes previously defined\\n                            dataset[str(initials)]={}\\n\\n                            dataset[str(initials)][\"rewards\"] = def_reward(subjData[\"Game\"][\"Angle\"][\"SuccessMedian\"], subjData[\"Game\"][\"Angle\"][\"SuccessFunnel\"])\\n             \\n\\n                            dataset[str(initials)][\"cueballpos\"] = subjData[\"PreProcessed\"][\"cbpos\"]\\n                            \\n                            dataset[str(initials)][\"start_ind\"], dataset[str(initials)][\"hit_ind\"] = start_hit_timesteps(subjData[\"PreProcessed\"])   #, subjData[\"Game\"][\"VRData\"][\"TrialNumber\"])\\n                        \\n                            dataset[str(initials)][\"redballpos\"] = subjData[\"PreProcessed\"][\"rbpos\"]\\n                            \\n                            if pocketSub == \\'Left\\':\\n                                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner5pos\"]\\n                            else:\\n                                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner6pos\"]\\n\\n                            dataset[str(initials)][\"cueposfront\"] = subjData[\"PreProcessed\"][\"cueposfront\"]\\n\\n                            dataset[str(initials)][\"cueposback\"] = subjData[\"PreProcessed\"][\"cueposback\"]\\n\\n                            dataset[str(initials)][\"cuedirection\"] = subjData[\"PreProcessed\"][\"cuedirection\"]\\n\\n                            dataset[str(initials)][\"cuevel\"] = subjData[\"PreProcessed\"][\"cuevel\"]\\n\\n                            #dataset[str(initials)][\"rewards\"] = def_reward(subjData[\\'Game\\'][\"Angle\"][\"Success\"], subjData[\"Game\"][\"Angle\"][\"SuccessFunnel\"], subjData[\"Game\"][\"Angle\"][\"SuccessMedian\"])\\n             \\n                            print(\"Imported \" + initials + \" as reward subject \" + \"for \" + pocketSub + \" pocket\" + \" in round: \" + path)\\n                            k+=1\\n            else:\\n                break\\n            \\n    return dataset'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def resultsMultipleSubjects(pathList, gameType, pocketSide):\n",
    "    '''\n",
    "    Args:\n",
    "    - pathList (list): list of all paths of the folder with raw data\n",
    "    - gameType (str): error/reward - game type \n",
    "    - pocketSide (str): left/right - game pocket\n",
    " \n",
    "    '''\n",
    "\t\n",
    "    print(\"result Multiple Subject function\")\n",
    "    # Declare possible gameType and raise error otherwise\n",
    "    game_types = ['error', 'reward']\n",
    "    if gameType.lower() in game_types:\n",
    "        mode = gameType.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid game type. Expected one of: %s\" % game_types)\n",
    "\n",
    "    # Call dataframe for both rounds together\n",
    "    dataset = {}\n",
    "    \n",
    "    # Declare possible pocketSide and raise error otherwise\n",
    "    pocketChoice = ['left', 'right', 'all']\n",
    "    if pocketSide.lower() in pocketChoice:\n",
    "        pocket = pocketSide.lower()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid pocket. Expected one of: %s\" % pocketChoice)\n",
    "\n",
    "\n",
    "    k=0\n",
    "    for path in pathList:\n",
    "        for sub in next(os.walk(path))[1]:\n",
    "            print(sub)\n",
    "            if k < 1:\n",
    "                if mode == 'reward':\n",
    "\n",
    "                    # Derive path for a specific subject and game type\n",
    "                    pathSubj = path + str(sub)\n",
    "                    for fil in range(len(sorted(os.listdir(pathSubj + '/Game/')))):\n",
    "                        if sorted(os.listdir(pathSubj + '/Game/'))[fil].find(\"Block2\") > -1:\n",
    "                            blockFile = sorted(os.listdir(pathSubj + '/Game/'))[fil]\n",
    "\n",
    "\n",
    "                    if (blockFile.find('Left') > -1 and pocket != 'right'):\n",
    "                        pocketSub = 'Left'\n",
    "                    elif (blockFile.find('Right') > -1 and pocket != 'left'):\n",
    "                        pocketSub = 'Right'\n",
    "                    else:\n",
    "                        pocketSub = 'None'\n",
    "\n",
    "                    if blockFile.find('Reward') > -1:\n",
    "\n",
    "                        if pocketSub == 'None':\n",
    "                            continue\n",
    "                        else:\n",
    "                            # Derive initials of the subject\n",
    "                            initials = os.path.basename(pathSubj)\n",
    "\n",
    "                            # Derive All Results for One Subject\n",
    "                            subjData = resultsSingleSubject(initials, 'reward', path, pocketSub)\n",
    "\n",
    "                            # Add data to dataframes previously defined\n",
    "                            dataset[str(initials)]={}\n",
    "\n",
    "                            dataset[str(initials)][\"rewards\"] = def_reward(subjData[\"Game\"][\"Angle\"][\"SuccessMedian\"], subjData[\"Game\"][\"Angle\"][\"SuccessFunnel\"])\n",
    "             \n",
    "\n",
    "                            dataset[str(initials)][\"cueballpos\"] = subjData[\"PreProcessed\"][\"cbpos\"]\n",
    "                            \n",
    "                            dataset[str(initials)][\"start_ind\"], dataset[str(initials)][\"hit_ind\"] = start_hit_timesteps(subjData[\"PreProcessed\"])   #, subjData[\"Game\"][\"VRData\"][\"TrialNumber\"])\n",
    "                        \n",
    "                            dataset[str(initials)][\"redballpos\"] = subjData[\"PreProcessed\"][\"rbpos\"]\n",
    "                            \n",
    "                            if pocketSub == 'Left':\n",
    "                                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner5pos\"]\n",
    "                            else:\n",
    "                                dataset[str(initials)][\"targetcornerpos\"] = subjData[\"PreProcessed\"][\"corner6pos\"]\n",
    "\n",
    "                            dataset[str(initials)][\"cueposfront\"] = subjData[\"PreProcessed\"][\"cueposfront\"]\n",
    "\n",
    "                            dataset[str(initials)][\"cueposback\"] = subjData[\"PreProcessed\"][\"cueposback\"]\n",
    "\n",
    "                            dataset[str(initials)][\"cuedirection\"] = subjData[\"PreProcessed\"][\"cuedirection\"]\n",
    "\n",
    "                            dataset[str(initials)][\"cuevel\"] = subjData[\"PreProcessed\"][\"cuevel\"]\n",
    "\n",
    "                            #dataset[str(initials)][\"rewards\"] = def_reward(subjData['Game'][\"Angle\"][\"Success\"], subjData[\"Game\"][\"Angle\"][\"SuccessFunnel\"], subjData[\"Game\"][\"Angle\"][\"SuccessMedian\"])\n",
    "             \n",
    "                            print(\"Imported \" + initials + \" as reward subject \" + \"for \" + pocketSub + \" pocket\" + \" in round: \" + path)\n",
    "                            k+=1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    return dataset\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Reduced timesteps Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_Reduced(data, cue_data, cuevel, rewards, state_dim=14, action_dim=2):   #state_dim should be a multiple of 6\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = len(data[\"cueballpos\"][\"trial\"]) -1   #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    new_state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    action = np.zeros(action_dim)\n",
    "    #reward = np.zeros(1)\n",
    "    #ball_states=6\n",
    "    #cue_states=6\n",
    "    #cue_states_iterations = int((state_dim-ball_states)/(cue_states))\n",
    "\n",
    "    #Action Velocity, Force?\n",
    "    #See car 2D -> action space acceleration and cueDirection\n",
    "    actions = compute_impulseForce(cuevel[[\"x\", \"z\"]], cue_data[\"cuedirection\"][[\"x\", \"z\"]])\n",
    "        \n",
    "    for i in range(N):\n",
    "        count=0\n",
    "        count2=0\n",
    "        for x in data:\n",
    "            state[count] = data[str(x)][\"x\"].iloc[i]\n",
    "            state[count+1] = data[str(x)][\"z\"].iloc[i]\n",
    "            new_state[count] = data[str(x)][\"x\"].iloc[i+1]\n",
    "            new_state[count+1] = data[str(x)][\"z\"].iloc[i+1]\n",
    "            count+=2\n",
    "\n",
    "        #Add last j timesteps of the cuepos to states\n",
    "        #3 observations on x and z axis makes 6 observations\n",
    "        #for j in range(cue_states_iterations): #Warning if state_dim not multiple of 6, division fail for loop\n",
    "        for x in cue_data:\n",
    "            state[count2] = cue_data[str(x)][\"x\"].iloc[i]#-j]\n",
    "            state[count2+1] = cue_data[str(x)][\"z\"].iloc[i]#-j]\n",
    "            new_state[count2] = cue_data[str(x)][\"x\"].iloc[i+1]#-j]\n",
    "            new_state[count2+1] = cue_data[str(x)][\"z\"].iloc[i+1]#-j]\n",
    "            count2+=2\n",
    "   \n",
    "        #Action Velocity, Force?\n",
    "        action = actions[i][:]\n",
    "        #reward = rewards[i]\n",
    "        \n",
    "        if data['cueballpos']['trial'].iloc[i+1] != data['cueballpos']['trial'].iloc[i]:\n",
    "            done_bool = True\n",
    "            reward = rewards.iloc[i]\n",
    "        else:\n",
    "            done_bool = False\n",
    "            reward=0\n",
    "\n",
    "        trial_.append(data[\"cueballpos\"][\"trial\"].iloc[i])\n",
    "        state_.append(state.copy())\n",
    "        new_state_.append(new_state.copy())\n",
    "        action_.append(action.copy())\n",
    "        reward_.append(reward)\n",
    "        done_.append(done_bool)\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }\n",
    "\n",
    "def compute_impulseForce_load(self, cuevel, cuedirection):\n",
    "    impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "    shotMagnitude = np.zeros(1)\n",
    "    shotDir = np.zeros(cuedirection.shape)\n",
    "    #Reward: magnitude range\n",
    "    lbMagnitude = 0.4   #0.516149\n",
    "    ubMagnitude = 0.882607\n",
    "\n",
    "    shotMagnitude = np.linalg.norm(cuevel, axis=1)\n",
    "\n",
    "    for i in range(cuevel.shape[0]):\n",
    "        if shotMagnitude[i] > ubMagnitude:\n",
    "            shotMagnitude[i] = ubMagnitude\n",
    "            #print(\"upper bounded\")\n",
    "        #elif shotMagnitude[i] > lbMagnitude:\n",
    "            #print(i, shotMagnitude[i])\n",
    "        elif shotMagnitude[i] < lbMagnitude:\n",
    "            shotMagnitude[i] = 0\n",
    "\n",
    "        shotDir[i][:] = cuedirection[i][:]\n",
    "        if shotMagnitude[i] == 0:\n",
    "            impulseForce[i][:] = 0\n",
    "        else:\n",
    "            impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "    return impulseForce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline dict One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_One(data, cue_data, cuevel, rewards, state_dim=24, action_dim=2):   #state_dim should be a multiple of 6\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = len(data[\"cueballpos\"][\"trial\"]) -1   #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    new_state = np.zeros(state_dim)  #np.zeros(21)\n",
    "    action = np.zeros(action_dim)\n",
    "    #reward = np.zeros(1)\n",
    "    #gamma = 0.6\n",
    "    ball_states=6\n",
    "    cue_states=6\n",
    "    cue_states_iterations = int((state_dim-ball_states)/(cue_states))\n",
    "    #Action Velocity, Force?\n",
    "    #See car 2D -> action space acceleration and cueDirection\n",
    "    actions = compute_impulseForce(cuevel[[\"x\", \"z\"]], cue_data[\"cuedirection\"][[\"x\", \"z\"]])\n",
    "        \n",
    "    for i in range(N):\n",
    "        count=0\n",
    "        count2=0\n",
    "        for x in data:\n",
    "            state[count] = data[str(x)][\"x\"].iloc[i]\n",
    "            state[count+1] = data[str(x)][\"z\"].iloc[i]\n",
    "            new_state[count] = data[str(x)][\"x\"].iloc[i+1]\n",
    "            new_state[count+1] = data[str(x)][\"z\"].iloc[i+1]\n",
    "            count+=2\n",
    "\n",
    "        #Add last j timesteps of the cuepos to states\n",
    "        #3 observations on x and z axis makes 6 observations\n",
    "        for j in range(cue_states_iterations): #Warning if state_dim not multiple of 6, division fail for loop\n",
    "            for x in cue_data:\n",
    "                state[count2] = cue_data[str(x)][\"x\"].iloc[i-j]\n",
    "                state[count2+1] = cue_data[str(x)][\"z\"].iloc[i-j]\n",
    "                new_state[count2] = cue_data[str(x)][\"x\"].iloc[i+1-j]\n",
    "                new_state[count2+1] = cue_data[str(x)][\"z\"].iloc[i+1-j]\n",
    "                count2+=2\n",
    "   \n",
    "        #Action Velocity, Force?\n",
    "        action = actions[i][:]\n",
    "        #reward = rewards[i]\n",
    "        \n",
    "        if data['cueballpos']['trial'].iloc[i+1] != data['cueballpos']['trial'].iloc[i]:\n",
    "            done_bool = True\n",
    "            reward = rewards.iloc[i]\n",
    "        else:\n",
    "            done_bool = False\n",
    "            reward=0\n",
    "\n",
    "        trial_.append(data[\"cueballpos\"][\"trial\"].iloc[i])\n",
    "        state_.append(state.copy())\n",
    "        new_state_.append(new_state.copy())\n",
    "        action_.append(action.copy())\n",
    "        reward_.append(reward)\n",
    "        done_.append(done_bool)\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }\n",
    "\n",
    "def compute_impulseForce_load(self, cuevel, cuedirection):\n",
    "    impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "    shotMagnitude = np.zeros(1)\n",
    "    shotDir = np.zeros(cuedirection.shape)\n",
    "    #Reward: magnitude range\n",
    "    lbMagnitude = 0.4   #0.516149\n",
    "    ubMagnitude = 0.882607\n",
    "\n",
    "    shotMagnitude = np.linalg.norm(cuevel, axis=1)\n",
    "\n",
    "    for i in range(cuevel.shape[0]):\n",
    "        if shotMagnitude[i] > ubMagnitude:\n",
    "            shotMagnitude[i] = ubMagnitude\n",
    "            #print(\"upper bounded\")\n",
    "        #elif shotMagnitude[i] > lbMagnitude:\n",
    "            #print(i, shotMagnitude[i])\n",
    "        elif shotMagnitude[i] < lbMagnitude:\n",
    "            shotMagnitude[i] = 0\n",
    "\n",
    "        shotDir[i][:] = cuedirection[i][:]\n",
    "        if shotMagnitude[i] == 0:\n",
    "            impulseForce[i][:] = 0\n",
    "        else:\n",
    "            impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "    return impulseForce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL One dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_One_big_dict(data,initial, terminate_on_end=False):\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = data[initial][\"cueballpos\"][\"trial\"].iloc[-1]    #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "    trial_ = []\n",
    "\n",
    "    state = np.zeros(14)  #np.zeros(21)\n",
    "    new_state = np.zeros(14)  #np.zeros(21)\n",
    "    action = np.zeros(2)\n",
    "    reward = np.zeros(1)\n",
    "    #gamma = 0.6\n",
    "    episode_step=0\n",
    "    for i in range(N):\n",
    "        for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]+1, 1):\n",
    "            #state timestep t\n",
    "            state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j] \n",
    "            #state[0] = data[initial][\"cueballpos\"][y].iloc[j] \n",
    "            state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j] \n",
    "            state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j] \n",
    "            #state[2] = data[initial][\"redballpos\"][y].iloc[j]\n",
    "            state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j]\n",
    "            #state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j]\n",
    "            state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j]\n",
    "            state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j]\n",
    "            #state[6] = data[initial][\"cueposfront\"][y].iloc[j]\n",
    "            state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j]\n",
    "            state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j]\n",
    "            #state[8] = data[initial][\"cueposback\"][y].iloc[j]\n",
    "            state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j]\n",
    "            state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j]\n",
    "            #state[10] = data[initial][\"cueDirection\"][y].iloc[j]\n",
    "            state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j]\n",
    "            state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j]\n",
    "            state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j]\n",
    "\n",
    "            #state Timestep t+1\n",
    "            new_state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[0] = data[initial][\"cueballpos\"][y].iloc[j+1] \n",
    "            new_state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j+1] \n",
    "            new_state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[2] = data[initial][\"redballpos\"][y].iloc[j+1]\n",
    "            new_state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j+1]\n",
    "            #new_state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j+1]\n",
    "            new_state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j+1]\n",
    "            #new_state[6] = data[initial][\"cueposfront\"][y].iloc[j+1]\n",
    "            new_state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j+1]\n",
    "            new_state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j+1]\n",
    "            #new_state[8] = data[initial][\"cueposback\"][y].iloc[j+1]\n",
    "            new_state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j+1]\n",
    "            new_state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j+1]\n",
    "            #new_state[10] = data[initial][\"cueDirection\"][y].iloc[j+1]\n",
    "            new_state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j+1]\n",
    "            new_state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j+1]\n",
    "            new_state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j+1]\n",
    "\n",
    "            #Action Velocity, Force?\n",
    "            action = compute_impulseForce(data[initial][\"cuevel\"][[\"x\", \"z\"]].iloc[j], data[initial][\"cuedirection\"][[\"x\", \"z\"]].iloc[j])\n",
    "            #action[0] = data[initial][\"cuevel\"][\"y\"].iloc[j]\n",
    "            \n",
    "\n",
    "            if j == data[initial][\"hit_ind\"][i]:\n",
    "                done_bool = True\n",
    "                reward = data[initial][\"rewards\"][i]\n",
    "            else:\n",
    "                done_bool = False\n",
    "                ## Discounted reward ##\n",
    "                #reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "                reward = 0\n",
    "\n",
    "            final_timestep = (episode_step == data[initial][\"hit_ind\"][i]-1)\n",
    "            if (not terminate_on_end) and final_timestep:\n",
    "                # Skip this transition and don't apply terminals on the last step of an episode\n",
    "                episode_step = 0\n",
    "                continue\n",
    "            if done_bool or final_timestep:\n",
    "                episode_step = 0\n",
    "\n",
    "            trial_.append(data[initial][\"cueballpos\"][\"trial\"].iloc[j] )\n",
    "            state_.append(state)\n",
    "            new_state_.append(new_state)\n",
    "            action_.append(action)\n",
    "            reward_.append(reward)\n",
    "            done_.append(done_bool)\n",
    "            episode_step += 1\n",
    "    \n",
    "    return {\n",
    "        'trial': np.array(trial_),\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_states': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline load class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Offline_RL_dataset(object): #rewards,cueballpos,redballpos, targetcornerpos, cueposfront, cueposback, cuedirection, cuevel,\n",
    "    def __init__(self, state_dim=14, action_dim=2, nb_trial=250):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #number of trial\n",
    "        self.N = nb_trial    #int(cueballpos[0].iloc[-1])\n",
    "        self.state_dim = state_dim  \n",
    "        self.action_dim = action_dim\n",
    "        self.trajectories = []\n",
    "        \n",
    "        self.mean = np.zeros(self.state_dim)\n",
    "        self.std = np.zeros(self.state_dim)\n",
    "\n",
    "    def get_trajectories(self, list_data, rewards):\n",
    "        \n",
    "        state_ = []\n",
    "        new_state_ = []\n",
    "        action_ = []\n",
    "        reward_ = []\n",
    "        done_ = []\n",
    "\n",
    "        state = np.zeros(self.state_dim)\n",
    "        new_state = np.zeros(self.state_dim) \n",
    "        action = np.zeros(self.action_dim)\n",
    "        reward = np.zeros(1)\n",
    "\n",
    "        actions = self.compute_impulseForce_load(np.transpose(np.array([list_data[\"cuevel\"][1], list_data[\"cuevel\"][3]])), np.transpose(np.array([list_data[\"cuedirection\"][1], list_data[\"cuedirection\"][3]])))\n",
    "\n",
    "        for i in range(len(list_data[\"cueballpos\"])-1):\n",
    "            count=0\n",
    "            for x in list_data:\n",
    "                state[count] = list_data[str(x)][1].iloc[i]\n",
    "                state[count+1] = list_data[str(x)][3].iloc[i]\n",
    "                new_state[count] = list_data[str(x)][1].iloc[i+1]\n",
    "                new_state[count+1] = list_data[str(x)][3].iloc[i+1]\n",
    "                count+=2\n",
    "            #Action Velocity, Force?\n",
    "            action = actions[i][:]\n",
    "            reward = rewards.iloc[i]\n",
    "            \n",
    "            if cueballpos[0].iloc[i+1] != cueballpos[0].iloc[i]:\n",
    "                done_bool = True\n",
    "            else:\n",
    "                done_bool = False\n",
    "\n",
    "            state_.append(state.copy())\n",
    "            new_state_.append(new_state.copy())\n",
    "            action_.append(action.copy())\n",
    "            reward_.append(reward.copy())\n",
    "            done_.append(done_bool)\n",
    "\n",
    "            if list_data[\"cueballpos\"][0].iloc[i+1] != list_data[\"cueballpos\"][0].iloc[i]:\n",
    "                self.trajectories.append({\n",
    "                'size': np.array(state_).shape[0],\n",
    "                'states': np.array(state_),\n",
    "                'actions': np.array(action_),\n",
    "                'new_states': np.array(new_state_),\n",
    "                'rewards': np.array(reward_),\n",
    "                'terminals': np.array(done_),\n",
    "                })\n",
    "\n",
    "    def sample(self):   #, batch_size = 4):   #bacth size is the number of trajectory processed before gradient update\n",
    "        max_size = self.N\n",
    "        ind = 1 #np.random.randint(0, max_size)    #, size=batch_size)\n",
    "        return (torch.FloatTensor(self.trajectories[ind]['size']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['states']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['actions']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['new_states']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['rewards']).to(self.device),\n",
    "            torch.FloatTensor(self.trajectories[ind]['terminals']).to(self.device))\n",
    "\n",
    "    def compute_impulseForce_load(self, cuevel, cuedirection):\n",
    "        impulseForce = np.zeros(cuevel.shape)       #(N,2)\n",
    "        shotMagnitude = np.zeros(1)\n",
    "        shotDir = np.zeros(cuedirection.shape)\n",
    "        #Reward: magnitude range\n",
    "        lbMagnitude = 0.4   #0.516149\n",
    "        ubMagnitude = 0.882607\n",
    "\n",
    "        shotMagnitude = np.linalg.norm(cuevel, axis=1)\n",
    "\n",
    "        for i in range(cuevel.shape[0]):\n",
    "            if shotMagnitude[i] > ubMagnitude:\n",
    "                shotMagnitude[i] = ubMagnitude\n",
    "                #print(\"upper bounded\")\n",
    "            #elif shotMagnitude[i] > lbMagnitude:\n",
    "                #print(i, shotMagnitude[i])\n",
    "            elif shotMagnitude[i] < lbMagnitude:\n",
    "                shotMagnitude[i] = 0\n",
    "\n",
    "            shotDir[i][:] = cuedirection[i][:]\n",
    "            if shotMagnitude[i] == 0:\n",
    "                impulseForce[i][:] = 0\n",
    "            else:\n",
    "                impulseForce[i][:] = shotMagnitude[i] * shotDir[i][:]\n",
    "        return impulseForce\n",
    "\n",
    "    def compute_mean_std(self, list_data, eps = 1e-3):\n",
    "        count = 0\n",
    "        for i,x in enumerate(list_data):\n",
    "            self.mean[count] = list_data[str(x)][1].mean(0)  #, keepdim=True)\n",
    "            self.mean[count+1] = list_data[str(x)][3].mean(0)\n",
    "\n",
    "            self.std[count] = list_data[str(x)][1].std(0)  #, keepdim=True)\n",
    "            self.std[count+1] = list_data[str(x)][3].std(0)\n",
    "            count += 2\n",
    "\n",
    "    def normalize_states(self, eps = 1e-3):\n",
    "        for i in range(2):  #self.N\n",
    "            for j in range(self.state_dim):\n",
    "                self.trajectories[i]['states'][:][j] = (self.trajectories[i]['states'][:][j] - self.mean[j])/self.std[j]\n",
    "                self.trajectories[i]['new_states'][:][j] = (self.trajectories[i]['new_states'][:][j] - self.mean[j])/self.std[j]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_RL_dataset(data,initial, terminate_on_end=False):\n",
    "    \"\"\"\n",
    "        Returns datasets formatted for use by standard Q-learning algorithms,\n",
    "        with states, actions, next_states, rewards, and a terminal\n",
    "        flag.\n",
    "\n",
    "        Args:\n",
    "            cueballPos, redballPos, targetcornerPos, cuePosfront, cuePosback: Recorded stateervation States (for each subject)\n",
    "            cueVel: Recorded Action new_state (for each Subject)\n",
    "            reward: np.array with one reward per trial\n",
    "            start_ind: starting index of each trial\n",
    "            hit_ind: hitting the cue ball index for each trial\n",
    "            target_corner: string indicating the corner in which to pocket\n",
    "            terminate_on_end (bool): Set done=True on the last timestep\n",
    "            in a trajectory. Default is False, and will discard the\n",
    "            last timestep in each trajectory.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing keys:\n",
    "                states: An N x dim_state array of states.\n",
    "                actions: An N x dim_action array of actions.\n",
    "                next_states: An N x dim_state array of next states.\n",
    "                rewards: An N-dim float array of rewards.\n",
    "                terminals: An N-dim boolean array of \"done\" or episode termination flags.\n",
    "        \"\"\"\n",
    "    \n",
    "    print(\"Offline RL Dataset function\")\n",
    "    #number of trial\n",
    "    N = data[\"AAB\"][\"cueballpos\"][\"trial\"].iloc[-1]    #len(data[initial][\"start_ind\"])\n",
    "    state_ = []\n",
    "    new_state_ = []\n",
    "    action_ = []\n",
    "    reward_ = []\n",
    "    done_ = []\n",
    "\n",
    "    state = np.zeros(14)  #np.zeros(21)\n",
    "    new_state = np.zeros(14)  #np.zeros(21)\n",
    "    action = np.zeros(2)\n",
    "    reward = np.zeros(1)\n",
    "    gamma = 0.6 #discounted reward\n",
    "    # The newer version of the dataset adds an explicit\n",
    "    # timeouts field. Keep old method for backwards compatability.\n",
    "    #use_timeouts = False\n",
    "    #if 'timeouts' in dataset:\n",
    "        #use_timeouts = True\n",
    "    \n",
    "    '''terminate_on_end (bool): Set done=True on the last timestep\n",
    "        in a trajectory. Default is False, and will discard the\n",
    "        last timestep in each trajectory.'''\n",
    "\n",
    "    episode_step = 0\n",
    "    trajectories = []\n",
    "    for i in range(N):\n",
    "        \"\"\"\n",
    "        episode = {'true_state': [],\n",
    "                   'true_next_state': [],\n",
    "                   'x': [],\n",
    "                   'a': [],\n",
    "                   'r': [],\n",
    "                   'x_prime': [],\n",
    "                   'done': [],\n",
    "                   'base_propensity': [],\n",
    "                   'target_propensity': [],\n",
    "                   'frames': [],\n",
    "                   'extra_propensity': []}\n",
    "        \"\"\"\n",
    "        for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]+1, 1):\n",
    "            #state timestep t\n",
    "            state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j] \n",
    "            #state[0] = data[initial][\"cueballpos\"][y].iloc[j] \n",
    "            state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j] \n",
    "            state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j] \n",
    "            #state[2] = data[initial][\"redballpos\"][y].iloc[j]\n",
    "            state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j]\n",
    "            state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j]\n",
    "            state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j]\n",
    "            state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j]\n",
    "            #state[6] = data[initial][\"cueposfront\"][y].iloc[j]\n",
    "            state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j]\n",
    "            state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j]\n",
    "            #state[8] = data[initial][\"cueposback\"][y].iloc[j]\n",
    "            state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j]\n",
    "            state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j]\n",
    "            #state[10] = data[initial][\"cueDirection\"][y].iloc[j]\n",
    "            state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j]\n",
    "            state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j]\n",
    "            state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j]\n",
    "\n",
    "            #state Timestep t+1\n",
    "            new_state[0] = data[initial][\"cueballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[0] = data[initial][\"cueballpos\"][y].iloc[j+1] \n",
    "            new_state[1] = data[initial][\"cueballpos\"][\"z\"].iloc[j+1] \n",
    "            new_state[2] = data[initial][\"redballpos\"][\"x\"].iloc[j+1] \n",
    "            #new_state[2] = data[initial][\"redballpos\"][y].iloc[j+1]\n",
    "            new_state[3] = data[initial][\"redballpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"x\"].iloc[j+1]\n",
    "            new_state[4] = data[initial][\"targetcornerpos\"][\"y\"].iloc[j+1]\n",
    "            new_state[5] = data[initial][\"targetcornerpos\"][\"z\"].iloc[j+1]\n",
    "            new_state[6] = data[initial][\"cueposfront\"][\"x\"].iloc[j+1]\n",
    "            #new_state[6] = data[initial][\"cueposfront\"][y].iloc[j+1]\n",
    "            new_state[7] = data[initial][\"cueposfront\"][\"z\"].iloc[j+1]\n",
    "            new_state[8] = data[initial][\"cueposback\"][\"x\"].iloc[j+1]\n",
    "            #new_state[8] = data[initial][\"cueposback\"][y].iloc[j+1]\n",
    "            new_state[9] = data[initial][\"cueposback\"][\"z\"].iloc[j+1]\n",
    "            new_state[10] = data[initial][\"cuedirection\"][\"x\"].iloc[j+1]\n",
    "            #new_state[10] = data[initial][\"cueDirection\"][y].iloc[j+1]\n",
    "            new_state[11] = data[initial][\"cuedirection\"][\"z\"].iloc[j+1]\n",
    "            new_state[12] = data[initial][\"cuevel\"][\"x\"].iloc[j+1]\n",
    "            new_state[13] = data[initial][\"cuevel\"][\"z\"].iloc[j+1]\n",
    "\n",
    "            #Action Velocity, Force?\n",
    "            action = compute_impulseForce(data[initial][\"cuevel\"][[\"x\", \"z\"]].iloc[j], data[initial][\"cuedirection\"][[\"x\", \"z\"]].iloc[j])\n",
    "            #action[0] = data[initial][\"cuevel\"][\"y\"].iloc[j]\n",
    "            \n",
    "\n",
    "            if j == data[initial][\"hit_ind\"][i]:\n",
    "                done_bool = True\n",
    "                reward = data[initial][\"rewards\"][i]\n",
    "            else:\n",
    "                done_bool = False\n",
    "                ## Discounted reward ##\n",
    "                gamma = 0.9\n",
    "                reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "                #reward = 0\n",
    "\n",
    "            final_timestep = (episode_step == data[initial][\"hit_ind\"][i]-1)\n",
    "            if (not terminate_on_end) and final_timestep:\n",
    "                # Skip this transition and don't apply terminals on the last step of an episode\n",
    "                episode_step = 0\n",
    "                continue\n",
    "            if done_bool or final_timestep:\n",
    "                episode_step = 0\n",
    "            print(\"appending episode: \", j)\n",
    "            state_.append(state)\n",
    "            new_state_.append(new_state)\n",
    "            action_.append(action)\n",
    "            reward_.append(reward)\n",
    "            done_.append(done_bool)\n",
    "            episode_step += 1\n",
    "        \n",
    "        print(\"trajectory: \", i)\n",
    "        trajectories.append({\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'new_state': np.array(new_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        })\n",
    "    \n",
    "    return trajectories\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'states': state_,\n",
    "        'actions': action_,\n",
    "        'next_states': next_state_,\n",
    "        'rewards': reward_,\n",
    "        'terminals': done_,\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'states': np.array(state_),\n",
    "        'actions': np.array(action_),\n",
    "        'next_states': np.array(next_state_),\n",
    "        'rewards': np.array(reward_),\n",
    "        'terminals': np.array(done_),\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef convert_D4RL(self, dataset):\n",
    "\t\tself.state = np.array(dataset['states'])\n",
    "\t\tself.action = np.array(dataset['actions'])\n",
    "\t\tself.next_state = np.array(dataset['next_states'])\n",
    "\t\tself.reward = np.array(dataset['rewards']).reshape(-1,1)\n",
    "\t\tself.not_done = 1. - np.array(dataset['terminals']).reshape(-1,1)\n",
    "\t\tself.size = self.state.shape[0]\n",
    "\n",
    "\n",
    "\tdef normalize_states(self, eps = 1e-3):\n",
    "\t\tmean = self.state.mean(0,keepdims=True)\n",
    "\t\tstd = self.state.std(0,keepdims=True) + eps\n",
    "\t\tself.state = (self.state - mean)/std\n",
    "\t\tself.next_state = (self.next_state - mean)/std\n",
    "\t\treturn mean, std\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\t\t\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n",
    "\n",
    "\n",
    "class TD3_BC(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tdiscount=0.99,\n",
    "\t\ttau=0.005,\n",
    "\t\tpolicy_noise=0.2,\n",
    "\t\tnoise_clip=0.5,\n",
    "\t\tpolicy_freq=2,\n",
    "\t\talpha=2.5,\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\t\tself.policy_noise = policy_noise\n",
    "\t\tself.noise_clip = noise_clip\n",
    "\t\tself.policy_freq = policy_freq\n",
    "\t\tself.alpha = alpha\n",
    "\n",
    "\t\tself.total_it = 0\n",
    "\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=256):\n",
    "\t\tself.total_it += 1\n",
    "\n",
    "\t\t# Sample replay buffer \n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Select action according to policy and add clipped noise\n",
    "\t\t\tnoise = (\n",
    "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
    "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
    "\t\t\t\n",
    "\t\t\tnext_action = (\n",
    "\t\t\t\tself.actor_target(next_state) + noise\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t\t# Compute the target Q value\n",
    "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t#if (self.total_it) % 5 == 0:\n",
    "\t\t\t#print(\"iteration \", self.total_it, \" critic_loss: \", critic_loss)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "\t\tif self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor loss\n",
    "\t\t\tpi = self.actor(state)\n",
    "\t\t\tQ = self.critic.Q1(state, pi)\n",
    "\t\t\tlmbda = self.alpha/Q.abs().mean().detach()\n",
    "\n",
    "\t\t\tactor_loss = -lmbda * Q.mean() + F.mse_loss(pi, action) \n",
    "\t\t\t\n",
    "\t\t\t#print(\"iteration \", self.total_it, \" actor_loss: \", actor_loss)\n",
    "\n",
    "\t\t\t# Optimize the actor \n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "\tdef save(self, filename):\n",
    "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\t\t\n",
    "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "\tdef load(self, filename):\n",
    "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        log_prob = (dist.log_prob(e) - torch.log(1 - action.pow(2) + epsilon)).sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
    "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
    "        \"\"\"\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        return action.detach().cpu()\n",
    "    \n",
    "    def get_det_action(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        return torch.tanh(mu).detach().cpu()\n",
    "\n",
    "class DeepActor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, device, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepActor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.device = device\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        in_dim = hidden_size+state_size\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(in_dim, hidden_size)\n",
    "        self.fc3 = nn.Linear(in_dim, hidden_size)\n",
    "        self.fc4 = nn.Linear(in_dim, hidden_size)\n",
    "\n",
    "\n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "        #self.reset_parameters() # check if this improves training\n",
    "\n",
    "    def reset_parameters(self, init_w=3e-3):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.mu.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state: torch.tensor):\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.cat([x, state], dim=1)\n",
    "        x = F.relu(self.fc4(x))  \n",
    "\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        log_prob = (dist.log_prob(e) - torch.log(1 - action.pow(2) + epsilon)).sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
    "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
    "        \"\"\"\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        e = dist.rsample().to(state.device)\n",
    "        action = torch.tanh(e)\n",
    "        return action.detach().cpu()\n",
    "    \n",
    "    def get_det_action(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        return torch.tanh(mu).detach().cpu()\n",
    "    \n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256, seed=1, N=32, device=\"cuda:0\"):\n",
    "        super(IQN, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = hidden_size\n",
    "        self.pis = torch.FloatTensor([np.pi * i for i in range(1, self.n_cos + 1)]).view(1, 1, self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "        self.head = nn.Linear(self.action_size + self.input_shape, hidden_size) \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, hidden_size)\n",
    "        self.ff_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ff_2 = nn.Linear(hidden_size, 1)    \n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device)\n",
    "        cos = torch.cos(taus * self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        x = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(x  ))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication > reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1) * cos_x).view(batch_size * num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        #action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        #x = torch.cat((x,action),dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n",
    "\n",
    "class DeepIQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, device=\"cuda:0\"):\n",
    "        super(DeepIQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.input_dim = action_size+state_size+layer_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.ff_1 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_2 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_3 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_4 = nn.Linear(self.layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])  \n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        xs = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(xs))\n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_1(x))   \n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_2(x))\n",
    "\n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication > reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        state = input.repeat(num_tau,1).reshape(num_tau,batch_size*self.input_shape).transpose(0,1).reshape(batch_size*num_tau,self.input_shape)\n",
    "        \n",
    "        x = torch.cat((x,action,state),dim=1)\n",
    "        x = torch.relu(self.ff_3(x))\n",
    "\n",
    "        out = self.ff_4(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n",
    "\n",
    "\n",
    "\n",
    "class CQLSAC(nn.Module):\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                        state_size,\n",
    "                        action_size,\n",
    "                        device\n",
    "                ):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        super(CQLSAC, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.tau = 1e-2\n",
    "        hidden_size = 256\n",
    "        learning_rate = 5e-4\n",
    "        self.clip_grad_param = 1\n",
    "\n",
    "        self.target_entropy = -action_size  # -dim(A)\n",
    "\n",
    "        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "        self.alpha_optimizer = optim.Adam(params=[self.log_alpha], lr=learning_rate) \n",
    "        \n",
    "        # CQL params\n",
    "        self.with_lagrange = False\n",
    "        self.temp = 1.0\n",
    "        self.cql_weight = 1.0\n",
    "        self.target_action_gap = 0.0\n",
    "        self.cql_log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.cql_alpha_optimizer = optim.Adam(params=[self.cql_log_alpha], lr=learning_rate) \n",
    "        \n",
    "        # Actor Network \n",
    "\n",
    "        self.actor_local = Actor(state_size, action_size, hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=learning_rate)     \n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "\n",
    "        self.critic1 = IQN(state_size, action_size, hidden_size, seed=1).to(device)\n",
    "        self.critic2 = IQN(state_size, action_size, hidden_size, seed=2).to(device)\n",
    "        \n",
    "        assert self.critic1.parameters() != self.critic2.parameters()\n",
    "        \n",
    "        self.critic1_target = IQN(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "\n",
    "        self.critic2_target = IQN(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=learning_rate) \n",
    "\n",
    "    def get_action(self, state, eval=False):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if eval:\n",
    "                action = self.actor_local.get_det_action(state)\n",
    "            else:\n",
    "                action = self.actor_local.get_action(state)\n",
    "        return action.numpy()\n",
    "\n",
    "    def calc_policy_loss(self, states, alpha):\n",
    "        actions_pred, log_pis = self.actor_local.evaluate(states)\n",
    "\n",
    "        q1 = self.critic1.get_qvalues(states, actions_pred.squeeze(0))   \n",
    "        q2 = self.critic2.get_qvalues(states, actions_pred.squeeze(0))\n",
    "        min_Q = torch.min(q1,q2).cpu()\n",
    "        actor_loss = ((alpha * log_pis.cpu() - min_Q )).mean()\n",
    "        return actor_loss, log_pis\n",
    "\n",
    "    def _compute_policy_values(self, state_pi, state_q):\n",
    "        with torch.no_grad():\n",
    "            actions_pred, log_pis = self.actor_local.evaluate(state_pi)\n",
    "        \n",
    "        qs1 = self.critic1.get_qvalues(state_q, actions_pred)\n",
    "        qs2 = self.critic2.get_qvalues(state_q, actions_pred)\n",
    "        \n",
    "        return qs1-log_pis, qs2-log_pis\n",
    "    \n",
    "    def _compute_random_values(self, state, actions, critic):\n",
    "        random_values = critic.get_qvalues(state, actions)\n",
    "        random_log_prstate = math.log(0.5 ** self.action_size)\n",
    "        return random_values - random_log_prstate\n",
    "    \n",
    "    def train(self, step, experiences, gamma, d=1):\n",
    "        \"\"\"Updates actor, critics and entropy_alpha parameters using given batch of experience tuples.\n",
    "        Q_targets = r +  * (min_critic_target(next_state, actor_target(next_state)) -  *log_pi(next_action|next_state))\n",
    "        Critic_loss = MSE(Q, Q_target)\n",
    "        Actor_loss =  * log_pi(a|s) - Q(s,a)\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        current_alpha = copy.deepcopy(self.alpha)\n",
    "        actor_loss, log_pis = self.calc_policy_loss(states, current_alpha)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Compute alpha loss\n",
    "        alpha_loss = - (self.log_alpha.exp() * (log_pis.cpu() + self.target_entropy).detach().cpu()).mean()\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        with torch.no_grad():\n",
    "            next_action, _ = self.actor_local.evaluate(next_states)\n",
    "            #next_action = next_action.unsqueeze(1).repeat(1, 10, 1).view(next_action.shape[0] * 10, next_action.shape[1])\n",
    "            #temp_next_states = next_states.unsqueeze(1).repeat(1, 10, 1).view(next_states.shape[0] * 10, next_states.shape[1])\n",
    "            \n",
    "            Q_target1_next, _ = self.critic1_target(next_states, next_action) #.view(states.shape[0], 10, 1).max(1)[0].view(-1, 1)\n",
    "            # batch_size, num_tau, 1    \n",
    "            Q_target2_next, _ = self.critic2_target(next_states, next_action) #.view(states.shape[0], 10, 1).max(1)[0].view(-1, 1)\n",
    "            Q_target_next = torch.min(Q_target1_next, Q_target2_next).transpose(1,2)\n",
    "\n",
    "            # Compute Q targets for current states (y_i)\n",
    "            Q_targets = rewards.cpu().unsqueeze(-1) + (gamma * (1 - dones.cpu().unsqueeze(-1)) * Q_target_next.cpu()) \n",
    "\n",
    "\n",
    "        # Compute critic loss\n",
    "        q1, taus1 = self.critic1(states, actions)\n",
    "        q2, taus2 = self.critic2(states, actions)\n",
    "        assert Q_targets.shape == (256, 1, 32), \"have shape: {}\".format(Q_targets.shape)\n",
    "        assert q1.shape == (256, 32, 1)\n",
    "        \n",
    "        # Quantile Huber loss\n",
    "        td_error1 = Q_targets - q1.cpu()\n",
    "        td_error2 = Q_targets - q2.cpu()\n",
    "        \n",
    "        assert td_error1.shape == (256, 32, 32), \"wrong td error shape\"\n",
    "        huber_l_1 = calculate_huber_loss(td_error1, 1.0)\n",
    "        huber_l_2 = calculate_huber_loss(td_error2, 1.0)\n",
    "        \n",
    "        quantil_l_1 = abs(taus1.cpu() - (td_error1.detach() < 0).float()) * huber_l_1 / 1.0\n",
    "        quantil_l_2 = abs(taus2.cpu() - (td_error2.detach() < 0).float()) * huber_l_2 / 1.0\n",
    "\n",
    "        critic1_loss = quantil_l_1.sum(dim=1).mean(dim=1).mean()\n",
    "        critic2_loss = quantil_l_2.sum(dim=1).mean(dim=1).mean()\n",
    "\n",
    "        \n",
    "        # CQL addon\n",
    "\n",
    "        random_actions = torch.FloatTensor(q1.shape[0] * 10, actions.shape[-1]).uniform_(-1, 1).to(self.device)\n",
    "        num_repeat = int (random_actions.shape[0] / states.shape[0])\n",
    "        temp_states = states.unsqueeze(1).repeat(1, num_repeat, 1).view(states.shape[0] * num_repeat, states.shape[1])\n",
    "        temp_next_states = next_states.unsqueeze(1).repeat(1, num_repeat, 1).view(next_states.shape[0] * num_repeat, next_states.shape[1])\n",
    "        \n",
    "        current_pi_values1, current_pi_values2  = self._compute_policy_values(temp_states, temp_states)\n",
    "        next_pi_values1, next_pi_values2 = self._compute_policy_values(temp_next_states, temp_states)\n",
    "        \n",
    "        random_values1 = self._compute_random_values(temp_states, random_actions, self.critic1).reshape(states.shape[0], num_repeat, 1)\n",
    "        random_values2 = self._compute_random_values(temp_states, random_actions, self.critic2).reshape(states.shape[0], num_repeat, 1)\n",
    "\n",
    "        current_pi_values1 = current_pi_values1.reshape(states.shape[0], num_repeat, 1)\n",
    "        current_pi_values2 = current_pi_values2.reshape(states.shape[0], num_repeat, 1)\n",
    "        next_pi_values1 = next_pi_values1.reshape(states.shape[0], num_repeat, 1)\n",
    "        next_pi_values2 = next_pi_values2.reshape(states.shape[0], num_repeat, 1)      \n",
    "        \n",
    "        cat_q1 = torch.cat([random_values1, current_pi_values1, next_pi_values1], 1)\n",
    "        cat_q2 = torch.cat([random_values2, current_pi_values2, next_pi_values2], 1)\n",
    "        \n",
    "        assert cat_q1.shape == (states.shape[0], 3 * num_repeat, 1), f\"cat_q1 instead has shape: {cat_q1.shape}\"\n",
    "        assert cat_q2.shape == (states.shape[0], 3 * num_repeat, 1), f\"cat_q2 instead has shape: {cat_q2.shape}\"\n",
    "        \n",
    "\n",
    "        cql1_scaled_loss = (torch.logsumexp(cat_q1 / self.temp, dim=1).mean() * self.cql_weight * self.temp - q1.mean()) * self.cql_weight\n",
    "        cql2_scaled_loss = (torch.logsumexp(cat_q2 / self.temp, dim=1).mean() * self.cql_weight * self.temp - q2.mean()) * self.cql_weight\n",
    "        \n",
    "        cql_alpha_loss = torch.FloatTensor([0.0])\n",
    "        cql_alpha = torch.FloatTensor([0.0])\n",
    "        if self.with_lagrange:\n",
    "            cql_alpha = torch.clamp(self.cql_log_alpha.exp(), min=0.0, max=1000000.0).to(self.device)\n",
    "            cql1_scaled_loss = cql_alpha * (cql1_scaled_loss - self.target_action_gap)\n",
    "            cql2_scaled_loss = cql_alpha * (cql2_scaled_loss - self.target_action_gap)\n",
    "\n",
    "            self.cql_alpha_optimizer.zero_grad()\n",
    "            cql_alpha_loss = (- cql1_scaled_loss - cql2_scaled_loss) * 0.5 \n",
    "            cql_alpha_loss.backward(retain_graph=True)\n",
    "            self.cql_alpha_optimizer.step()\n",
    "        \n",
    "        total_c1_loss = critic1_loss + cql1_scaled_loss\n",
    "        total_c2_loss = critic2_loss + cql2_scaled_loss\n",
    "        \n",
    "        \n",
    "        # Update critics\n",
    "        # critic 1\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        total_c1_loss.backward(retain_graph=True)\n",
    "        clip_grad_norm_(self.critic1.parameters(), self.clip_grad_param)\n",
    "        self.critic1_optimizer.step()\n",
    "        # critic 2\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        total_c2_loss.backward()\n",
    "        clip_grad_norm_(self.critic2.parameters(), self.clip_grad_param)\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic1, self.critic1_target)\n",
    "        self.soft_update(self.critic2, self.critic2_target)\n",
    "        \n",
    "        return actor_loss.item(), alpha_loss.item(), critic1_loss.item(), critic2_loss.item(), cql1_scaled_loss.item(), cql2_scaled_loss.item(), current_alpha, cql_alpha_loss.item(), cql_alpha.item()\n",
    "\n",
    "    def soft_update(self, local_model , target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        _target = *_local + (1 - )*_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    assert loss.shape == (td_errors.shape[0], 32, 32), \"huber loss has wrong shape\"\n",
    "    return loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "        # Experiment\n",
    "        \"policy\": \"TD3_BC\",\n",
    "        \"seed\": 0, \n",
    "        \"eval_freq\": 5e3,\n",
    "        \"max_timesteps\": 250,   #1e6,\n",
    "        \"save_model\": \"store_true\",\n",
    "        \"load_model\": \"\",                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "\t    # TD3\n",
    "\t    \"expl_noise\": 0.1,\n",
    "        \"batch_size\": 256,\n",
    "        \"discount\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"policy_noise\": 0.2,\n",
    "        \"noise_clip\": 0.5,\n",
    "        \"policy_freq\": 2,\n",
    "        # TD3 + BC\n",
    "\t    \"alpha\": 2.5,\n",
    "        \"normalize\": True,\n",
    "        \"state_dim\": 18,\n",
    "\t\t\"action_dim\": 3,\n",
    "\t\t\"max_action\": 1,\n",
    "\t\t\"discount\": 0.99,\n",
    "\t\t\"tau\": 0.005,\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "        \"state_dim\": 18,\n",
    "\t\t\"action_dim\": 3,\n",
    "\t\t\"max_action\": 1,\n",
    "\t\t\"discount\": 0.99,\n",
    "\t\t\"tau\": 0.005,\n",
    "\t\t# TD3\n",
    "\t\t\"policy_noise\": 0.2,    #args.policy_noise * max_action,\n",
    "\t\t\"noise_clip\": 0.5,  #args.noise_clip * max_action,\n",
    "\t\t\"policy_freq\": 2,\n",
    "\t\t# TD3 + BC\n",
    "\t\t\"alpha\": 2.5\n",
    "\t}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, **kwargs):   #config\n",
    "    #np.random.seed(config.seed)\n",
    "    #random.seed(config.seed)\n",
    "    #torch.manual_seed(config.seed)      \n",
    "\n",
    "    ############################# Define Path and Load Dataset ######################################\n",
    "\n",
    "\tpath = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\"\n",
    "\tpath2 = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 2/\"\n",
    "    ## WARNING check which type of feedback in which round, and which corner for each subject\n",
    "\t#corner =  \"left\"\n",
    "\t#data = resultsSingleSubject(initial, \"reward\", path, corner)\n",
    "\t'''dic for one subject composed of ~1000 timepoints for one shot, 25 shots in one block, and 10 blocks\n",
    "    First 3 blocks are baseline learning, then 6 blocks of adaptation to perturbation, and one final washout block\n",
    "    That is 250 shots per subjects, 300'564 points in the dictionnary'''\n",
    "\n",
    "\t# Environment State Properties\n",
    "\tcorner = \"all\"\n",
    "\tstate_dim=14\n",
    "\taction_dim=2\n",
    "\tmax_action = 1\n",
    "\tnormalize = True\n",
    "    # Agent parameters\n",
    "\tkwargs = {\n",
    "\t\t\"state_dim\": state_dim,\n",
    "\t\t\"action_dim\": action_dim,\n",
    "\t\t\"max_action\": max_action,\n",
    "\t\t\"discount\": args['discount'],\n",
    "\t\t\"tau\": args['tau'],\n",
    "\t\t# TD3\n",
    "\t\t\"policy_noise\": args['policy_noise'] * max_action,\n",
    "\t\t\"noise_clip\": args['noise_clip'] * max_action,\n",
    "\t\t\"policy_freq\": args['policy_freq'],\n",
    "\t\t# TD3 + BC\n",
    "\t\t\"alpha\": args['alpha']\n",
    "\t    }\n",
    "    \n",
    "\t# Initialize Agent\n",
    "\tif args['policy'] == \"TD3_BC\":\n",
    "\t\tpolicy = TD3_BC(**kwargs) \n",
    "\telif args['policy'] == \"CQL_SAC\":\n",
    "\t\tpolicy = CQLSAC(state_size=state_dim, action_size=action_dim, device=device)\n",
    "\telse:\n",
    "\t\traise ValueError(\"Chose Agent between [TD3_BC, CQL_SAC]\")\n",
    "\n",
    "\tif args['load_model'] != \"\":\n",
    "\t\tpolicy_file = file_name if args['load_model'] == \"default\" else args['load_model']\n",
    "\t\tpolicy.load(f\"./models/{policy_file}\")\n",
    "    \n",
    "\t'''\n",
    "    # Dataframe for all subjects\n",
    "\tinitial = \"AAB\"\n",
    "\tdata = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "\t#data = resultsMultipleSubjects([path, path2], 'reward', 'all')\n",
    "\n",
    "\t#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "\t#pd_dataset = pd.DataFrame.from_dict(dataset)\n",
    "\t#pd_dataset.to_csv(\"RL_dataset/AAB.csv\")\n",
    "\t#print(\"AAB saved\")\n",
    "\treturn data\n",
    "\t'''\n",
    "\t#dataset = pd.read_csv(\"RL_dataset/AAB.csv\")\n",
    "\tinitial = \"AAB\"\n",
    "\tdata = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "\t#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "\t#replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "\t#replay_buffer.convert_D4RL(dataset)\n",
    "\treturn data\n",
    "\t\"\"\"\n",
    "\tif normalize:\n",
    "\t\tmean,std = replay_buffer.normalize_states() \n",
    "\telse:\n",
    "\t\tmean,std = 0,1\n",
    "\t\n",
    "\t################## Training #######################\n",
    "\t#for t in range(int(args.max_timesteps)):\n",
    "\t\t#policy.train(replay_buffer, args.batch_size)\n",
    "\n",
    "\n",
    "\tsteps = 0\n",
    "\taverage10 = deque(maxlen=10)\n",
    "\ttotal_steps = 0\n",
    "\tbatch_size = 64 #256\n",
    "\n",
    "\tfor i in range(1, 100):\n",
    "\t\tepisode_steps = 0\n",
    "\t\trewards = 0\n",
    "\t\twhile True:\n",
    "\t\t\tsteps += 1\n",
    "\t\t\tprint(\"step: \", steps)\n",
    "\t\t\tpolicy.train(replay_buffer)\n",
    "\n",
    "\t\taverage10.append(rewards)\n",
    "\t\ttotal_steps += episode_steps\n",
    "\t\tprint(\"Episode: {} | Reward: {} | Polciy Loss: {} | Steps: {}\".format(i, reward, policy_loss, steps))\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD3_BC\n",
      "---------------------------------------\n",
      "Policy: TD3_BC, Seed: 0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize policy\n",
    "policy = TD3_BC(**kwargs)\n",
    "\n",
    "print(args[\"policy\"])\n",
    "file_name = f\"{args['policy']}_{args['seed']}\"\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Policy: {args['policy']}, Seed: {args['seed']}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "\n",
    "if args['save_model'] and not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "    \n",
    "#dataset = train(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_405/3957423419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result Multiple Subject function\n",
      "/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/ AS\n",
      "Result Single Subject function\n",
      "Success Derivation function\n",
      "2 fake successes removed\n",
      "Reward function\n",
      "start hit timesteps function\n",
      "Imported AS as reward subject for Right pocket in round: /mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\n"
     ]
    }
   ],
   "source": [
    "############################# Define Path and Load Dataset ######################################\n",
    "\n",
    "path = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 1/\"\n",
    "path2 = \"/mnt/c/Users/dario/Documents/DARIO/ETUDES/ICL/code/data/Round 2/\"\n",
    "## WARNING check which type of feedback in which round, and which corner for each subject\n",
    "#corner =  \"left\"\n",
    "#data = resultsSingleSubject(initial, \"reward\", path, corner)\n",
    "'''dic for one subject composed of ~1000 timepoints for one shot, 25 shots in one block, and 10 blocks\n",
    "First 3 blocks are baseline learning, then 6 blocks of adaptation to perturbation, and one final washout block\n",
    "That is 250 shots per subjects, 300'564 points in the dictionnary'''\n",
    "\n",
    "# Environment State Properties\n",
    "'''\n",
    "corner = \"all\"\n",
    "state_dim=14\n",
    "action_dim=2\n",
    "max_action = 1\n",
    "normalize = True\n",
    "# Agent parameters\n",
    "kwargs = {\n",
    "\t\"state_dim\": state_dim,\n",
    "\t\"action_dim\": action_dim,\n",
    "\t\"max_action\": max_action,\n",
    "\t\"discount\": args['discount'],\n",
    "\t\"tau\": args['tau'],\n",
    "\t# TD3\n",
    "\t\"policy_noise\": args['policy_noise'] * max_action,\n",
    "\t\"noise_clip\": args['noise_clip'] * max_action,\n",
    "\t\"policy_freq\": args['policy_freq'],\n",
    "\t# TD3 + BC\n",
    "\t\"alpha\": args['alpha']\n",
    "\t}\n",
    "'''\n",
    "#dataset = pd.read_csv(\"RL_dataset/AAB.csv\")\n",
    "initial = \"BL\"\n",
    "data = resultsMultipleSubjects(path, initial, 'reward', 'all')\n",
    "#data = resultsMultipleSubjects([path, path2], 'reward', 'all')\n",
    "\n",
    "#dataset = Offline_RL_dataset(data, initial, terminate_on_end=True)\n",
    "#replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "#replay_buffer.convert_D4RL(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_ = []\n",
    "cueballpos = []\n",
    "cueballvel = []\n",
    "redballpos = []\n",
    "targetcornerpos = []\n",
    "cueposfront = []\n",
    "cueposback = []\n",
    "cuedirection = []\n",
    "cuevel = []\n",
    "\n",
    "total_len_trajectories = 0\n",
    "\n",
    "for i in range(data[initial][\"start_ind\"].shape[0]):\n",
    "    for j in range(data[initial][\"start_ind\"][i], data[initial][\"hit_ind\"][i]+1, 1):\n",
    "        total_len_trajectories += 1\n",
    "        if j == data[initial][\"hit_ind\"][i]:\n",
    "            done_bool = True\n",
    "            reward = data[initial][\"rewards\"][i]\n",
    "        else:\n",
    "            done_bool = False\n",
    "            ## Discounted reward ##\n",
    "            #gamma = 0.9\n",
    "            #reward = gamma**(j - data[initial][\"hit_ind\"][i]) * data[initial][\"rewards\"][i]\n",
    "            reward = 0\n",
    "        reward_.append(reward)\n",
    "    cueballpos.append(np.array((data[initial][\"cueballpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueballpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueballpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueballpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1])))\n",
    "    cueballvel.append(np.array((data[initial][\"cueballvel\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueballvel\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueballvel\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueballvel\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1])))\n",
    "    redballpos.append(np.array((data[initial][\"redballpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"redballpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"redballpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"redballpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1])))\n",
    "    targetcornerpos.append(np.array((data[initial][\"targetcornerpos\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"targetcornerpos\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"targetcornerpos\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"targetcornerpos\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1])))\n",
    "    cueposfront.append(np.array((data[initial][\"cueposfront\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueposfront\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueposfront\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueposfront\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1])))\n",
    "    cueposback.append(np.array((data[initial][\"cueposback\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueposback\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueposback\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cueposback\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1])))\n",
    "    cuedirection.append(np.array((data[initial][\"cuedirection\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cuedirection\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cuedirection\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cuedirection\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1])))\n",
    "    cuevel.append(np.array((data[initial][\"cuevel\"][\"trial\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cuevel\"][\"x\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cuevel\"][\"y\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1], data[initial][\"cuevel\"][\"z\"][data[initial][\"start_ind\"][i]:data[initial][\"hit_ind\"][i]+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "data  AS  saved\n"
     ]
    }
   ],
   "source": [
    "dic = {'rewards': np.array(reward_), \n",
    "        'cueballpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "        'cueballvel': np.zeros(total_len_trajectories, dtype=object),\n",
    "        'redballpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "        'targetcornerpos': np.zeros(total_len_trajectories, dtype=object),\n",
    "        'cueposfront': np.zeros(total_len_trajectories, dtype=object),\n",
    "        'cueposback': np.zeros(total_len_trajectories, dtype=object),\n",
    "        'cuedirection': np.zeros(total_len_trajectories, dtype=object),\n",
    "        'cuevel': np.zeros(total_len_trajectories, dtype=object)}\n",
    "\n",
    "transition_num = 0\n",
    "for i in range(data[initial][\"start_ind\"].shape[0]):\n",
    "        for j in range(cueballpos[i].shape[1]):\n",
    "                dic['cueballpos'][transition_num] = [cueballpos[i][0][j],cueballpos[i][1][j], cueballpos[i][2][j], cueballpos[i][3][j]]\n",
    "                dic['cueballvel'][transition_num] = [cueballvel[i][0][j],cueballvel[i][1][j], cueballvel[i][2][j], cueballvel[i][3][j]]\n",
    "                dic['redballpos'][transition_num] = [redballpos[i][0][j], redballpos[i][1][j], redballpos[i][2][j], redballpos[i][3][j]]\n",
    "                dic['targetcornerpos'][transition_num] = [targetcornerpos[i][0][j], targetcornerpos[i][1][j], targetcornerpos[i][2][j], targetcornerpos[i][3][j]]\n",
    "                dic['cueposfront'][transition_num] = [cueposfront[i][0][j], cueposfront[i][1][j], cueposfront[i][2][j], cueposfront[i][3][j]]\n",
    "                dic['cueposback'][transition_num] = [cueposback[i][0][j], cueposback[i][1][j], cueposback[i][2][j], cueposback[i][3][j]]\n",
    "                dic['cuedirection'][transition_num] = [cuedirection[i][0][j], cuedirection[i][1][j], cuedirection[i][2][j], cuedirection[i][3][j]]\n",
    "                dic['cuevel'][transition_num] = [cuevel[i][0][j], cuevel[i][1][j], cuevel[i][2][j], cuevel[i][3][j]]\n",
    "                transition_num += 1\n",
    "\n",
    "        if i%50 == 0:\n",
    "                print(i)\n",
    "pd_dataset = pd.DataFrame.from_dict(dic)\n",
    "pd_dataset.to_csv(\"RL_dataset/reduced_data/\"+initial+\"_reduced_data.csv\")\n",
    "print(\"data \", initial, \" saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save RL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"RL_dataset/reduced_data/\"+initial+\"_reduced_data.csv\", header = 0, \\\n",
    "        names = ['rewards','cueballpos', 'cueballvel','redballpos', 'targetcornerpos', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel'], usecols = [1,2,3,4,5,6,7,8,9], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "rewards= pd.DataFrame.from_records(np.array(df['rewards'].astype(str).str.split(','))).astype(float)\n",
    "cueballpos = pd.DataFrame.from_records(np.array(df['cueballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueballvel = pd.DataFrame.from_records(np.array(df['cueballvel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "redballpos = pd.DataFrame.from_records(np.array(df['redballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "targetcornerpos = pd.DataFrame.from_records(np.array(df['targetcornerpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposfront = pd.DataFrame.from_records(np.array(df['cueposfront'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposback = pd.DataFrame.from_records(np.array(df['cueposback'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuedirection = pd.DataFrame.from_records(np.array(df['cuedirection'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuevel = pd.DataFrame.from_records(np.array(df['cuevel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_data = {'cueballpos': cueballpos,\n",
    "             'cueballvel': cueballvel,\n",
    "            'redballpos': redballpos,\n",
    "            'targetcornerpos': targetcornerpos\n",
    "            }\n",
    "cue_data = {'cueposfront': cueposfront,\n",
    "        'cueposback': cueposback,\n",
    "        'cuedirection': cuedirection\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline RL Dataset function\n"
     ]
    }
   ],
   "source": [
    "dataset = Offline_Reduced(ball_data, cue_data, cuevel, rewards[0])\n",
    "#dataset = Offline_One(ball_data, cue_data, cuevel, rewards[0])  #rewars[0] to get float value from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS saved\n"
     ]
    }
   ],
   "source": [
    "# Dataframe does not accept 2-d arrays\n",
    "# transform 2-d array (n states times 14 dimensions (state)) to 1-d array of list (of length 14)\n",
    "#dataset_RL = Offline_RL_dataset(data,rewards,cuedirection, cuevel,cueballpos, terminate_on_end=True)\n",
    "#Offline_RL_load(rewards,cueballpos,redballpos, targetcornerpos, cueposfront, cueposback, cuedirection, cuevel, terminate_on_end=True)\n",
    "#dataset = Offline_One_big_dict(data, initial)\n",
    "\n",
    "new_d = {'trial': dataset[\"trial\"],\n",
    "        'states': np.zeros(dataset[\"states\"].shape[0], dtype=object),\n",
    "        'actions': np.zeros(dataset[\"actions\"].shape[0], dtype=object),\n",
    "        'new_states': np.zeros(dataset[\"new_states\"].shape[0], dtype=object),\n",
    "        'rewards': dataset[\"rewards\"],\n",
    "        'terminals': dataset[\"terminals\"]}\n",
    "for i in range(dataset[\"states\"].shape[0]):\n",
    "    new_d['states'][i] = dataset[\"states\"][i][:].tolist()\n",
    "    new_d['actions'][i] = dataset[\"actions\"][i][:].tolist()\n",
    "    new_d['new_states'][i] = dataset[\"new_states\"][i][:].tolist()\n",
    "\n",
    "pd_dataset = pd.DataFrame.from_dict(new_d)\n",
    "pd_dataset.to_csv(\"RL_dataset/Offline_reduced/\"+initial+\"_Offline_reduced.csv\")\n",
    "print(initial, \"saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "initial=\"AAB\"\n",
    "df = pd.read_csv(\"RL_dataset/raw_data/\"+initial+\"_raw_data.csv\", header = 0, \\\n",
    "        names = ['rewards','cueballpos','redballpos', 'targetcornerpos', 'cueposfront', 'cueposback', 'cuedirection', 'cuevel'], usecols = [1,2,3,4,5,6,7,8], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "rewards= pd.DataFrame.from_records(np.array(df['rewards'].astype(str).str.split(','))).astype(float)\n",
    "cueballpos = pd.DataFrame.from_records(np.array(df['cueballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "redballpos = pd.DataFrame.from_records(np.array(df['redballpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "targetcornerpos = pd.DataFrame.from_records(np.array(df['targetcornerpos'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposfront = pd.DataFrame.from_records(np.array(df['cueposfront'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cueposback = pd.DataFrame.from_records(np.array(df['cueposback'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuedirection = pd.DataFrame.from_records(np.array(df['cuedirection'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)\n",
    "cuevel = pd.DataFrame.from_records(np.array(df['cuevel'].str.split(',')), columns=[\"trial\",\"x\",\"y\",\"z\"]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = {'cueballpos': cueballpos,\n",
    "             'redballpos': redballpos, \n",
    "             'targetcornerpos': targetcornerpos,\n",
    "             'cueposfront': cueposfront, \n",
    "             'cueposback': cueposback,\n",
    "             'cuedirection': cuedirection,\n",
    "              'cuevel': cuevel}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(10).reshape(-1, 2))\n",
    "print(df)\n",
    "df_subset = df.sample(2)\n",
    "print(df_subset)\n",
    "df = df.drop(df_subset.index)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df.sample(frac=0.8,random_state=200)\n",
    "test=df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saved dataset\n",
    "df = pd.read_csv(\"RL_dataset/Offline_24_states/AAB_new_states.csv\", header = 0, \\\n",
    "        names = ['trial','states','actions','new_states','rewards','terminals'], usecols = [1,2,3,4,5,6], lineterminator = \"\\n\")\n",
    "df = df.replace([r'\\n', r'\\[', r'\\]'], '', regex=True) \n",
    "states = pd.DataFrame.from_records(np.array(df['states'].str.split(','))).astype(float)\n",
    "actions= pd.DataFrame.from_records(np.array(df['actions'].str.split(','))).astype(float)\n",
    "new_states = pd.DataFrame.from_records(np.array(df['new_states'].str.split(','))).astype(float)\n",
    "trial = df['trial'].astype(int)\n",
    "#Train/Test split\n",
    "trial_ind = np.arange(1,trial.iloc[-1]+1)\n",
    "train_trial = np.random.choice(trial_ind, size=200, replace=False)  #distrib proba for each value, could be useful to weight more \"important\" trajectories\n",
    "test_trial = np.delete(trial_ind, train_trial-1)\n",
    "\n",
    "train_ind = trial.isin(train_trial)\n",
    "test_ind = trial.isin(test_trial)\n",
    "\n",
    "train_set = {'trial': trial[train_ind],\n",
    "                'states': states[train_ind],\n",
    "                'actions': actions[train_ind],\n",
    "                'new_states': new_states[train_ind],\n",
    "                'rewards': df['rewards'][train_ind],\n",
    "                'terminals': df['terminals'][train_ind]}\n",
    "\n",
    "test_set = {'trial': trial[test_ind],\n",
    "                'states': states[test_ind],\n",
    "                'actions': actions[test_ind],\n",
    "                'new_states': new_states[test_ind],\n",
    "                'rewards': df['rewards'][test_ind],\n",
    "                'terminals': df['terminals'][test_ind]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dic):\n",
    "    ind = np.random.randint(1, 250) \n",
    "    print(ind)\n",
    "    return (torch.FloatTensor(dic['states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['actions'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.FloatTensor(dic['new_states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['rewards'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.Tensor(dic['terminals'][dic['trial'] == ind].to_numpy()).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(dic, ind):\n",
    "    return (torch.FloatTensor(dic['states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['actions'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.FloatTensor(dic['new_states'][dic['trial'] == ind].to_numpy()).to(device), torch.FloatTensor(dic['rewards'][dic['trial'] == ind].to_numpy()).to(device), \n",
    "            torch.Tensor(dic['terminals'][dic['trial'] == ind].to_numpy()).to(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Raw dataset and RL dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Offline_RL_dataset(nb_trials=list_data['cueballpos'][\"trial\"].iloc[-1])\n",
    "train_set.get_trajectories(list_data, rewards)\n",
    "#train_set.compute_mean_std(list_data)\n",
    "#train_set.normalize_states()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, new_states, rewards, terminals = sample(dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "for t in range(int(args[\"max_timesteps\"])):\n",
    "\tpolicy.train(replay_buffer, args[\"batch_size\"])\n",
    "\t'''\n",
    "\t# Evaluate episode\n",
    "\tif (t + 1) % args[\"eval_freq\"] == 0:\n",
    "\t\tprint(f\"Time steps: {t+1}\")\n",
    "\t\tevaluations.append(eval_policy(policy, args.env, args.seed, mean, std))\n",
    "\t\t#np.save(f\"./results/{file_name}\", evaluations)\n",
    "\t\t#if args.save_model: policy.save(f\"./models/{file_name}\")\n",
    "\t'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns score\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, eval_dataset, mean, std, eval_episodes=2):\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, done = eval_env.reset(), False\n",
    "\t\twhile not done:\n",
    "\t\t\tstate = (np.array(state).reshape(1,-1) - mean)/std\n",
    "\t\t\taction = policy.select_action(state)\n",
    "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\td4rl_score = eval_env.get_normalized_score(avg_reward) * 100\n",
    "\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}, D4RL score: {d4rl_score:.3f}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\treturn d4rl_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_j_steps = 25\n",
    "gamma = 0.98\n",
    "mag = MAGIC(gamma)\n",
    "info = [data.actions(),\n",
    "                data.rewards(),\n",
    "                data.base_propensity(),\n",
    "                data.target_propensity(),\n",
    "                Qs\n",
    "                ]\n",
    "magic_evaluation = mag.evaluate(info, num_j_steps, True)\n",
    "print(magic_evaluation[0], (magic_evaluation[0] - true )**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import _collections_abc\n",
    "\n",
    "def _count_elements(mapping, iterable):\n",
    "    'Tally elements from the iterable.'\n",
    "    mapping_get = mapping.get\n",
    "    for elem in iterable:\n",
    "        mapping[elem] = mapping_get(elem, 0) + 1\n",
    "'''\n",
    "try:                                    # Load C helper function if available\n",
    "    from _collections import _count_elements\n",
    "except ImportError:\n",
    "    pass\n",
    "'''\n",
    "\n",
    "class itemgetter:\n",
    "    \"\"\"\n",
    "    Return a callable object that fetches the given item(s) from its operand.\n",
    "    After f = itemgetter(2), the call f(r) returns r[2].\n",
    "    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\n",
    "    \"\"\"\n",
    "    __slots__ = ('_items', '_call')\n",
    "\n",
    "    def __init__(self, item, *items):\n",
    "        if not items:\n",
    "            self._items = (item,)\n",
    "            def func(obj):\n",
    "                return obj[item]\n",
    "            self._call = func\n",
    "        else:\n",
    "            self._items = items = (item,) + items\n",
    "            def func(obj):\n",
    "                return tuple(obj[i] for i in items)\n",
    "            self._call = func\n",
    "\n",
    "    def __call__(self, obj):\n",
    "        return self._call(obj)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s.%s(%s)' % (self.__class__.__module__,\n",
    "                              self.__class__.__name__,\n",
    "                              ', '.join(map(repr, self._items)))\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, self._items\n",
    "\n",
    "\n",
    "class Counter(dict):\n",
    "    '''Dict subclass for counting hashable items.  Sometimes called a bag\n",
    "    or multiset.  Elements are stored as dictionary keys and their counts\n",
    "    are stored as dictionary values.\n",
    "    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n",
    "    >>> c.most_common(3)                # three most common elements\n",
    "    [('a', 5), ('b', 4), ('c', 3)]\n",
    "    >>> sorted(c)                       # list all unique elements\n",
    "    ['a', 'b', 'c', 'd', 'e']\n",
    "    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n",
    "    'aaaaabbbbcccdde'\n",
    "    >>> sum(c.values())                 # total of all counts\n",
    "    15\n",
    "    >>> c['a']                          # count of letter 'a'\n",
    "    5\n",
    "    >>> for elem in 'shazam':           # update counts from an iterable\n",
    "    ...     c[elem] += 1                # by adding 1 to each element's count\n",
    "    >>> c['a']                          # now there are seven 'a'\n",
    "    7\n",
    "    >>> del c['b']                      # remove all 'b'\n",
    "    >>> c['b']                          # now there are zero 'b'\n",
    "    0\n",
    "    >>> d = Counter('simsalabim')       # make another counter\n",
    "    >>> c.update(d)                     # add in the second counter\n",
    "    >>> c['a']                          # now there are nine 'a'\n",
    "    9\n",
    "    >>> c.clear()                       # empty the counter\n",
    "    >>> c\n",
    "    Counter()\n",
    "    Note:  If a count is set to zero or reduced to zero, it will remain\n",
    "    in the counter until the entry is deleted or the counter is cleared:\n",
    "    >>> c = Counter('aaabbc')\n",
    "    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n",
    "    >>> c.most_common()                 # 'b' is still in, but its count is zero\n",
    "    [('a', 3), ('c', 1), ('b', 0)]\n",
    "    '''\n",
    "    # References:\n",
    "    #   http://en.wikipedia.org/wiki/Multiset\n",
    "    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n",
    "    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n",
    "    #   http://code.activestate.com/recipes/259174/\n",
    "    #   Knuth, TAOCP Vol. II section 4.6.3\n",
    "\n",
    "    def __init__(self, iterable=None, /, **kwds):\n",
    "        '''Create a new, empty Counter object.  And if given, count elements\n",
    "        from an input iterable.  Or, initialize the count from another mapping\n",
    "        of elements to their counts.\n",
    "        >>> c = Counter()                           # a new, empty counter\n",
    "        >>> c = Counter('gallahad')                 # a new counter from an iterable\n",
    "        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n",
    "        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.update(iterable, **kwds)\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        'The count of elements not in the Counter is zero.'\n",
    "        # Needed so that self[missing_item] does not raise KeyError\n",
    "        return 0\n",
    "\n",
    "    def total(self):\n",
    "        'Sum of the counts'\n",
    "        return sum(self.values())\n",
    "\n",
    "    def most_common(self, n=None):\n",
    "        '''List the n most common elements and their counts from the most\n",
    "        common to the least.  If n is None, then list all element counts.\n",
    "        >>> Counter('abracadabra').most_common(3)\n",
    "        [('a', 5), ('b', 2), ('r', 2)]\n",
    "        '''\n",
    "        # Emulate Bag.sortedByCount from Smalltalk\n",
    "        if n is None:\n",
    "            return sorted(self.items(), key=_itemgetter(1), reverse=True)\n",
    "\n",
    "        # Lazy import to speedup Python startup time\n",
    "        import heapq\n",
    "        return heapq.nlargest(n, self.items(), key=_itemgetter(1))\n",
    "\n",
    "    def elements(self):\n",
    "        '''Iterator over elements repeating each as many times as its count.\n",
    "        >>> c = Counter('ABCABC')\n",
    "        >>> sorted(c.elements())\n",
    "        ['A', 'A', 'B', 'B', 'C', 'C']\n",
    "        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n",
    "        >>> import math\n",
    "        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n",
    "        >>> math.prod(prime_factors.elements())\n",
    "        1836\n",
    "        Note, if an element's count has been set to zero or is a negative\n",
    "        number, elements() will ignore it.\n",
    "        '''\n",
    "        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n",
    "        return _chain.from_iterable(_starmap(_repeat, self.items()))\n",
    "\n",
    "    # Override dict methods where necessary\n",
    "\n",
    "    @classmethod\n",
    "    def fromkeys(cls, iterable, v=None):\n",
    "        # There is no equivalent method for counters because the semantics\n",
    "        # would be ambiguous in cases such as Counter.fromkeys('aaabbc', v=2).\n",
    "        # Initializing counters to zero values isn't necessary because zero\n",
    "        # is already the default value for counter lookups.  Initializing\n",
    "        # to one is easily accomplished with Counter(set(iterable)).  For\n",
    "        # more exotic cases, create a dictionary first using a dictionary\n",
    "        # comprehension or dict.fromkeys().\n",
    "        raise NotImplementedError(\n",
    "            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')\n",
    "\n",
    "    def update(self, iterable=None, /, **kwds):\n",
    "        '''Like dict.update() but add counts instead of replacing them.\n",
    "        Source can be an iterable, a dictionary, or another Counter instance.\n",
    "        >>> c = Counter('which')\n",
    "        >>> c.update('witch')           # add elements from another iterable\n",
    "        >>> d = Counter('watch')\n",
    "        >>> c.update(d)                 # add elements from another counter\n",
    "        >>> c['h']                      # four 'h' in which, witch, and watch\n",
    "        4\n",
    "        '''\n",
    "        # The regular dict.update() operation makes no sense here because the\n",
    "        # replace behavior results in the some of original untouched counts\n",
    "        # being mixed-in with all of the other counts for a mismash that\n",
    "        # doesn't have a straight-forward interpretation in most counting\n",
    "        # contexts.  Instead, we implement straight-addition.  Both the inputs\n",
    "        # and outputs are allowed to contain zero and negative counts.\n",
    "\n",
    "        if iterable is not None:\n",
    "            if isinstance(iterable, _collections_abc.Mapping):\n",
    "                if self:\n",
    "                    self_get = self.get\n",
    "                    for elem, count in iterable.items():\n",
    "                        self[elem] = count + self_get(elem, 0)\n",
    "                else:\n",
    "                    # fast path when counter is empty\n",
    "                    super().update(iterable)\n",
    "            else:\n",
    "                _count_elements(self, iterable)\n",
    "        if kwds:\n",
    "            self.update(kwds)\n",
    "\n",
    "    def subtract(self, iterable=None, /, **kwds):\n",
    "        '''Like dict.update() but subtracts counts instead of replacing them.\n",
    "        Counts can be reduced below zero.  Both the inputs and outputs are\n",
    "        allowed to contain zero and negative counts.\n",
    "        Source can be an iterable, a dictionary, or another Counter instance.\n",
    "        >>> c = Counter('which')\n",
    "        >>> c.subtract('witch')             # subtract elements from another iterable\n",
    "        >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n",
    "        >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n",
    "        0\n",
    "        >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n",
    "        -1\n",
    "        '''\n",
    "        if iterable is not None:\n",
    "            self_get = self.get\n",
    "            if isinstance(iterable, _collections_abc.Mapping):\n",
    "                for elem, count in iterable.items():\n",
    "                    self[elem] = self_get(elem, 0) - count\n",
    "            else:\n",
    "                for elem in iterable:\n",
    "                    self[elem] = self_get(elem, 0) - 1\n",
    "        if kwds:\n",
    "            self.subtract(kwds)\n",
    "\n",
    "    def copy(self):\n",
    "        'Return a shallow copy.'\n",
    "        return self.__class__(self)\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (dict(self),)\n",
    "\n",
    "    def __delitem__(self, elem):\n",
    "        'Like dict.__delitem__() but does not raise KeyError for missing values.'\n",
    "        if elem in self:\n",
    "            super().__delitem__(elem)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self:\n",
    "            return f'{self.__class__.__name__}()'\n",
    "        try:\n",
    "            # dict() preserves the ordering returned by most_common()\n",
    "            d = dict(self.most_common())\n",
    "        except TypeError:\n",
    "            # handle case where values are not orderable\n",
    "            d = dict(self)\n",
    "        return f'{self.__class__.__name__}({d!r})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        'True if all counts agree. Missing counts are treated as zero.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] == other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        'True if any counts disagree. Missing counts are treated as zero.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return not self == other\n",
    "\n",
    "    def __le__(self, other):\n",
    "        'True if all counts in self are a subset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] <= other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        'True if all counts in self are a proper subset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return self <= other and self != other\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        'True if all counts in self are a superset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return all(self[e] >= other[e] for c in (self, other) for e in c)\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        'True if all counts in self are a proper superset of those in other.'\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        return self >= other and self != other\n",
    "\n",
    "    def __add__(self, other):\n",
    "        '''Add counts from two counters.\n",
    "        >>> Counter('abbb') + Counter('bcc')\n",
    "        Counter({'b': 4, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            newcount = count + other[elem]\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        ''' Subtract count, but keep only results with positive counts.\n",
    "        >>> Counter('abbbc') - Counter('bccd')\n",
    "        Counter({'b': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            newcount = count - other[elem]\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count < 0:\n",
    "                result[elem] = 0 - count\n",
    "        return result\n",
    "\n",
    "    def __or__(self, other):\n",
    "        '''Union is the maximum of value in either of the input counters.\n",
    "        >>> Counter('abbb') | Counter('bcc')\n",
    "        Counter({'b': 3, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            newcount = other_count if count < other_count else count\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        for elem, count in other.items():\n",
    "            if elem not in self and count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __and__(self, other):\n",
    "        ''' Intersection is the minimum of corresponding counts.\n",
    "        >>> Counter('abbb') & Counter('bcc')\n",
    "        Counter({'b': 1})\n",
    "        '''\n",
    "        if not isinstance(other, Counter):\n",
    "            return NotImplemented\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            newcount = count if count < other_count else other_count\n",
    "            if newcount > 0:\n",
    "                result[elem] = newcount\n",
    "        return result\n",
    "\n",
    "    def __pos__(self):\n",
    "        'Adds an empty counter, effectively stripping negative and zero counts'\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            if count > 0:\n",
    "                result[elem] = count\n",
    "        return result\n",
    "\n",
    "    def __neg__(self):\n",
    "        '''Subtracts from an empty counter.  Strips positive and zero counts,\n",
    "        and flips the sign on negative counts.\n",
    "        '''\n",
    "        result = Counter()\n",
    "        for elem, count in self.items():\n",
    "            if count < 0:\n",
    "                result[elem] = 0 - count\n",
    "        return result\n",
    "\n",
    "    def _keep_positive(self):\n",
    "        '''Internal method to strip elements with a negative or zero count'''\n",
    "        nonpositive = [elem for elem, count in self.items() if not count > 0]\n",
    "        for elem in nonpositive:\n",
    "            del self[elem]\n",
    "        return self\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        '''Inplace add from another counter, keeping only positive counts.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c += Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 4, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, count in other.items():\n",
    "            self[elem] += count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        '''Inplace subtract counter, but keep only results with positive counts.\n",
    "        >>> c = Counter('abbbc')\n",
    "        >>> c -= Counter('bccd')\n",
    "        >>> c\n",
    "        Counter({'b': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, count in other.items():\n",
    "            self[elem] -= count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __ior__(self, other):\n",
    "        '''Inplace union is the maximum of value from either counter.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c |= Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 3, 'c': 2, 'a': 1})\n",
    "        '''\n",
    "        for elem, other_count in other.items():\n",
    "            count = self[elem]\n",
    "            if other_count > count:\n",
    "                self[elem] = other_count\n",
    "        return self._keep_positive()\n",
    "\n",
    "    def __iand__(self, other):\n",
    "        '''Inplace intersection is the minimum of corresponding counts.\n",
    "        >>> c = Counter('abbb')\n",
    "        >>> c &= Counter('bcc')\n",
    "        >>> c\n",
    "        Counter({'b': 1})\n",
    "        '''\n",
    "        for elem, count in self.items():\n",
    "            other_count = other[elem]\n",
    "            if other_count < count:\n",
    "                self[elem] = other_count\n",
    "        return self._keep_positive()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "class getQs(object):\n",
    "\tdef __init__(self, data, pi_e, processor, action_space_dim = 3):\n",
    "\t\tself.data = data\n",
    "\t\tself.pi_e = pi_e\n",
    "\t\tself.processor = processor\n",
    "\t\tself.action_space_dim = action_space_dim\n",
    "\n",
    "\tdef get(self, model):\n",
    "\t\tQs = []\n",
    "\t\tbatchsize = 1\n",
    "\t\tnum_batches = int(np.ceil(len(self.data)/batchsize))\n",
    "\t\t# frames = np.array([x['frames'] for x in self.trajectories])\n",
    "\t\tfor batchnum in trange(num_batches, desc='Batch'):\n",
    "\t\t\tlow_ = batchsize*batchnum\n",
    "\t\t\thigh_ = min(batchsize*(batchnum+1), len(self.data))\n",
    "\n",
    "\t\t\tpos = self.data.states(False, low_=low_,high_=high_)\n",
    "\t\t\tacts = self.data.actions()[low_:high_]\n",
    "\n",
    "\n",
    "\t\t\t# episodes = self.trajectories[low_:high_]\n",
    "\t\t\t# pos = np.vstack([np.vstack(x['x']) for x in episodes])\n",
    "\t\t\t# N = np.hstack([[low_ + n]*len(x['x']) for n,x in enumerate(episodes)])\n",
    "\t\t\t# acts = np.hstack([x['a'] for x in episodes])\n",
    "\t\t\t# pos = np.array([np.array(frames[int(N[idx])])[pos[idx].astype(int)] for idx in range(len(pos))])\n",
    "\t\t\ttraj_Qs = model.Q(self.pi_e, self.processor(pos))\n",
    "\n",
    "\t\t\ttraj_Qs = traj_Qs.reshape(-1, self.action_space_dim)\n",
    "\t\t\t# lengths = self.data.lengths()\n",
    "\n",
    "\t\t\t# endpts = np.cumsum(np.hstack([[0], lengths]))\n",
    "\t\t\t# for start,end in zip(endpts[:-1], endpts[1:]):\n",
    "\t\t\t# \tQs.append(traj_Qs[start:end])\n",
    "\t\t\tQs.append(traj_Qs)\n",
    "\n",
    "\t\treturn Qs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class defaultCNN(nn.Module):\n",
    "    def __init__(self, shape, action_space_dim):\n",
    "        super(defaultCNN, self).__init__()\n",
    "        self.c, self.h, self.w = shape\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(self.c, 16, (2,2)),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*(self.h-1)*(self.w-1), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=.001)\n",
    "            torch.nn.init.normal_(m.bias, mean=0.0, std=.001)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        output = self.net(state)\n",
    "        return torch.masked_select(output, action)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "    def predict_w_softmax(self, state):\n",
    "        return nn.Softmax()(self.net(state))\n",
    "\n",
    "class defaultModelBasedCNN(nn.Module):\n",
    "    def __init__(self, shape, action_space_dim):\n",
    "        super(defaultModelBasedCNN, self).__init__()\n",
    "        self.c, self.h, self.w = shape\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.c, 4, (5, 5)),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(4, 8, (3, 3)),\n",
    "        )\n",
    "\n",
    "        self.states_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, (3, 3)),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(16, action_space_dim, (5, 5)),\n",
    "        )\n",
    "        \n",
    "        self.rewards_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*(self.h-4-2)*(self.w-4-2), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim),\n",
    "        )\n",
    "\n",
    "        self.dones_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*(self.h-4-2)*(self.w-4-2), 8),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(8, action_space_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=.001)\n",
    "            torch.nn.init.normal_(m.bias, mean=0.0, std=.001)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        T, R, D = self.states_head(self.features(state)), self.rewards_head(self.features(state)), self.dones_head(self.features(state))\n",
    "        return T[np.arange(len(action)), action.float().argmax(1), ...][:,None,:,:], torch.masked_select(R, action), torch.masked_select(D, action)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.states_head(self.features(state)), self.rewards_head(self.features(state)), self.dones_head(self.features(state))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std):\n",
    "        super(PropensityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "        self.std = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x):\n",
    "        # Sample from the Gaussian distribution with mean predicted by the model and fixed standard deviation\n",
    "        mean = self.forward(x)\n",
    "        std = torch.tensor(self.std).expand_as(mean)\n",
    "        return torch.normal(mean, std)\n",
    "\n",
    "# Define the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim, action_dim, std=0.1)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop through the dataset and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the loss and backpropagate\n",
    "    loss = criterion(model(state), action)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std):\n",
    "        super(PropensityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "        self.std = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x):\n",
    "        # Sample from the Gaussian distribution with mean predicted by the model and fixed standard deviation\n",
    "        mean = self.forward(x)\n",
    "        std = torch.tensor(self.std).expand_as(mean)\n",
    "        return torch.normal(mean, std)\n",
    "\n",
    "# Define the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim, action_dim, std=0.1)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop through the dataset and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the loss and backpropagate\n",
    "    loss = criterion(model(state), action)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropensityModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, std=1.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, action_dim)\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x).numpy()\n",
    "    \n",
    "    def log_prob(self, x, actions):\n",
    "        means = self.forward(x)\n",
    "        log_prstate = -0.5 * ((actions - means) / self.std) ** 2 - 0.5 * np.log(2 * np.pi) - np.log(self.std)\n",
    "        return log_prstate.sum(1, keepdim=True)\n",
    "\n",
    "# Create the model\n",
    "state_dim = 14  #np.zeros(14)  #np.zeros(21)\n",
    "action_dim = 2  #np.zeros(2)\n",
    "model = PropensityModel(state_dim=state_dim, action_dim=action_dim)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Loop over the data and update the model\n",
    "batch_size = 64\n",
    "for i in range(5000):\n",
    "    state, action, new_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Compute the log probabilities of the actions\n",
    "    log_prstate = model.log_prob(state, action)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = -log_prstate.mean()\n",
    "    \n",
    "    # Perform backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Use the model to predict the propensity of an action given a state\n",
    "#state = np.array([[1, 2, 3, 4]])\n",
    "#print(model.predict(torch.from_numpy(state).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_propensity(self, behavior_data):    #cfg):\n",
    "    # WARN: Only works in tabular env with discrete action space. Current implementation is a max likelihood\n",
    "\n",
    "    model = defaultCNN(self.states()[0][0].shape, self.n_actions) #cfg.to_regress_pi_b['model']\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    self.processed_data = self.fill()\n",
    "\n",
    "    batch_size = 32 #cfg.to_regress_pi_b['batch_size']\n",
    "    dataset_length = self.num_tuples()\n",
    "    perm = np.random.permutation(range(dataset_length))\n",
    "    eighty_percent_of_set = int(.8*len(perm))\n",
    "    training_idxs = perm[:eighty_percent_of_set]\n",
    "    validation_idxs = perm[eighty_percent_of_set:]\n",
    "    training_steps_per_epoch = int(np.ceil(len(training_idxs)/float(batch_size)))\n",
    "    validation_steps_per_epoch = int(np.ceil(len(validation_idxs)/float(batch_size)))\n",
    "\n",
    "    for k in tqdm(range(100)):#cfg.to_regress_pi_b['max_epochs']\n",
    "        \n",
    "        train_gen = self.generator(training_idxs, fixed_permutation=True, batch_size=batch_size)\n",
    "        val_gen = self.generator(validation_idxs, fixed_permutation=True, batch_size=batch_size)\n",
    "\n",
    "        # TODO: earlyStopping, LR reduction\n",
    "\n",
    "        for step in range(training_steps_per_epoch):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                (inp, out) = next(train_gen)\n",
    "                states = torch.from_numpy(inp).float()\n",
    "                actions = torch.from_numpy(out).float().argmax(1)\n",
    "\n",
    "            prediction = model.predict_w_softmax(states)\n",
    "            \n",
    "            loss = nn.NLLLoss()(torch.log(prediction), actions)\n",
    "                                        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), 1.0)   #cfg.to_regress_pi_b['clipnorm']\n",
    "            optimizer.step()\n",
    "\n",
    "    for episode_num, states in enumerate(np.squeeze(self.states())):\n",
    "        base_propensity = []\n",
    "        for state in states:\n",
    "            base_propensity.append(model.predict_w_softmax(torch.from_numpy(state[None,None,...]).float()).detach().numpy()[0].tolist())\n",
    "\n",
    "        self.trajectories[episode_num]['base_propensity'] = base_propensity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAGIC algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omega(self):\n",
    "        return np.array([[episode['target_propensity'][idx][int(act)]/episode['base_propensity'][idx][int(act)] for idx,act in enumerate(episode['a'])] for episode in self.trajectories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGIC(object):\n",
    "    \"\"\"Algorithm: MAGIC.\n",
    "    \"\"\"\n",
    "    NUM_SUBSETS_FOR_CB_ESTIMATES = 25\n",
    "    CONFIDENCE_INTERVAL = 0.9\n",
    "    NUM_BOOTSTRAP_SAMPLES = 50\n",
    "    BOOTSTRAP_SAMPLE_PCT = 0.5\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "            Discount factor.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def evaluate(self, info, num_j_steps, is_wdr, return_Qs = False):\n",
    "        \"\"\"Get MAGIC estimate from Q + IPS.\n",
    "        Parameters\n",
    "        ----------\n",
    "        info : list\n",
    "            [list of actions, list of rewards, list of base propensity, list of target propensity, list of Qhat]\n",
    "        num_j_steps : int\n",
    "            Parameter to MAGIC algorithm\n",
    "        is_wdr : bool\n",
    "            Use Weighted Doubly Robust?\n",
    "        return_Qs : bool\n",
    "            Return trajectory-wise estimate alongside full DR estimate?\n",
    "            Default: False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            [MAGIC estimate, normalized MAGIC, std error, normalized std error]\n",
    "            If return_Qs is true, also returns trajectory-wise estimate\n",
    "        \"\"\"\n",
    "\n",
    "        (actions,\n",
    "        rewards,\n",
    "        base_propensity,\n",
    "        target_propensities,\n",
    "        estimated_q_values) = MAGIC.transform_to_equal_length_trajectories(*info)\n",
    "\n",
    "        num_trajectories = actions.shape[0]\n",
    "        trajectory_length = actions.shape[1]\n",
    "\n",
    "        j_steps = [float(\"inf\")]\n",
    "\n",
    "        if num_j_steps > 1:\n",
    "            j_steps.append(-1)\n",
    "        if num_j_steps > 2:\n",
    "            interval = trajectory_length // (num_j_steps - 1)\n",
    "            j_steps.extend([i * interval for i in range(1, num_j_steps - 1)])\n",
    "\n",
    "        base_propensity_for_logged_action = np.sum(\n",
    "            np.multiply(base_propensity, actions), axis=2\n",
    "        )\n",
    "        target_propensity_for_logged_action = np.sum(\n",
    "            np.multiply(target_propensities, actions), axis=2\n",
    "        )\n",
    "        estimated_q_values_for_logged_action = np.sum(\n",
    "            np.multiply(estimated_q_values, actions), axis=2\n",
    "        )\n",
    "        estimated_state_values = np.sum(\n",
    "            np.multiply(target_propensities, estimated_q_values), axis=2\n",
    "        )\n",
    "\n",
    "        importance_weights = target_propensity_for_logged_action / base_propensity_for_logged_action\n",
    "        importance_weights[np.isnan(importance_weights)] = 0.\n",
    "        importance_weights = np.cumprod(importance_weights, axis=1)\n",
    "        importance_weights = MAGIC.normalize_importance_weights(\n",
    "            importance_weights, is_wdr\n",
    "        )\n",
    "\n",
    "        importance_weights_one_earlier = (\n",
    "            np.ones([num_trajectories, 1]) * 1.0 / num_trajectories\n",
    "        )\n",
    "        importance_weights_one_earlier = np.hstack(\n",
    "            [importance_weights_one_earlier, importance_weights[:, :-1]]\n",
    "        )\n",
    "\n",
    "        discounts = np.logspace(\n",
    "            start=0, stop=trajectory_length - 1, num=trajectory_length, base=self.gamma\n",
    "        )\n",
    "\n",
    "        j_step_return_trajectories = []\n",
    "        for j_step in j_steps:\n",
    "            j_step_return_trajectories.append(\n",
    "                MAGIC.calculate_step_return(\n",
    "                    rewards,\n",
    "                    discounts,\n",
    "                    importance_weights,\n",
    "                    importance_weights_one_earlier,\n",
    "                    estimated_state_values,\n",
    "                    estimated_q_values_for_logged_action,\n",
    "                    j_step,\n",
    "                )\n",
    "            )\n",
    "        j_step_return_trajectories = np.array(j_step_return_trajectories)\n",
    "\n",
    "        j_step_returns = np.sum(j_step_return_trajectories, axis=1)\n",
    "\n",
    "        if len(j_step_returns) == 1:\n",
    "            weighted_doubly_robust = j_step_returns[0]\n",
    "            weighted_doubly_robust_std_error = 0.0\n",
    "        else:\n",
    "            # break trajectories into several subsets to estimate confidence bounds\n",
    "            infinite_step_returns = []\n",
    "            num_subsets = int(\n",
    "                min(\n",
    "                    num_trajectories / 2,\n",
    "                    MAGIC.NUM_SUBSETS_FOR_CB_ESTIMATES,\n",
    "                )\n",
    "            )\n",
    "            interval = num_trajectories / num_subsets\n",
    "            for i in range(num_subsets):\n",
    "                trajectory_subset = np.arange(\n",
    "                    int(i * interval), int((i + 1) * interval)\n",
    "                )\n",
    "                importance_weights = (\n",
    "                    target_propensity_for_logged_action[trajectory_subset]\n",
    "                    / base_propensity_for_logged_action[trajectory_subset]\n",
    "                )\n",
    "                importance_weights[np.isnan(importance_weights)] = 0.\n",
    "                importance_weights = np.cumprod(importance_weights, axis=1)\n",
    "                importance_weights = MAGIC.normalize_importance_weights(\n",
    "                    importance_weights, is_wdr\n",
    "                )\n",
    "                importance_weights_one_earlier = (\n",
    "                    np.ones([len(trajectory_subset), 1]) * 1.0 / len(trajectory_subset)\n",
    "                )\n",
    "                importance_weights_one_earlier = np.hstack(\n",
    "                    [importance_weights_one_earlier, importance_weights[:, :-1]]\n",
    "                )\n",
    "                infinite_step_return = np.sum(\n",
    "                    MAGIC.calculate_step_return(\n",
    "                        rewards[trajectory_subset],\n",
    "                        discounts,\n",
    "                        importance_weights,\n",
    "                        importance_weights_one_earlier,\n",
    "                        estimated_state_values[trajectory_subset],\n",
    "                        estimated_q_values_for_logged_action[trajectory_subset],\n",
    "                        float(\"inf\"),\n",
    "                    )\n",
    "                )\n",
    "                infinite_step_returns.append(infinite_step_return)\n",
    "\n",
    "            # Compute weighted_doubly_robust mean point estimate using all data\n",
    "            weighted_doubly_robust, xs = self.compute_weighted_doubly_robust_point_estimate(\n",
    "                j_steps,\n",
    "                num_j_steps,\n",
    "                j_step_returns,\n",
    "                infinite_step_returns,\n",
    "                j_step_return_trajectories,\n",
    "            )\n",
    "\n",
    "            # Use bootstrapping to compute weighted_doubly_robust standard error\n",
    "            bootstrapped_means = []\n",
    "            sample_size = int(\n",
    "                MAGIC.BOOTSTRAP_SAMPLE_PCT\n",
    "                * num_subsets\n",
    "            )\n",
    "            for _ in range(\n",
    "                MAGIC.NUM_BOOTSTRAP_SAMPLES\n",
    "            ):\n",
    "                random_idxs = np.random.choice(num_j_steps, sample_size, replace=False)\n",
    "                random_idxs.sort()\n",
    "                wdr_estimate = self.compute_weighted_doubly_robust_point_estimate(\n",
    "                    j_steps=[j_steps[i] for i in random_idxs],\n",
    "                    num_j_steps=sample_size,\n",
    "                    j_step_returns=j_step_returns[random_idxs],\n",
    "                    infinite_step_returns=infinite_step_returns,\n",
    "                    j_step_return_trajectories=j_step_return_trajectories[random_idxs],\n",
    "                )\n",
    "                bootstrapped_means.append(wdr_estimate)\n",
    "            weighted_doubly_robust_std_error = np.std(bootstrapped_means)\n",
    "\n",
    "        episode_values = np.sum(np.multiply(rewards, discounts), axis=1)\n",
    "        denominator = np.nanmean(episode_values)\n",
    "        if abs(denominator) < 1e-6:\n",
    "            return [0]*4\n",
    "\n",
    "        # print (weighted_doubly_robust,\n",
    "        #         weighted_doubly_robust / denominator,\n",
    "        #         weighted_doubly_robust_std_error,\n",
    "        #         weighted_doubly_robust_std_error / denominator)\n",
    "\n",
    "        if return_Qs:\n",
    "            return [weighted_doubly_robust,\n",
    "                    weighted_doubly_robust / denominator,\n",
    "                    weighted_doubly_robust_std_error,\n",
    "                    weighted_doubly_robust_std_error / denominator], np.dot(xs, j_step_return_trajectories)\n",
    "        else:\n",
    "            return [weighted_doubly_robust,\n",
    "                    weighted_doubly_robust / denominator,\n",
    "                    weighted_doubly_robust_std_error,\n",
    "                    weighted_doubly_robust_std_error / denominator]\n",
    "\n",
    "    def compute_weighted_doubly_robust_point_estimate(\n",
    "        self,\n",
    "        j_steps,\n",
    "        num_j_steps,\n",
    "        j_step_returns,\n",
    "        infinite_step_returns,\n",
    "        j_step_return_trajectories,\n",
    "    ):\n",
    "        low_bound, high_bound = MAGIC.confidence_bounds(\n",
    "            infinite_step_returns,\n",
    "            MAGIC.CONFIDENCE_INTERVAL,\n",
    "        )\n",
    "        # decompose error into bias + variance\n",
    "        j_step_bias = np.zeros([num_j_steps])\n",
    "        where_lower = np.where(j_step_returns < low_bound)[0]\n",
    "        j_step_bias[where_lower] = low_bound - j_step_returns[where_lower]\n",
    "        where_higher = np.where(j_step_returns > high_bound)[0]\n",
    "        j_step_bias[where_higher] = j_step_returns[where_higher] - high_bound\n",
    "\n",
    "        covariance = np.cov(j_step_return_trajectories)\n",
    "        error = covariance + j_step_bias.T * j_step_bias\n",
    "\n",
    "        # minimize mse error\n",
    "        constraint = {\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - 1.0}\n",
    "\n",
    "        x = np.zeros([len(j_steps)])\n",
    "        res = sp.optimize.minimize(\n",
    "            mse_loss,\n",
    "            x,\n",
    "            args=error,\n",
    "            constraints=constraint,\n",
    "            bounds=[(0, 1) for _ in range(x.shape[0])],\n",
    "        )\n",
    "        x = np.array(res.x)\n",
    "        return float(np.dot(x, j_step_returns)), x\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_to_equal_length_trajectories(\n",
    "        actions,\n",
    "        rewards,\n",
    "        logged_propensities,\n",
    "        target_propensities,\n",
    "        estimated_q_values,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Take in samples (action, rewards, propensities, etc.) and output lists\n",
    "        of equal-length trajectories (episodes) according to terminals.\n",
    "        As the raw trajectories are of various lengths, the shorter ones are\n",
    "        filled with zeros(ones) at the end.\n",
    "        \"\"\"\n",
    "        num_actions = len(target_propensities[0][0])\n",
    "\n",
    "        def to_equal_length(x, fill_value):\n",
    "            x_equal_length = np.array(\n",
    "                list(itertools.zip_longest(*x, fillvalue=fill_value))\n",
    "            ).swapaxes(0, 1)\n",
    "            return x_equal_length\n",
    "\n",
    "        action_trajectories = to_equal_length(\n",
    "            [np.eye(num_actions)[act] for act in actions], np.zeros([num_actions])\n",
    "        )\n",
    "        reward_trajectories = to_equal_length(rewards, 0)\n",
    "        logged_propensity_trajectories = to_equal_length(\n",
    "            logged_propensities, np.zeros([num_actions])\n",
    "        )\n",
    "        target_propensity_trajectories = to_equal_length(\n",
    "            target_propensities, np.zeros([num_actions])\n",
    "        )\n",
    "\n",
    "        # Hack for now. Delete.\n",
    "        estimated_q_values = [[np.hstack(y).tolist() for y in x] for x in estimated_q_values]\n",
    "\n",
    "        Q_value_trajectories = to_equal_length(\n",
    "            estimated_q_values, np.zeros([num_actions])\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            action_trajectories,\n",
    "            reward_trajectories,\n",
    "            logged_propensity_trajectories,\n",
    "            target_propensity_trajectories,\n",
    "            Q_value_trajectories,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_importance_weights(\n",
    "        importance_weights, is_wdr\n",
    "    ):\n",
    "        if is_wdr:\n",
    "            sum_importance_weights = np.sum(importance_weights, axis=0)\n",
    "            where_zeros = np.where(sum_importance_weights == 0.0)[0]\n",
    "            sum_importance_weights[where_zeros] = len(importance_weights)\n",
    "            importance_weights[:, where_zeros] = 1.0\n",
    "            importance_weights /= sum_importance_weights\n",
    "            return importance_weights\n",
    "        else:\n",
    "            importance_weights /= importance_weights.shape[0]\n",
    "            return importance_weights\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_step_return(\n",
    "        rewards,\n",
    "        discounts,\n",
    "        importance_weights,\n",
    "        importance_weights_one_earlier,\n",
    "        estimated_state_values,\n",
    "        estimated_q_values,\n",
    "        j_step,\n",
    "    ):\n",
    "        trajectory_length = len(rewards[0])\n",
    "        num_trajectories = len(rewards)\n",
    "        j_step = int(min(j_step, trajectory_length - 1))\n",
    "\n",
    "        weighted_discounts = np.multiply(discounts, importance_weights)\n",
    "        weighted_discounts_one_earlier = np.multiply(\n",
    "            discounts, importance_weights_one_earlier\n",
    "        )\n",
    "\n",
    "        importance_sampled_cumulative_reward = np.sum(\n",
    "            np.multiply(weighted_discounts[:, : j_step + 1], rewards[:, : j_step + 1]),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        if j_step < trajectory_length - 1:\n",
    "            direct_method_value = (\n",
    "                weighted_discounts_one_earlier[:, j_step + 1]\n",
    "                * estimated_state_values[:, j_step + 1]\n",
    "            )\n",
    "        else:\n",
    "            direct_method_value = np.zeros([num_trajectories])\n",
    "\n",
    "        control_variate = np.sum(\n",
    "            np.multiply(\n",
    "                weighted_discounts[:, : j_step + 1], estimated_q_values[:, : j_step + 1]\n",
    "            )\n",
    "            - np.multiply(\n",
    "                weighted_discounts_one_earlier[:, : j_step + 1],\n",
    "                estimated_state_values[:, : j_step + 1],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        j_step_return = (\n",
    "            importance_sampled_cumulative_reward + direct_method_value - control_variate\n",
    "        )\n",
    "\n",
    "        return j_step_return\n",
    "\n",
    "    @staticmethod\n",
    "    def confidence_bounds(x, confidence):\n",
    "        n = len(x)\n",
    "        m, se = np.mean(x), sp.stats.sem(x)\n",
    "        h = se * sp.stats.t._ppf((1 + confidence) / 2.0, n - 1)\n",
    "        return m - h, m + h\n",
    "\n",
    "\n",
    "def mse_loss(x, error):\n",
    "    return np.dot(np.dot(x, error), x.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
